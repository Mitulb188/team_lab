{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff8450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "#import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "# print(torch.__version__)\n",
    "# print(torchaudio.__version__)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa # For audio loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420e3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_spectrogram(spectrogram, max_time_steps, padding_value=-80.0):\n",
    "    \"\"\"\n",
    "    Pads or truncates a 2D spectrogram to a specified number of time steps (width).\n",
    "\n",
    "    Padding is applied to the end of the time axis (axis 1).\n",
    "    Truncation is performed by taking the initial part of the time axis.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (np.ndarray): The input 2D spectrogram (num_features, num_time_frames).\n",
    "        max_time_steps (int): The target number of time frames (width) for the spectrogram.\n",
    "        padding_value (float, optional): The value to use for padding.\n",
    "                                         Defaults to -80.0 (suitable for dB-scaled log-Mel spectrograms).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The padded or truncated spectrogram of shape (num_features, max_time_steps).\n",
    "    \"\"\"\n",
    "    if not isinstance(spectrogram, np.ndarray) or spectrogram.ndim != 2:\n",
    "        raise ValueError(\"Input spectrogram must be a 2D NumPy array.\")\n",
    "    if not isinstance(max_time_steps, int) or max_time_steps <= 0:\n",
    "        raise ValueError(\"max_time_steps must be a positive integer.\")\n",
    "    \n",
    "    #print(spectrogram.shape)\n",
    "\n",
    "    num_features, current_time_steps = spectrogram.shape\n",
    "    \n",
    "\n",
    "    max_time_steps = 157\n",
    "\n",
    "\n",
    "    if current_time_steps == max_time_steps:\n",
    "        # Already the correct length\n",
    "        return spectrogram\n",
    "    elif current_time_steps < max_time_steps:\n",
    "        # Pad if shorter\n",
    "        pad_width = max_time_steps - current_time_steps\n",
    "        # np.pad takes a list of tuples for padding widths: ((before_axis0, after_axis0), (before_axis1, after_axis1), ...)\n",
    "        # We only want to pad axis 1 (time steps) at the end.\n",
    "        padded_spectrogram = np.pad(\n",
    "            spectrogram,\n",
    "            pad_width=((0, 0), (0, pad_width)), # No padding on feature axis, pad_width at the end of time axis\n",
    "            mode='constant',\n",
    "            constant_values=padding_value\n",
    "        )\n",
    "        return padded_spectrogram\n",
    "    else: # current_time_steps > max_time_steps\n",
    "        # Truncate if longer (take from the beginning of the time axis)\n",
    "        truncated_spectrogram = spectrogram[:, :max_time_steps]\n",
    "        return truncated_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41acb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_flac_files_in_folder(feature_vector,folder_path, target_sample_rate=16000, duration=10):\n",
    "    \"\"\"\n",
    "    Reads all .flac files from a given folder, loads them using librosa,\n",
    "    and prints basic information.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing .flac files.\n",
    "        target_sample_rate (int, optional): The target sampling rate to resample audio to.\n",
    "                                            If None, loads at original sampling rate.\n",
    "                                            Defaults to None.\n",
    "        duration (float, optional): Maximum duration (in seconds) to load for each audio file.\n",
    "                                    If None, loads the entire file. Defaults to None.\n",
    "    \"\"\"\n",
    "    print(f\"Processing FLAC files in folder: {folder_path}\\n\")\n",
    "    found_flac_files = False\n",
    "    count = 0\n",
    "    # Check if the folder exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Folder not found at '{folder_path}'\")\n",
    "        return\n",
    "    freq = {}\n",
    "    # Iterate over all entries in the folder\n",
    "    for item_name in os.listdir(folder_path):\n",
    "        # Construct the full path to the item\n",
    "        item_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "        # Check if it's a file and ends with .flac\n",
    "        if os.path.isfile(item_path) and item_name.lower().endswith(\".flac\"):\n",
    "            found_flac_files = True\n",
    "            count = count + 1\n",
    "            print(f\"--- Found FLAC file: {item_name} ---\")\n",
    "            try:\n",
    "                # Load the audio file\n",
    "                # audio_data is a NumPy array containing the waveform\n",
    "                # sr is the sampling rate of the loaded audio\n",
    "                audio_data, sr = librosa.load(item_path, sr=target_sample_rate, duration=duration)\n",
    "\n",
    "                print(f\"  Successfully loaded.\")\n",
    "                print(f\"  Shape of audio data: {audio_data.shape}\")\n",
    "                print(f\"  Sampling rate: {sr} Hz\")\n",
    "                print(f\"  Duration: {librosa.get_duration(y=audio_data, sr=sr):.2f} seconds\")\n",
    "                # You can add more processing here, e.g., feature extraction\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
    "                print(f\"  Mel spectrogram shape: {mel_spectrogram.shape}\")\n",
    "                padded_mel_spectrogram = pad_or_truncate_spectrogram(mel_spectrogram,157,-80)\n",
    "                feature_vector.append(padded_mel_spectrogram)\n",
    "                print(f\"Added to the list - Shape of MFCC features: {padded_mel_spectrogram.shape}\")\n",
    "                if padded_mel_spectrogram.shape[1] not in freq:\n",
    "                    freq[padded_mel_spectrogram.shape[1]] = 1\n",
    "                else:\n",
    "                    freq[padded_mel_spectrogram.shape[1]] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading or processing file {item_name}: {e}\")\n",
    "            print(\"-\" * 30) # Separator\n",
    "    # print(count)\n",
    "    # print(freq)\n",
    "    if not found_flac_files:\n",
    "        print(f\"No .flac files found in '{folder_path}'.\")\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c537798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_cqcc(y, sr, hop_length=512, fmin=None, n_bins=90, bins_per_octave=12, n_cqcc = 128):\n",
    "    \n",
    "    if fmin is None:\n",
    "        # Set a default fmin similar to librosa's default if not provided\n",
    "        # This corresponds to C1\n",
    "        fmin = librosa.note_to_hz('C1')\n",
    "    try:\n",
    "        # 1. Calculate Constant Q Transform (CQT)\n",
    "        # We use the magnitude CQT directly\n",
    "        cqt_result = librosa.cqt(y=y, sr=sr,\n",
    "                                hop_length=hop_length,\n",
    "                                fmin=fmin,\n",
    "                                n_bins=n_bins,\n",
    "                                bins_per_octave=bins_per_octave)\n",
    "\n",
    "        # Get the magnitude (absolute value) - CQT returns complex values\n",
    "        cqt_mag = np.abs(cqt_result)\n",
    "\n",
    "        # Handle potential zero values before log\n",
    "        cqt_mag[cqt_mag == 0] = np.finfo(float).eps\n",
    "\n",
    "        # 2. Calculate Log Power/Magnitude\n",
    "        log_cqt_mag = np.log(cqt_mag)\n",
    "\n",
    "        # 3. Apply Discrete Cosine Transform (DCT) - Type II is common\n",
    "        # Apply DCT along the frequency axis (axis=0)\n",
    "        cqcc = dct(log_cqt_mag, type=2, axis=0, norm='ortho')\n",
    "\n",
    "        # 4. Keep only the first n_cqcc coefficients\n",
    "        cqcc_truncated = cqcc[:n_cqcc, :]\n",
    "\n",
    "        return cqcc_truncated\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating CQT or CQCC: {e}\")\n",
    "        # Handle cases where CQT might fail (e.g., very short audio)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357efff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_array(audio_folder_path , label_file_path):\n",
    "    count = 0\n",
    "    audio_folder_path = audio_folder_path\n",
    "    label_file_path = label_file_path\n",
    "    audio_name_list = []\n",
    "    label_dict = {}\n",
    "\n",
    "    for audio_name in os.listdir(audio_folder_path):\n",
    "        # Construct the full path to the item\n",
    "        item_path = os.path.join(audio_folder_path, audio_name)\n",
    "\n",
    "        # Check if it's a file and ends with .flac\n",
    "        if os.path.isfile(item_path) and audio_name.lower().endswith(\".flac\"):\n",
    "            found_flac_files = True\n",
    "            count = count + 1\n",
    "            #print(f\"--- Found FLAC file: {audio_name.strip('.flac')} ---\")\n",
    "            audio_name_list.append(audio_name.strip('.flac'))\n",
    "\n",
    "    with open(label_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        #print(lines)\n",
    "    for line_num, line in enumerate(lines):\n",
    "        #print(line_num)\n",
    "        parts = line.strip().split()\n",
    "        # Check if line has at least 4 parts (SpeakerID, Filename, Key) - adjust if format differs\n",
    "        if len(parts) >= 4:\n",
    "            file_name = parts[1]  # Assuming filename is the second element\n",
    "            key = parts[-1]     # Assuming label key is the last element\n",
    "            #print(key)\n",
    "\n",
    "            if key == \"bonafide\" and file_name in audio_name_list:\n",
    "                label = 1\n",
    "                label_dict[file_name] = label\n",
    "                \n",
    "            elif key == \"spoof\" and file_name in audio_name_list:\n",
    "                label = 0\n",
    "                label_dict[file_name] = label\n",
    "        else:\n",
    "            print(f\"Warning: Skipping malformed line {line_num + 1}: {line.strip()}\")\n",
    "    return label_dict,audio_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8871de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, OneOf, SomeOf, Gain, TimeMask\n",
    "\n",
    "# # --- 1. Setup paths and augmentation pipeline ---\n",
    "# original_bonafide_path = \"bonafide_audio_train\"\n",
    "# spoof_path = \"spoof_audio_train\"\n",
    "# augmented_bonafide_path = \"augmented_bonafide\"\n",
    "\n",
    "# # Create a directory to save augmented files\n",
    "# os.makedirs(augmented_bonafide_path, exist_ok=True)\n",
    "\n",
    "# # Define the augmentation pipeline\n",
    "# augment = Compose([\n",
    "#     # Apply one of the following \"major\" transformations to 80% of the samples\n",
    "#     OneOf([\n",
    "#         TimeStretch(min_rate=0.85, max_rate=1.15),\n",
    "#         PitchShift(min_semitones=-1.5, max_semitones=1.5),\n",
    "#     ], p=0.8),\n",
    "\n",
    "#     # Apply a small amount of noise to 40% of the samples\n",
    "#     AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.008, p=0.4),\n",
    "\n",
    "#     # Apply a small gain adjustment to 40% of the samples\n",
    "#     Gain(min_gain_in_db=-4, max_gain_in_db=4, p=0.4),\n",
    "\n",
    "#     # Apply a time mask to 30% of the samples\n",
    "#     TimeMask(min_band_part=0.02, max_band_part=0.08, p=0.3),\n",
    "# ])\n",
    "\n",
    "# # --- 2. Calculate how many new files to generate ---\n",
    "# bonafide_files = os.listdir(original_bonafide_path)\n",
    "# spoof_files_count = 22799 # Your majority class count\n",
    "# bonafide_files_count = len(bonafide_files)\n",
    "# num_augmentations_needed = spoof_files_count - bonafide_files_count\n",
    "# augmentations_per_file = round(num_augmentations_needed / bonafide_files_count)\n",
    "\n",
    "# print(f\"Generating {augmentations_per_file} augmented samples per Bonafide file.\")\n",
    "\n",
    "# # --- 3. Apply augmentation ---\n",
    "# for filename in bonafide_files:\n",
    "#     # Load the audio file\n",
    "#     audio, sr = librosa.load(os.path.join(original_bonafide_path, filename), sr=16000)\n",
    "    \n",
    "#     # Create multiple augmented versions\n",
    "#     for i in range(augmentations_per_file):\n",
    "#         # Apply the augmentation pipeline\n",
    "#         augmented_audio = augment(samples=audio, sample_rate=sr)\n",
    "        \n",
    "#         # Define a new filename and save the file\n",
    "#         new_filename = f\"aug_{i}_{filename}\"\n",
    "#         sf.write(os.path.join(augmented_bonafide_path, new_filename), augmented_audio, sr)\n",
    "\n",
    "# print(\"Data augmentation complete.\")\n",
    "# # Now your training data can be loaded from the spoof, original bonafide, and augmented bonafide folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b636d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723bc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_labels = sorted(label_list.items())\n",
    "# labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d51c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_labels = sorted(label_list.items())\n",
    "# labels = []\n",
    "# for i in range(0,len(sorted_labels)):\n",
    "#     labels.append(sorted_labels[i][1])\n",
    "# labels = np.array(labels)\n",
    "# feature_vector = np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "273b9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b3aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_vector = np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "177f9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ae1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(label_list_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "326e75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_labels_val = sorted(label_list_val.items())\n",
    "# labels_val= []\n",
    "# for i in range(0,len(sorted_labels_val)):\n",
    "#     labels_val.append(sorted_labels_val[i][1])\n",
    "# labels_val = np.array(labels_val)\n",
    "# #labels_val\n",
    "# feature_vector_val = np.array(feature_vector_val)\n",
    "# print(feature_vector_val.shape, labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95ae47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG FOR Pre-Processing for EVALUATION ONLY ---\n",
    "# your_folder_with_flac_files_val = \"ASVSpoof19/LA/ASVspoof2019_LA_eval/flac\"\n",
    "# audio_folder_path_val = \"ASVSpoof19/LA/ASVspoof2019_LA_eval/flac\"\n",
    "# label_file_val = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\" \n",
    "# folder_path_val = your_folder_with_flac_files_val\n",
    "# val_file_sorted_path= \"val_file_list.txt\"\n",
    "# print(f\"Processing FLAC files in folder: {folder_path_val}\\n\")\n",
    "# label_list_val, audio_name_list_val = create_label_array(audio_folder_path=audio_folder_path_val, label_file_path=label_file_val)\n",
    "# sorted_labels_val = sorted(label_list_val.items())\n",
    "# labels_val= []\n",
    "# for i in range(0,len(sorted_labels_val)):\n",
    "#     labels_val.append(sorted_labels_val[i][1])\n",
    "# labels_val = np.array(labels_val)\n",
    "# # print(len(label_list_val), len(audio_name_list_val),len(feature_vector_val))\n",
    "# feature_vector_val = []\n",
    "# found_flac_files_val = False\n",
    "# count_val = 0\n",
    "# target_sample_rate_val=16000\n",
    "# duration_val=5.0\n",
    "# # Check if the folder exists\n",
    "# if not os.path.isdir(folder_path_val):\n",
    "#     print(f\"Error: Folder not found at '{folder_path_val}'\")\n",
    "# freq_val = {}\n",
    "# with open(val_file_sorted_path, \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "# # Iterate over all entries in the folder\n",
    "# print(\"Hey User!!, You are now creating the Evaluation set and their corresponding labels\")\n",
    "# for item_name in lines:\n",
    "#     # Construct the full path to the item\n",
    "#     #print(item_name)\n",
    "#     item_path = str(os.path.join(folder_path_val, item_name)).strip('\\n')\n",
    "#     #print(item_path)\n",
    "#     # Check if it's a file and ends with .flac\n",
    "#     # if os.path.isfile(item_path) and item_name.lower().endswith(\".flac\"):\n",
    "#     #     found_flac_files = True\n",
    "#     # print(item_path.split('/'))\n",
    "#     # print(item_name.lower())\n",
    "#     #print(item_name.strip('.flac\\n') in str(list(label_list_val.keys())))\n",
    "#     if item_name.strip('.flac\\n') in str(list(label_list_val.keys())):\n",
    "#         # audio_data_val, sr_val = librosa.load(item_path, sr=target_sample_rate_val, duration=duration_val)\n",
    "#         #print(\"My Name is Mitukk\")\n",
    "#         # print(f\"--- Found FLAC file: {item_name} ---\")\n",
    "#         audio_data_val, sr_val = librosa.load(item_path, sr=target_sample_rate_val, duration=duration_val)\n",
    "#         cqcc_features = feature_extraction_cqcc(audio_data_val, sr_val, n_cqcc=128)\n",
    "#         if cqcc_features is not None:\n",
    "#             count_val = count_val + 1\n",
    "#             padded_cqcc = pad_or_truncate_spectrogram(cqcc_features,157,-80)\n",
    "#             feature_vector_val.append(padded_cqcc)\n",
    "#             print(f\"Added to the list - Shape of CQCC features: {padded_cqcc.shape}\")\n",
    "\n",
    "\n",
    "# # label_list_val, audio_name_list_val = create_label_array(audio_folder_path=audio_folder_path_val, label_file_path=label_file_val)\n",
    "\n",
    "# print(len(label_list_val), len(audio_name_list_val),len(feature_vector_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218161c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a4ef37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabel_array\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_array' is not defined"
     ]
    }
   ],
   "source": [
    "label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111c60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating a lookup dictionary for labels.\n",
      "Found 25380 labels in the protocol file.\n",
      "\n",
      "Step 2: Processing FLAC files in folder: ASVSpoof19/LA/ASVspoof2019_LA_train/flac\n",
      "Extracting features and aligning with labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0%|          | 48/25379 [00:09<22:35, 18.69file/s]  /mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=127\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   0%|          | 51/25379 [00:09<20:58, 20.13file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=121\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   4%|▍         | 1032/25379 [01:03<23:09, 17.53file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=114\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   4%|▍         | 1117/25379 [01:07<21:19, 18.96file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=122\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   5%|▍         | 1180/25379 [01:11<22:19, 18.07file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=103\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   5%|▌         | 1356/25379 [01:20<20:20, 19.69file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=107\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   7%|▋         | 1683/25379 [01:38<22:04, 17.90file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=120\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   7%|▋         | 1686/25379 [01:38<22:10, 17.80file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=87\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   7%|▋         | 1705/25379 [01:39<22:23, 17.63file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=125\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   7%|▋         | 1792/25379 [01:44<22:31, 17.46file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=110\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:   9%|▉         | 2237/25379 [02:08<21:58, 17.56file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=85\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  11%|█         | 2854/25379 [02:41<19:49, 18.93file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=119\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  12%|█▏        | 3032/25379 [02:51<19:44, 18.87file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=126\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  13%|█▎        | 3189/25379 [02:59<19:24, 19.06file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=99\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  13%|█▎        | 3225/25379 [03:01<20:16, 18.21file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=118\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  13%|█▎        | 3403/25379 [03:11<21:41, 16.89file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=102\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  15%|█▍        | 3713/25379 [03:27<18:03, 19.99file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=82\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  16%|█▌        | 4052/25379 [03:45<20:53, 17.01file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=109\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  19%|█▉        | 4834/25379 [04:28<18:32, 18.47file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=124\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  20%|█▉        | 4996/25379 [04:36<17:45, 19.14file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=96\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  20%|█▉        | 5047/25379 [04:39<17:55, 18.90file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=113\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  21%|██        | 5211/25379 [04:48<18:23, 18.28file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=123\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  22%|██▏       | 5582/25379 [05:08<17:25, 18.94file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=88\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  26%|██▌       | 6549/25379 [06:00<18:12, 17.24file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=94\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  30%|██▉       | 7528/25379 [06:53<15:22, 19.36file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=104\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  34%|███▍      | 8581/25379 [07:50<14:18, 19.56file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=100\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  35%|███▍      | 8830/25379 [08:03<14:34, 18.93file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=92\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  42%|████▏     | 10623/25379 [09:41<14:05, 17.45file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=111\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  42%|████▏     | 10779/25379 [09:49<13:54, 17.49file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=117\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  69%|██████▉   | 17540/25379 [15:56<06:54, 18.92file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=98\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  73%|███████▎  | 18557/25379 [16:51<06:44, 16.87file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=95\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  79%|███████▉  | 20128/25379 [18:16<04:42, 18.61file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=89\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  81%|████████  | 20593/25379 [18:42<04:29, 17.75file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=97\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  84%|████████▍ | 21349/25379 [19:22<03:43, 18.02file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=105\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features:  90%|█████████ | 22882/25379 [20:46<02:09, 19.27file/s]/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=128 is too small for input signal of length=106\n",
      "  return f(*args, **kwargs)\n",
      "Extracting Features: 100%|██████████| 25379/25379 [23:01<00:00, 18.37file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Complete ---\n",
      "Number of CQCC feature vectors created: 25379\n",
      "Number of Prosody feature vectors created: 25379\n",
      "Number of corresponding labels found: 25379\n",
      "\n",
      "Successfully saved ALIGNED data:\n",
      "CQCC Features saved to 'cqcc_features_aligned.npy' with shape: (25379, 90, 157)\n",
      "Prosody Features saved to 'prosody_features_aligned.npy' with shape: (25379, 6)\n",
      "Labels saved to 'labels_aligned.npy' with shape: (25379,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "# This script now requires the 'praat-parselmouth' library.\n",
    "# Install it using: pip install praat-parselmouth\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "\n",
    "# --- Feature Extraction Functions ---\n",
    "\n",
    "def feature_extraction_cqcc(audio_data, sr, n_bins=90):\n",
    "    \"\"\"\n",
    "    Placeholder for the CQCC feature extraction function.\n",
    "    In a real scenario, this would compute actual CQCC features.\n",
    "    \"\"\"\n",
    "    # Using librosa's CQT and cepstral mean subtraction as a stand-in for CQCC\n",
    "    cqt = librosa.cqt(y=audio_data, sr=sr, n_bins=n_bins, bins_per_octave=12)\n",
    "    cqt_mag = np.abs(cqt)\n",
    "\n",
    "        # Handle potential zero values before log\n",
    "    cqt_mag[cqt_mag == 0] = np.finfo(float).eps\n",
    "\n",
    "        # 2. Calculate Log Power/Magnitude\n",
    "    log_cqt_mag = np.log(cqt_mag)\n",
    "\n",
    "        # 3. Apply Discrete Cosine Transform (DCT) - Type II is common\n",
    "        # Apply DCT along the frequency axis (axis=0)\n",
    "    cqcc = dct(log_cqt_mag, type=2, axis=0, norm='ortho')\n",
    "\n",
    "        # 4. Keep only the first n_cqcc coefficients\n",
    "    cqcc_truncated = cqcc[:128, :]\n",
    "\n",
    "    return cqcc_truncated\n",
    "\n",
    "def extract_prosodic_features(audio_path):\n",
    "    \"\"\"\n",
    "    Extracts fundamental prosodic features using Parselmouth (Praat).\n",
    "    Features include F0, jitter, shimmer, and HNR.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: A vector of the specified prosodic features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load sound with parselmouth for Praat features\n",
    "        sound = parselmouth.Sound(audio_path)\n",
    "        \n",
    "        # Create a pitch object from the sound\n",
    "        pitch = call(sound, \"To Pitch\", 0.0, 75, 600) # time_step, min_pitch, max_pitch\n",
    "        \n",
    "        # Create a point process from the pitch object for jitter/shimmer calculation\n",
    "        point_process = call(pitch, \"To PointProcess\")\n",
    "        \n",
    "        # Create a harmonicity object for HNR calculation\n",
    "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "        \n",
    "        # Extract the requested features\n",
    "        mean_f0 = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "        std_f0 = call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "        local_jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        local_shimmer = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        mean_hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "        std_hnr = call(harmonicity, \"Get standard deviation\", 0, 0)\n",
    "\n",
    "    except Exception:\n",
    "        # Praat can fail on silent or very noisy audio. Provide default values for all features.\n",
    "        mean_f0, std_f0, local_jitter, local_shimmer, mean_hnr, std_hnr = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Combine all features into a single vector\n",
    "    prosodic_vector = np.array([\n",
    "        mean_f0, std_f0, \n",
    "        local_jitter, local_shimmer, \n",
    "        mean_hnr, std_hnr\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return prosodic_vector\n",
    "\n",
    "\n",
    "def pad_or_truncate_spectrogram(spectrogram, max_time_steps, padding_value=-80.0):\n",
    "    \"\"\"\n",
    "    Pads or truncates a 2D spectrogram to a specified number of time steps (width).\n",
    "\n",
    "    Padding is applied to the end of the time axis (axis 1).\n",
    "    Truncation is performed by taking the initial part of the time axis.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (np.ndarray): The input 2D spectrogram (num_features, num_time_frames).\n",
    "        max_time_steps (int): The target number of time frames (width) for the spectrogram.\n",
    "        padding_value (float, optional): The value to use for padding.\n",
    "                                         Defaults to -80.0 (suitable for dB-scaled log-Mel spectrograms).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The padded or truncated spectrogram of shape (num_features, max_time_steps).\n",
    "    \"\"\"\n",
    "    if not isinstance(spectrogram, np.ndarray) or spectrogram.ndim != 2:\n",
    "        raise ValueError(\"Input spectrogram must be a 2D NumPy array.\")\n",
    "    if not isinstance(max_time_steps, int) or max_time_steps <= 0:\n",
    "        raise ValueError(\"max_time_steps must be a positive integer.\")\n",
    "    \n",
    "    #print(spectrogram.shape)\n",
    "\n",
    "    num_features, current_time_steps = spectrogram.shape\n",
    "    \n",
    "\n",
    "    max_time_steps = 157\n",
    "\n",
    "\n",
    "    if current_time_steps == max_time_steps:\n",
    "        # Already the correct length\n",
    "        return spectrogram\n",
    "    elif current_time_steps < max_time_steps:\n",
    "        # Pad if shorter\n",
    "        pad_width = max_time_steps - current_time_steps\n",
    "        # np.pad takes a list of tuples for padding widths: ((before_axis0, after_axis0), (before_axis1, after_axis1), ...)\n",
    "        # We only want to pad axis 1 (time steps) at the end.\n",
    "        padded_spectrogram = np.pad(\n",
    "            spectrogram,\n",
    "            pad_width=((0, 0), (0, pad_width)), # No padding on feature axis, pad_width at the end of time axis\n",
    "            mode='constant',\n",
    "            constant_values=padding_value\n",
    "        )\n",
    "        return padded_spectrogram\n",
    "    else: # current_time_steps > max_time_steps\n",
    "        # Truncate if longer (take from the beginning of the time axis)\n",
    "        truncated_spectrogram = spectrogram[:, :max_time_steps]\n",
    "        return truncated_spectrogram\n",
    "\n",
    "def create_label_dictionary(label_file_path):\n",
    "    \"\"\"\n",
    "    Reads the ASVspoof protocol file and creates a dictionary mapping\n",
    "    a file's base name to its label (1 for bonafide, 0 for spoof).\n",
    "    \"\"\"\n",
    "    label_dict = {}\n",
    "    with open(label_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                file_basename = parts[1]\n",
    "                label = 0 if parts[-1] == 'bonafide' else 1\n",
    "                label_dict[file_basename] = label\n",
    "    return label_dict\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Configuration\n",
    "cqcc_output_file = \"cqcc_features_aligned.npy\"\n",
    "prosody_output_file = \"prosody_features_aligned.npy\"\n",
    "labels_output_file = \"labels_aligned.npy\"\n",
    "\n",
    "audio_folder_path = \"ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "label_file = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "trn_file_sorted_path = \"trn_file_list.txt\"\n",
    "\n",
    "target_sample_rate = 16000\n",
    "duration = 5.0\n",
    "\n",
    "# # --- Setup Dummy Files and Folders for Demonstration ---\n",
    "# print(\"--- Setting up dummy files and folders for demonstration ---\")\n",
    "# os.makedirs(audio_folder_path, exist_ok=True)\n",
    "# os.makedirs(os.path.dirname(label_file), exist_ok=True)\n",
    "\n",
    "# dummy_labels_content = [\n",
    "#     \"LA_0079 LA_T_1138215 - A01 bonafide\",\n",
    "#     \"LA_0079 LA_T_1234567 - A02 spoof\",\n",
    "#     \"LA_0079 LA_T_7654321 - A03 bonafide\",\n",
    "# ]\n",
    "# with open(label_file, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(dummy_labels_content))\n",
    "\n",
    "# dummy_flac_files = [\"LA_T_1138215.flac\", \"LA_T_1234567.flac\", \"LA_T_7654321.flac\"]\n",
    "# with open(trn_file_sorted_path, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(dummy_flac_files))\n",
    "\n",
    "# for flac_file in dummy_flac_files:\n",
    "#     dummy_audio = np.sin(np.linspace(0, 440 * 2 * np.pi, int(target_sample_rate * 2)))\n",
    "#     sf.write(os.path.join(audio_folder_path, flac_file), dummy_audio, target_sample_rate)\n",
    "# print(\"--- Dummy setup complete ---\\n\")\n",
    "# # --- End of Setup ---\n",
    "\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.isdir(audio_folder_path):\n",
    "    print(f\"Error: Folder not found at '{audio_folder_path}'\")\n",
    "else:\n",
    "    print(\"Step 1: Creating a lookup dictionary for labels.\")\n",
    "    label_dictionary = create_label_dictionary(label_file)\n",
    "    print(f\"Found {len(label_dictionary)} labels in the protocol file.\")\n",
    "\n",
    "    print(f\"\\nStep 2: Processing FLAC files in folder: {audio_folder_path}\")\n",
    "    cqcc_feature_vector = []\n",
    "    prosody_feature_vector = []\n",
    "    label_vector = []\n",
    "\n",
    "    try:\n",
    "        with open(trn_file_sorted_path, \"r\") as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sorted file list not found at '{trn_file_sorted_path}'\")\n",
    "        lines = []\n",
    "\n",
    "    print(\"Extracting features and aligning with labels...\")\n",
    "    for item_name in tqdm(lines, desc=\"Extracting Features\", unit=\"file\"):\n",
    "        base_name = os.path.splitext(item_name)[0]\n",
    "\n",
    "        if base_name in label_dictionary:\n",
    "            item_path = os.path.join(audio_folder_path, item_name)\n",
    "\n",
    "            if os.path.isfile(item_path):\n",
    "                try:\n",
    "                    # Load audio data once for librosa features\n",
    "                    audio_data, sr = librosa.load(item_path, sr=target_sample_rate, duration=duration)\n",
    "                    \n",
    "                    # Extract CQCC features from the loaded audio data\n",
    "                    cqcc_features = feature_extraction_cqcc(audio_data, sr, n_bins=90)\n",
    "                    \n",
    "                    # Extract prosodic features using the file path (for Parselmouth)\n",
    "                    prosody_features = extract_prosodic_features(item_path)\n",
    "\n",
    "                    if cqcc_features is not None and prosody_features is not None:\n",
    "                        # Standardize CQCC shape\n",
    "                        padded_cqcc = pad_or_truncate_spectrogram(cqcc_features, 157, -80)\n",
    "                        \n",
    "                        # Append all features and the label to their respective lists\n",
    "                        cqcc_feature_vector.append(padded_cqcc)\n",
    "                        prosody_feature_vector.append(prosody_features)\n",
    "                        label_vector.append(label_dictionary[base_name])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nWarning: Could not process file {item_path}. Error: {e}\")\n",
    "\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    print(f\"Number of CQCC feature vectors created: {len(cqcc_feature_vector)}\")\n",
    "    print(f\"Number of Prosody feature vectors created: {len(prosody_feature_vector)}\")\n",
    "    print(f\"Number of corresponding labels found: {len(label_vector)}\")\n",
    "\n",
    "    # --- Convert lists to NumPy arrays and save to .npy files ---\n",
    "    if cqcc_feature_vector and label_vector:\n",
    "        cqcc_array = np.array(cqcc_feature_vector, dtype=np.float32)\n",
    "        prosody_array = np.array(prosody_feature_vector, dtype=np.float32)\n",
    "        label_array = np.array(label_vector, dtype=np.int64)\n",
    "\n",
    "        # Save the arrays to .npy files\n",
    "        np.save(cqcc_output_file, cqcc_array)\n",
    "        np.save(prosody_output_file, prosody_array)\n",
    "        np.save(labels_output_file, label_array)\n",
    "\n",
    "        print(f\"\\nSuccessfully saved ALIGNED data:\")\n",
    "        print(f\"CQCC Features saved to '{cqcc_output_file}' with shape: {cqcc_array.shape}\")\n",
    "        print(f\"Prosody Features saved to '{prosody_output_file}' with shape: {prosody_array.shape}\")\n",
    "        print(f\"Labels saved to '{labels_output_file}' with shape: {label_array.shape}\")\n",
    "    else:\n",
    "        print(\"\\nNo features were extracted or labels created. Nothing to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2792200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_vector and labels:\n",
    "    # Convert the feature list to a NumPy array\n",
    "    feature_array = np.array(feature_vector)\n",
    "    # Convert the label list to a NumPy array\n",
    "    label_array = np.array(labels)\n",
    "\n",
    "    # Save the arrays to .npy files\n",
    "    np.save(cqcc_output_file, feature_array)\n",
    "    np.save(labels_output_file, label_array)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved data:\")\n",
    "    print(f\"Features saved to '{cqcc_output_file}' with shape: {feature_array.shape}\")\n",
    "    print(f\"Labels saved to '{labels_output_file}' with shape: {label_array.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo features were extracted or labels created. Nothing to save.\")\n",
    "print(len(label_list), len(audio_name_list),len(feature_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(feature_vector[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# # import cqcc # You may need to install this: pip install cqcc\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import warnings\n",
    "# import parselmouth\n",
    "# from parselmouth.praat import call\n",
    "\n",
    "# # Suppress warnings from librosa and other libraries for a cleaner output\n",
    "# def calculate_prosodic_features(audio_path):\n",
    "#     \"\"\"Calculates prosodic features for a given audio file.\"\"\"\n",
    "#     try:\n",
    "#         sound = parselmouth.Sound(audio_path)\n",
    "#         pitch = sound.to_pitch()\n",
    "#         point_process = call(pitch, \"To PointProcess\")\n",
    "#         harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "        \n",
    "#         features = {\n",
    "#             'mean_f0': call(pitch, \"Get mean\", 0, 0, \"Hertz\"),\n",
    "#             'std_f0': call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\"),\n",
    "#             'jitter': call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3),\n",
    "#             'shimmer': call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6),\n",
    "#             'mean_hnr': call(harmonicity, \"Get mean\", 0, 0),\n",
    "#             'std_hnr': call(harmonicity, \"Get standard deviation\", 0, 0)\n",
    "#         }\n",
    "        \n",
    "#         # Replace NaN values with 0.0 for model compatibility\n",
    "#         for key, value in features.items():\n",
    "#             if isinstance(value, float) and np.isnan(value):\n",
    "#                 features[key] = 0.0\n",
    "#         return features\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def extract_cqcc_features(audio, sr, n_cqcc=20):\n",
    "#     \"\"\"Extracts Constant Q-Cepstral Coefficients (CQCC).\"\"\"\n",
    "#     try:\n",
    "#         features = cqcc.cqcc(audio, sr, n_cqcc=n_cqcc)\n",
    "#         # Return the features averaged over the time axis for a fixed-size vector\n",
    "#         return np.mean(features, axis=1)\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def process_all_features(bonafide_dirs, spoof_dir, protocol_file, output_path):\n",
    "#     \"\"\"\n",
    "#     Processes all audio files to extract both CQCC and prosodic features in a single loop,\n",
    "#     ensuring perfect alignment.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Unified Feature Extraction Process ---\")\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     # 1. Load protocol file to map filenames to attack IDs\n",
    "#     try:\n",
    "#         protocol_df = pd.read_csv(protocol_file, sep=\" \", header=None, names=['speaker_id', 'filename', 'attack_type', 'system_id', 'label'])\n",
    "#         attack_id_map = pd.Series(protocol_df.system_id.values, index=protocol_df.filename).to_dict()\n",
    "#         print(\"Successfully loaded protocol file for Attack ID mapping.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Protocol file not found at {protocol_file}. Cannot map attack IDs.\")\n",
    "#         return\n",
    "\n",
    "#     # 2. Gather all file paths and assign preliminary labels\n",
    "#     all_files_to_process = []\n",
    "#     for directory in bonafide_dirs:\n",
    "#         try:\n",
    "#             files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.flac', '.wav'))]\n",
    "#             for f_path in files:\n",
    "#                 all_files_to_process.append({'filepath': f_path, 'label': 0}) # 0 for bonafide\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Warning: Directory not found: {directory}. Skipping.\")\n",
    "    \n",
    "#     try:\n",
    "#         files = [os.path.join(spoof_dir, f) for f in os.listdir(spoof_dir) if f.endswith(('.flac', '.wav'))]\n",
    "#         for f_path in files:\n",
    "#             all_files_to_process.append({'filepath': f_path, 'label': 1}) # 1 for spoof\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: Directory not found: {spoof_dir}. Skipping.\")\n",
    "\n",
    "#     print(f\"Found {len(all_files_to_process)} total audio files to process.\")\n",
    "\n",
    "#     # 3. Main processing loop\n",
    "#     cqcc_list = []\n",
    "#     prosodic_list = []\n",
    "\n",
    "#     for file_info in tqdm(all_files_to_process, desc=\"Extracting All Features\"):\n",
    "#         filepath = file_info['filepath']\n",
    "#         label = file_info['label']\n",
    "        \n",
    "#         # Load audio once\n",
    "#         try:\n",
    "#             audio, sr = librosa.load(filepath, sr=16000, duration = 5.0) # Use a fixed sample rate\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError loading {filepath}: {e}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         cqcc_feats = feature_extraction_cqcc(audio, sr, n_cqcc=128)\n",
    "#         prosodic_feats = calculate_prosodic_features(filepath)\n",
    "\n",
    "#         # Only add the sample if both feature extractions were successful\n",
    "#         if cqcc_feats is not None and prosodic_feats is not None:\n",
    "#             # Get metadata\n",
    "#             base_filename = os.path.basename(filepath)\n",
    "#             if base_filename.startswith('aug_'):\n",
    "#                 original_filename_key = '_'.join(base_filename.split('_')[2:]).replace('.flac', '')\n",
    "#             else:\n",
    "#                 original_filename_key = base_filename.replace('.flac', '')\n",
    "            \n",
    "#             attack_id = attack_id_map.get(original_filename_key, '-')\n",
    "#             padded_cqcc = pad_or_truncate_spectrogram(cqcc_feats, 157, -80)\n",
    "            \n",
    "#             # Append data to lists\n",
    "#             cqcc_list.append(padded_cqcc)\n",
    "\n",
    "            \n",
    "#             prosodic_data_row = {\n",
    "#                 'filename': base_filename,\n",
    "#                 'label': label,\n",
    "#                 'attack_id': attack_id,\n",
    "#                 **prosodic_feats\n",
    "#             }\n",
    "#             prosodic_list.append(prosodic_data_row)\n",
    "\n",
    "#     # 4. Save the processed and aligned data\n",
    "#     if not prosodic_list:\n",
    "#         print(\"No features were extracted. Please check your directories and files.\")\n",
    "#         return\n",
    "\n",
    "#     # Convert to final formats\n",
    "#     X_cqcc = np.array(cqcc_list)\n",
    "#     prosody_df = pd.DataFrame(prosodic_list)\n",
    "\n",
    "#     # Define output paths\n",
    "#     cqcc_save_path = os.path.join(output_path, \"cqcc_features.npy\")\n",
    "#     prosody_csv_save_path = os.path.join(output_path, \"prosodic_features_and_labels.csv\")\n",
    "\n",
    "#     # Save files\n",
    "#     np.save(cqcc_save_path, X_cqcc)\n",
    "#     prosody_df.to_csv(prosody_csv_save_path, index=False)\n",
    "\n",
    "#     print(f\"\\n--- Unified Feature Extraction Complete ---\")\n",
    "#     print(f\"Processed and aligned {len(prosody_df)} files successfully.\")\n",
    "#     print(f\"CQCC features saved to: {cqcc_save_path} with shape {X_cqcc.shape}\")\n",
    "#     print(f\"Prosodic features and labels saved to: {prosody_csv_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424be0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONAFIDE_DIRS = [\n",
    "#         \"bonafide_audio_train\",            # Original bonafide files\n",
    "#         \"augmented_bonafide\"   # Augmented bonafide files\n",
    "#     ]\n",
    "# SPOOF_AUDIO_DIR = \"spoof_audio_train\"\n",
    "    \n",
    "#     # This is where the final .npy files will be saved.\n",
    "# OUTPUT_DIR = \"processed_data\"\n",
    "\n",
    "#     # Set 'max_files_per_class' to a small number (e.g., 100) for a quick test run.\n",
    "#     # Set to 'None' to process all files.\n",
    "\n",
    "\n",
    "# PROTOCOL_FILE = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "# process_all_features(\n",
    "#     bonafide_dirs=BONAFIDE_DIRS,\n",
    "#     spoof_dir=SPOOF_AUDIO_DIR,\n",
    "#     protocol_file=PROTOCOL_FILE,\n",
    "#     output_path=OUTPUT_DIR\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a637d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# # import cqcc # You may need to install this: pip install cqcc\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import warnings\n",
    "# import parselmouth\n",
    "# from parselmouth.praat import call\n",
    "\n",
    "# # Suppress warnings from librosa and other libraries for a cleaner output\n",
    "# def calculate_prosodic_features(audio_path):\n",
    "#     \"\"\"Calculates prosodic features for a given audio file.\"\"\"\n",
    "#     try:\n",
    "#         sound = parselmouth.Sound(audio_path)\n",
    "#         pitch = sound.to_pitch()\n",
    "#         point_process = call(pitch, \"To PointProcess\")\n",
    "#         harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "        \n",
    "#         features = {\n",
    "#             'mean_f0': call(pitch, \"Get mean\", 0, 0, \"Hertz\"),\n",
    "#             'std_f0': call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\"),\n",
    "#             'jitter': call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3),\n",
    "#             'shimmer': call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6),\n",
    "#             'mean_hnr': call(harmonicity, \"Get mean\", 0, 0),\n",
    "#             'std_hnr': call(harmonicity, \"Get standard deviation\", 0, 0)\n",
    "#         }\n",
    "        \n",
    "#         # Replace NaN values with 0.0 for model compatibility\n",
    "#         for key, value in features.items():\n",
    "#             if isinstance(value, float) and np.isnan(value):\n",
    "#                 features[key] = 0.0\n",
    "#         return features\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def extract_cqcc_features(audio, sr, n_cqcc=20):\n",
    "#     \"\"\"Extracts Constant Q-Cepstral Coefficients (CQCC).\"\"\"\n",
    "#     try:\n",
    "#         features = cqcc.cqcc(audio, sr, n_cqcc=n_cqcc)\n",
    "#         # Return the features averaged over the time axis for a fixed-size vector\n",
    "#         return np.mean(features, axis=1)\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def process_all_features(bonafide_dirs, spoof_dir, protocol_file, output_path):\n",
    "#     \"\"\"\n",
    "#     Processes all audio files to extract both CQCC and prosodic features in a single loop,\n",
    "#     ensuring perfect alignment.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Unified Feature Extraction Process ---\")\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     # 1. Load protocol file to map filenames to attack IDs\n",
    "#     try:\n",
    "#         protocol_df = pd.read_csv(protocol_file, sep=\" \", header=None, names=['speaker_id', 'filename', 'attack_type', 'system_id', 'label'])\n",
    "#         attack_id_map = pd.Series(protocol_df.system_id.values, index=protocol_df.filename).to_dict()\n",
    "#         print(\"Successfully loaded protocol file for Attack ID mapping.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Protocol file not found at {protocol_file}. Cannot map attack IDs.\")\n",
    "#         return\n",
    "\n",
    "#     # 2. Gather all file paths and assign preliminary labels\n",
    "#     all_files_to_process = []\n",
    "#     for directory in bonafide_dirs:\n",
    "#         try:\n",
    "#             files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.flac', '.wav'))]\n",
    "#             for f_path in files:\n",
    "#                 all_files_to_process.append({'filepath': f_path, 'label': 0}) # 0 for bonafide\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Warning: Directory not found: {directory}. Skipping.\")\n",
    "    \n",
    "#     try:\n",
    "#         files = [os.path.join(spoof_dir, f) for f in os.listdir(spoof_dir) if f.endswith(('.flac', '.wav'))]\n",
    "#         for f_path in files:\n",
    "#             all_files_to_process.append({'filepath': f_path, 'label': 1}) # 1 for spoof\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: Directory not found: {spoof_dir}. Skipping.\")\n",
    "\n",
    "#     print(f\"Found {len(all_files_to_process)} total audio files to process.\")\n",
    "\n",
    "#     # 3. Main processing loop\n",
    "#     cqcc_list = []\n",
    "#     prosodic_list = []\n",
    "\n",
    "#     for file_info in tqdm(all_files_to_process, desc=\"Extracting All Features\"):\n",
    "#         filepath = file_info['filepath']\n",
    "#         label = file_info['label']\n",
    "        \n",
    "#         # Load audio once\n",
    "#         try:\n",
    "#             audio, sr = librosa.load(filepath, sr=16000, duration = 5.0) # Use a fixed sample rate\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError loading {filepath}: {e}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         cqcc_feats = feature_extraction_cqcc(audio, sr, n_cqcc=128)\n",
    "#         prosodic_feats = calculate_prosodic_features(filepath)\n",
    "\n",
    "#         # Only add the sample if both feature extractions were successful\n",
    "#         if cqcc_feats is not None and prosodic_feats is not None:\n",
    "#             # Get metadata\n",
    "#             base_filename = os.path.basename(filepath)\n",
    "#             if base_filename.startswith('aug_'):\n",
    "#                 original_filename_key = '_'.join(base_filename.split('_')[2:]).replace('.flac', '')\n",
    "#             else:\n",
    "#                 original_filename_key = base_filename.replace('.flac', '')\n",
    "            \n",
    "#             attack_id = attack_id_map.get(original_filename_key, '-')\n",
    "#             padded_cqcc = pad_or_truncate_spectrogram(cqcc_feats, 157, -80)\n",
    "            \n",
    "#             # Append data to lists\n",
    "#             cqcc_list.append(padded_cqcc)\n",
    "\n",
    "            \n",
    "#             prosodic_data_row = {\n",
    "#                 'filename': base_filename,\n",
    "#                 'label': label,\n",
    "#                 'attack_id': attack_id,\n",
    "#                 **prosodic_feats\n",
    "#             }\n",
    "#             prosodic_list.append(prosodic_data_row)\n",
    "\n",
    "#     # 4. Save the processed and aligned data\n",
    "#     if not prosodic_list:\n",
    "#         print(\"No features were extracted. Please check your directories and files.\")\n",
    "#         return\n",
    "\n",
    "#     # Convert to final formats\n",
    "#     X_cqcc = np.array(cqcc_list)\n",
    "#     prosody_df = pd.DataFrame(prosodic_list)\n",
    "\n",
    "#     # Define output paths\n",
    "#     cqcc_save_path = os.path.join(output_path, \"cqcc_features_test.npy\")\n",
    "#     prosody_csv_save_path = os.path.join(output_path, \"prosodic_features_and_labels_test.csv\")\n",
    "\n",
    "#     # Save files\n",
    "#     np.save(cqcc_save_path, X_cqcc)\n",
    "#     prosody_df.to_csv(prosody_csv_save_path, index=False)\n",
    "\n",
    "#     print(f\"\\n--- Unified Feature Extraction Complete ---\")\n",
    "#     print(f\"Processed and aligned {len(prosody_df)} files successfully.\")\n",
    "#     print(f\"CQCC features saved to: {cqcc_save_path} with shape {X_cqcc.shape}\")\n",
    "#     print(f\"Prosodic features and labels saved to: {prosody_csv_save_path}\")\n",
    "\n",
    "# BONAFIDE_DIRS = [\n",
    "#         \"bonafide_audio_test\",            # Original bonafide files \n",
    "#     ]\n",
    "# SPOOF_AUDIO_DIR = \"spoof_audio_test\"\n",
    "    \n",
    "#     # This is where the final .npy files will be saved.\n",
    "# OUTPUT_DIR = \"processed_data\"\n",
    "\n",
    "#     # Set 'max_files_per_class' to a small number (e.g., 100) for a quick test run.\n",
    "#     # Set to 'None' to process all files.\n",
    "\n",
    "\n",
    "# PROTOCOL_FILE = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "\n",
    "# process_all_features(\n",
    "#     bonafide_dirs=BONAFIDE_DIRS,\n",
    "#     spoof_dir=SPOOF_AUDIO_DIR,\n",
    "#     protocol_file=PROTOCOL_FILE,\n",
    "#     output_path=OUTPUT_DIR\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ACHTUNG FOR Pre-Processing for Development ONLY ---\n",
    "# your_folder_with_flac_files_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_dev/flac\"\n",
    "# audio_folder_path_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_dev/flac\"\n",
    "# label_file_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\" \n",
    "# folder_path_dev = your_folder_with_flac_files_dev\n",
    "# dev_file_sorted_path= \"dev_file_list.txt\"\n",
    "# print(f\"Processing FLAC files in folder: {folder_path_dev}\\n\")\n",
    "# label_list_dev, audio_name_list_dev = create_label_array(audio_folder_path=audio_folder_path_dev, label_file_path=label_file_dev)\n",
    "# sorted_labels_dev = sorted(label_list_dev.items())\n",
    "# labels_dev= []\n",
    "# for i in range(0,len(sorted_labels_dev)):\n",
    "#     labels_dev.append(sorted_labels_dev[i][1])\n",
    "# labels_dev = np.array(labels_dev)\n",
    "# # print(len(label_list_val), len(audio_name_list_val),len(feature_vector_val))\n",
    "# feature_vector_dev = []\n",
    "# found_flac_files_dev = False\n",
    "# count_dev = 0\n",
    "# target_sample_rate_dev=16000\n",
    "# duration_dev=5.0\n",
    "# # Check if the folder exists\n",
    "# if not os.path.isdir(folder_path_dev):\n",
    "#     print(f\"Error: Folder not found at '{folder_path_dev}'\")\n",
    "# freq_dev = {}\n",
    "# with open(dev_file_sorted_path, \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "# # Iterate over all entries in the folder\n",
    "# print(\"Hey User!!, You are now creating the Development set and their corresponding labels\")\n",
    "# for item_name in lines:\n",
    "#     # Construct the full path to the item\n",
    "#     #print(item_name)\n",
    "#     item_path = str(os.path.join(folder_path_dev, item_name)).strip('\\n')\n",
    "#     #print(item_path)\n",
    "#     # Check if it's a file and ends with .flac\n",
    "#     # if os.path.isfile(item_path) and item_name.lower().endswith(\".flac\"):\n",
    "#     #     found_flac_files = True\n",
    "#     # print(item_path.split('/'))\n",
    "#     # print(item_name.lower())\n",
    "#     #print(item_name.strip('.flac\\n') in str(list(label_list_val.keys())))\n",
    "#     if item_name.strip('.flac\\n') in str(list(label_list_dev.keys())):\n",
    "#         # audio_data_val, sr_val = librosa.load(item_path, sr=target_sample_rate_val, duration=duration_val)\n",
    "#         #print(\"My Name is Mitukk\")\n",
    "#         # print(f\"--- Found FLAC file: {item_name} ---\")\n",
    "#         audio_data_dev, sr_dev = librosa.load(item_path, sr=target_sample_rate_dev, duration=duration_dev)\n",
    "#         cqcc_features = feature_extraction_cqcc(audio_data_dev, sr_dev, n_cqcc=128)\n",
    "#         if cqcc_features is not None:\n",
    "#             count_dev = count_dev + 1\n",
    "#             padded_cqcc = pad_or_truncate_spectrogram(cqcc_features,157,-80)\n",
    "#             feature_vector_dev.append(padded_cqcc)\n",
    "#             print(f\"Added to the list - Shape of CQCC features: {padded_cqcc.shape}\")\n",
    "\n",
    "\n",
    "# # label_list_val, audio_name_list_val = create_label_array(audio_folder_path=audio_folder_path_val, label_file_path=label_file_val)\n",
    "\n",
    "# print(len(label_list_dev), len(audio_name_list_dev),len(feature_vector_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # --- ACHTUNG FOR Pre-Processing for Development ONLY ---\n",
    "\n",
    "# # Assuming feature_extraction_cqcc, pad_or_truncate_spectrogram, and create_label_array are defined elsewhere\n",
    "\n",
    "# # --- Configuration ---\n",
    "# your_folder_with_flac_files_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_dev/flac\"\n",
    "# audio_folder_path_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_dev/flac\"\n",
    "# label_file_dev = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "# folder_path_dev = your_folder_with_flac_files_dev\n",
    "# dev_file_sorted_path = \"dev_file_list.txt\"\n",
    "\n",
    "# # --- Feature Extraction Parameters ---\n",
    "# target_sample_rate_dev = 16000\n",
    "# duration_dev = 5.0\n",
    "\n",
    "# print(f\"Processing FLAC files in folder: {folder_path_dev}\\n\")\n",
    "\n",
    "# # --- Label Loading and Sorting ---\n",
    "# label_list_dev, audio_name_list_dev = create_label_array(audio_folder_path=audio_folder_path_dev, label_file_path=label_file_dev)\n",
    "# sorted_labels_dev = sorted(label_list_dev.items())\n",
    "# labels_dev = np.array([label for _, label in sorted_labels_dev])\n",
    "# label_keys_dev = set(label_list_dev.keys()) # Use a set for faster lookups\n",
    "\n",
    "# # --- Feature Extraction ---\n",
    "# feature_vector_dev = []\n",
    "\n",
    "# # Check if the folder exists\n",
    "# if not os.path.isdir(folder_path_dev):\n",
    "#     print(f\"Error: Folder not found at '{folder_path_dev}'\")\n",
    "# else:\n",
    "#     with open(dev_file_sorted_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     print(\"Hey User!!, You are now creating the Development set and their corresponding labels\")\n",
    "#     # Use tqdm for a progress bar over the file list\n",
    "#     for item_name in tqdm(lines, desc=\"Processing Dev Files\", unit=\"file\"):\n",
    "#         item_name_stripped = item_name.strip()\n",
    "#         item_path = os.path.join(folder_path_dev, item_name_stripped)\n",
    "        \n",
    "#         # Check if the file's key exists in the loaded labels\n",
    "#         if item_name_stripped.replace('.flac', '') in label_keys_dev:\n",
    "#             # Load audio file\n",
    "#             audio_data_dev, sr_dev = librosa.load(item_path, sr=target_sample_rate_dev, duration=duration_dev)\n",
    "            \n",
    "#             # Extract features\n",
    "#             cqcc_features = feature_extraction_cqcc(audio_data_dev, sr_dev, n_cqcc=128)\n",
    "            \n",
    "#             if cqcc_features is not None:\n",
    "#                 # Pad or truncate features to a fixed size\n",
    "#                 padded_cqcc = pad_or_truncate_spectrogram(cqcc_features, 157, -80)\n",
    "#                 feature_vector_dev.append(padded_cqcc)\n",
    "\n",
    "# # --- Final Summary ---\n",
    "# print(\"\\n--- Development Set Processing Complete ---\")\n",
    "# print(f\"Labels found: {len(label_list_dev)}\")\n",
    "# print(f\"Audio files listed in protocol: {len(audio_name_list_dev)}\")\n",
    "# print(f\"Feature vectors extracted: {len(feature_vector_dev)}\")\n",
    "\n",
    "# feature_vector_dev = np.array(feature_vector_dev, dtype=np.float32)\n",
    "# feature_vector_dev.shape\n",
    "# labels_dev = np.array(labels_dev, dtype = np.int64 )\n",
    "# labels_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f53318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('feature_vector_dev', feature_vector_dev)\n",
    "# np.save('labels_dev', labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db440cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58eb486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261892f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aa54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     your_folder_with_flac_files = \"ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "#     audio_folder_path = \"ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "#     label_file = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\" \n",
    "#     folder_path = your_folder_with_flac_files\n",
    "#     trn_file_sorted_path= \"trn_file_list.txt\"\n",
    "#     print(f\"Processing FLAC files in folder: {folder_path}\\n\")\n",
    "#     feature_vector = []\n",
    "#     found_flac_files = False\n",
    "#     count = 0\n",
    "#     target_sample_rate=16000\n",
    "#     duration=5.0\n",
    "#     # Example usage:\n",
    "#     # 1. Load files at their original sampling rate and full duration\n",
    "#     #process_flac_files_in_folder(your_folder_with_flac_files)\n",
    "\n",
    "#     # 2. Load files, resample to 16000 Hz, and load max 5 seconds\n",
    "#     #process_flac_files_in_folder(your_folder_with_flac_files, target_sample_rate=16000, duration=5.0)\n",
    "\n",
    "#     # --- For the code to run, uncomment one of the above examples and set the path ---\n",
    "#     # --- For demonstration, let's create a dummy folder and file if the path is not set\n",
    "\n",
    "#     # ACHTUNG !!!!!!!!  BELOW CODE IS FOR MFCC ONLY\n",
    "\n",
    "#     if your_folder_with_flac_files == \"path/to/your/flac_files_folder\":\n",
    "#         print(\"INFO: 'your_folder_with_flac_files' is not set to a real path.\")\n",
    "#         print(\"Please update the 'your_folder_with_flac_files' variable in the script.\")\n",
    "#         # Create a dummy folder and a dummy flac file for demonstration if it doesn't exist\n",
    "#         dummy_folder = \"dummy_flac_folder\"\n",
    "#         if not os.path.exists(dummy_folder):\n",
    "#             os.makedirs(dummy_folder)\n",
    "#             # Create a very short, silent dummy FLAC file (requires soundfile and numpy)\n",
    "#             try:\n",
    "#                 import soundfile as sf\n",
    "#                 import numpy as np\n",
    "#                 dummy_file_path = os.path.join(dummy_folder, \"dummy_audio.flac\")\n",
    "#                 sf.write(dummy_file_path, np.zeros(16000), 16000, format='FLAC') # 1 sec silence\n",
    "#                 print(f\"Created a dummy folder '{dummy_folder}' with 'dummy_audio.flac' for testing.\")\n",
    "#                 print(\"Running with the dummy folder:\")\n",
    "#                 process_flac_files_in_folder(dummy_folder, target_sample_rate=16000)\n",
    "#             except ImportError:\n",
    "#                 print(\"Skipping dummy file creation: soundfile or numpy not installed.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error creating dummy file: {e}\")\n",
    "#     else:\n",
    "#         # If the user has set a path, use it\n",
    "#         feature_vector_padded = process_flac_files_in_folder(feature_vector, your_folder_with_flac_files, target_sample_rate=16000, duration=5.0)\n",
    "#         # print(count)\n",
    "#         # print(freq)\n",
    "#         widths = sorted(freq.keys())\n",
    "#         counts = [freq[w] for w in widths]\n",
    "#         # print(widths)\n",
    "#         # print(counts)\n",
    "#         print(feature_vector_padded.shape)\n",
    "\n",
    "#     # ACHTUNG !!!!! COMMON PLOT FOR ALL THE FEATURES\n",
    "#     # Create the bar plot\n",
    "#     # plt.figure(figsize=(12, 6)) # Adjust figure size as needed\n",
    "#     # bars = plt.bar(widths, counts, color='skyblue', edgecolor='black', width=1.0) # width=1.0 makes bars touch for histogram feel\n",
    "#     # # Add labels and title\n",
    "#     # plt.xlabel(\"Spectrogram Width (Number of Time Frames)\")\n",
    "#     # plt.ylabel(\"Frequency (Count)\")\n",
    "#     # plt.title(\"Time Frame Distribution\")\n",
    "#     # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "#     # plt.savefig(\"Freq Distribution.png\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a53c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_vector_padded= np.array(feature_vector_padded)\n",
    "# feature_vector_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "655276f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttentionFusion(nn.Module):\n",
    "#     def __init__(self, cqcc_dim=128, prosodic_dim=32, hidden_dim=64):\n",
    "#         super().__init__()\n",
    "#         # Layer to project CQCC features to the hidden dimension\n",
    "#         self.cqcc_projection = nn.Linear(cqcc_dim, hidden_dim)\n",
    "#         # Layer to project prosodic features (the context) to the hidden dimension\n",
    "#         self.prosodic_projection = nn.Linear(prosodic_dim, hidden_dim)\n",
    "#         # Final layer to compute attention scores\n",
    "#         self.attention_scores = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, cqcc_features, prosodic_features):\n",
    "#         # Project both feature sets\n",
    "#         cqcc_proj = torch.tanh(self.cqcc_projection(cqcc_features))\n",
    "#         prosodic_proj = torch.tanh(self.prosodic_projection(prosodic_features))\n",
    "\n",
    "#         # Calculate attention weights based on the combined projected features\n",
    "#         # The prosodic features act as a query on the cqcc features\n",
    "#         attention_input = cqcc_proj * prosodic_proj\n",
    "#         attention_weights = F.softmax(self.attention_scores(attention_input), dim=1)\n",
    "\n",
    "#         # Apply the attention weights to the original CQCC features\n",
    "#         refined_cqcc = cqcc_features * attention_weights\n",
    "        \n",
    "#         # Fuse by concatenation, now with an attention-refined CQCC vector\n",
    "#         fused_vector = torch.cat([refined_cqcc, prosodic_features], dim=1)\n",
    "#         return fused_vector\n",
    "    \n",
    "\n",
    "# class AttentionFusionCNN(nn.Module):\n",
    "#     def __init__(self, input_features, input_time_steps, num_classes=2):\n",
    "#         super().__init__()\n",
    "#         # --- CNN Branch for CQCC features (unchanged) ---\n",
    "#         self.CNN = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=(1,1)),\n",
    "#             nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "#             nn.Conv2d(16, 32, kernel_size=(3,3), stride=(1,1)),\n",
    "#             nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "#             nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1)),\n",
    "#             nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2))\n",
    "#         )\n",
    "#         # Dynamic shape calculation (unchanged)\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.randn(1, 1, input_features, input_time_steps)\n",
    "#             dummy_output = self.CNN(dummy_input)\n",
    "#             self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "        \n",
    "#         self.flatten = nn.Flatten()\n",
    "        \n",
    "#         # --- Processing Layers for each branch (unchanged) ---\n",
    "#         self.LinearLayer = nn.Sequential(\n",
    "#             nn.Linear(self.flattened_size, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5), # Increased dropout for regularization\n",
    "#             nn.Linear(128, 128)\n",
    "#         )\n",
    "#         self.ProsodicLayer = nn.Sequential(\n",
    "#             nn.Linear(6, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 32)\n",
    "#         )\n",
    "\n",
    "#         # --- NEW: Instantiate the Attention Fusion layer ---\n",
    "#         self.attention_fusion = AttentionFusion()\n",
    "\n",
    "#         # --- Final Classifier (fusionLayer) ---\n",
    "#         # The input size is still 128 + 32 = 160, because the attention\n",
    "#         # layer refines the CQCC vector but doesn't change its dimension.\n",
    "#         self.fusionLayer = nn.Sequential(\n",
    "#             nn.Linear(160, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         # 1. Process CQCC features through the CNN branch\n",
    "#         x = x.unsqueeze(1)\n",
    "#         x = self.CNN(x)\n",
    "#         flattened_x = self.flatten(x)\n",
    "#         cqcc_embedding = self.LinearLayer(flattened_x)\n",
    "\n",
    "#         # 2. Process prosodic features through the MLP branch\n",
    "#         prosodic_embedding = self.ProsodicLayer(y)\n",
    "\n",
    "#         # 3. Fuse the embeddings using the Attention mechanism\n",
    "#         #    This replaces the simple torch.cat()\n",
    "#         fused_vector = self.attention_fusion(cqcc_embedding, prosodic_embedding)\n",
    "\n",
    "#         # 4. Pass the fused vector to the final classifier\n",
    "#         logits = self.fusionLayer(fused_vector)\n",
    "        \n",
    "#         return logits\n",
    "    \n",
    "\n",
    "#CNN Network Architecture\n",
    "class CNNnetwork(nn.Module):\n",
    "    def __init__(self, input_features, input_time_steps, num_classes):\n",
    "        super().__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(16,32, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, input_features, input_time_steps)\n",
    "            dummy_output = self.CNN(dummy_input)\n",
    "            self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        ##USE CROSS ATTENTION TO FUSE DIFFERENT DOMAINS(Prosodic and Highlevel CQCCs)\n",
    "        self.LinearLayer = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.ProsodicLayer = nn.Sequential(\n",
    "            nn.Linear(6, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32)\n",
    "        )\n",
    "\n",
    "        self.fusionLayer = nn.Sequential(\n",
    "            nn.Linear(160,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(x.shape)\n",
    "        x = self.CNN(x)\n",
    "        flattened_x = self.flatten(x)\n",
    "        output_x = self.LinearLayer(flattened_x)\n",
    "        y = self.ProsodicLayer(y)\n",
    "        logits = self.fusionLayer(torch.cat([output_x, y], dim=1))\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dac5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class AttentionFusion(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A layer that uses attention to fuse two feature vectors (e.g., from an LSTM and an MLP).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, primary_dim, context_dim, hidden_dim=64):\n",
    "#         super().__init__()\n",
    "#         # Layers to project the features into a common space\n",
    "#         self.primary_projection = nn.Linear(primary_dim, hidden_dim)\n",
    "#         self.context_projection = nn.Linear(context_dim, hidden_dim)\n",
    "\n",
    "#         # Final layer to compute the attention scores from the combined projection\n",
    "#         self.attention_scores = nn.Linear(hidden_dim, primary_dim)\n",
    "\n",
    "#     def forward(self, primary_features, context_features):\n",
    "#         # primary_features (e.g., CQCC embedding): [batch_size, primary_dim]\n",
    "#         # context_features (e.g., Prosodic embedding): [batch_size, context_dim]\n",
    "\n",
    "#         # Project both features into the hidden dimension\n",
    "#         primary_proj = torch.tanh(self.primary_projection(primary_features))\n",
    "#         context_proj = torch.tanh(self.context_projection(context_features))\n",
    "\n",
    "#         # Element-wise product of the projections to model their interaction\n",
    "#         interaction = primary_proj * context_proj\n",
    "\n",
    "#         # Compute attention weights from the interaction\n",
    "#         attention_weights = torch.sigmoid(self.attention_scores(interaction))\n",
    "\n",
    "#         # Apply the learned attention weights to the original primary features\n",
    "#         refined_primary = primary_features * attention_weights\n",
    "\n",
    "#         # Concatenate the attention-refined features with the context features\n",
    "#         fused_vector = torch.cat([refined_primary, context_features], dim=1)\n",
    "        \n",
    "#         return fused_vector\n",
    "    \n",
    "# class LSTMAttentionFusionNetwork(nn.Module):\n",
    "#     def __init__(self, input_features, num_classes, lstm_hidden_size=128, lstm_layers=2):\n",
    "#         super().__init__()\n",
    "#         # --- LSTM Branch for CQCC features (Unchanged) ---\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=input_features, # 90\n",
    "#             hidden_size=lstm_hidden_size,\n",
    "#             num_layers=lstm_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.3 if lstm_layers > 1 else 0 # Add dropout between LSTM layers\n",
    "#         )\n",
    "#         lstm_output_dim = lstm_hidden_size * 2\n",
    "\n",
    "#         #  MLP Branch for Prosodic features\n",
    "#         self.ProsodicLayer = nn.Sequential(\n",
    "#             nn.Linear(6, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 32)\n",
    "#         )\n",
    "        \n",
    "#         # Attention Fusion layer ---\n",
    "#         self.attention_fusion = AttentionFusion(\n",
    "#             primary_dim=lstm_output_dim, # 256\n",
    "#             context_dim=32\n",
    "#         )\n",
    "        \n",
    "       \n",
    "#         self.fusionLayer = nn.Sequential(\n",
    "#             nn.Linear(lstm_output_dim + 32, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         # 1. Permute CQCC data for LSTM: (batch, features, time) -> (batch, time, features)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "\n",
    "#         # 2. Process CQCC features through LSTM\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "\n",
    "#         # 3. Create CQCC embedding from the final hidden states of the BiLSTM\n",
    "#         cqcc_embedding = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "#         # 4. Process prosodic features\n",
    "#         prosodic_embedding = self.ProsodicLayer(y)\n",
    "\n",
    "#         # 5. Fuse the embeddings using the Attention mechanism\n",
    "#         #    This is the only change in the forward pass!\n",
    "#         fused_vector = self.attention_fusion(\n",
    "#             primary_features=cqcc_embedding,\n",
    "#             context_features=prosodic_embedding\n",
    "#         )\n",
    "\n",
    "#         # 6. Pass the attention-fused vector to the final classifier\n",
    "#         logits = self.fusionLayer(fused_vector)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a1ef011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Calculates the Equal Error Rate (EER).\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True binary labels (0 for spoof, 1 for bona fide).\n",
    "        y_scores (np.ndarray): Scores for the positive class (bona fide).\n",
    "                               Higher scores indicate a higher likelihood of being bona fide.\n",
    "    Returns:\n",
    "        float: The EER value. Returns NaN if EER cannot be computed.\n",
    "    \"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"Warning: EER calculation requires at least two classes in y_true.\")\n",
    "        return float('nan')\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # Find the point where abs(fpr - fnr) is minimal\n",
    "    # This is a common way to approximate EER from ROC curve points\n",
    "    # eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
    "    # eer = (fpr[eer_index] + fnr[eer_index]) / 2.0\n",
    "    \n",
    "    # More precise EER can be found using interpolation if needed, e.g., with scipy.optimize.brentq\n",
    "    # from scipy.optimize import brentq\n",
    "    # from scipy.interpolate import interp1d\n",
    "    try:\n",
    "        interp_fn = interp1d(fpr,fnr)\n",
    "        eer = brentq(lambda x : x  - interp_fn(x), 0.0, 1.0)\n",
    "    except Exception as e: # Fallback if interpolation fails (e.g. NaNs in fpr/tpr, or no intersection)\n",
    "        print(f\"Warning: brentq EER calculation failed ({e}), using approximation.\")\n",
    "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
    "        eer = (fpr[eer_index] + fnr[eer_index]) / 2.0\n",
    "\n",
    "\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f53a19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prosodicFeatures = pd.read_csv('prosodic_features_train.csv')\n",
    "# prosodicFeatures = pd.DataFrame(prosodicFeatures)\n",
    "# prosodicFeatures.tail\n",
    "# prosodicFeatures = prosodicFeatures.drop([\"attack_id\",\"filename\"], axis=\"columns\") \n",
    "# prosodicFeaturesLabels = torch.Tensor(prosodicFeatures[\"label\"].to_numpy(dtype=np.float32))\n",
    "# prosodicFeatures = prosodicFeatures.drop([\"label\"], axis=\"columns\")\n",
    "# print(prosodicFeatures.head(10))\n",
    "# prosodicFeatures = prosodicFeatures.to_numpy(dtype=np.float32)\n",
    "# prosodicFeatures = torch.Tensor(prosodicFeatures)\n",
    "# print(prosodicFeatures.shape)\n",
    "# print(prosodicFeaturesLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af83989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prosodicFeatures.shape\n",
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fc79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f35543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24844, 6])\n",
      "torch.Size([24844])\n"
     ]
    }
   ],
   "source": [
    "prosodicFeatures_val = pd.read_csv('processed_data/prosodic_features_and_labels_val.csv')\n",
    "prosodicFeatures_val = pd.DataFrame(prosodicFeatures_val)\n",
    "prosodicFeatures_val.tail\n",
    "prosodicFeatures_val = prosodicFeatures_val.drop([\"attack_id\", \"filename\"], axis=\"columns\")\n",
    "prosodicFeaturesLabels_val = torch.Tensor(prosodicFeatures_val[\"label\"].to_numpy(dtype=np.float32))\n",
    "prosodicFeatures_val = prosodicFeatures_val.drop([\"label\"], axis=\"columns\")\n",
    "prosodicFeatures_val = prosodicFeatures_val.to_numpy(dtype=np.float32)\n",
    "prosodicFeatures_val = torch.Tensor(prosodicFeatures_val)\n",
    "print(prosodicFeatures_val.shape)\n",
    "print(prosodicFeaturesLabels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39bb0335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71237, 6])\n",
      "torch.Size([71237])\n"
     ]
    }
   ],
   "source": [
    "prosodicFeatures_test = pd.read_csv('processed_data/prosodic_features_and_labels_test.csv')\n",
    "prosodicFeatures_test = pd.DataFrame(prosodicFeatures_test)\n",
    "prosodicFeatures_test.tail\n",
    "prosodicFeatures_test = prosodicFeatures_test.drop([\"attack_id\",\"filename\"], axis=\"columns\") \n",
    "prosodicFeaturesLabels_test = torch.Tensor(prosodicFeatures_test[\"label\"].to_numpy(dtype=np.float32))\n",
    "prosodicFeatures_test = prosodicFeatures_test.drop([\"label\"], axis=\"columns\")\n",
    "prosodicFeatures_test= prosodicFeatures_test.to_numpy(dtype=np.float32)\n",
    "prosodicFeatures_test = torch.Tensor(prosodicFeatures_test)\n",
    "print(prosodicFeatures_test.shape)\n",
    "print(prosodicFeaturesLabels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54c72bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parselmouth\n",
    "# from parselmouth.praat import call\n",
    "\n",
    "# def calculate_prosodic_features(audio_path):\n",
    "#     \"\"\"\n",
    "#     Calculates various prosodic features for a given audio file using Parselmouth.\n",
    "\n",
    "#     Args:\n",
    "#         audio_path (str): The full path to the audio file.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the calculated features, or None if processing fails.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Parselmouth can load the file directly\n",
    "#         sound = parselmouth.Sound(audio_path)\n",
    "\n",
    "#         # --- F0 (Fundamental Frequency) ---\n",
    "#         pitch = sound.to_pitch()\n",
    "#         mean_f0 = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "#         std_f0 = call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "#         # --- Jitter and Shimmer ---\n",
    "#         point_process = call(pitch, \"To PointProcess\")\n",
    "#         jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "#         shimmer = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "#         # --- HNR (Harmonics-to-Noise Ratio) ---\n",
    "#         harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "#         mean_hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "#         std_hnr = call(harmonicity, \"Get standard deviation\", 0, 0)\n",
    "\n",
    "#         # Replace NaN values (which Praat returns as '--undefined--') with 0.0\n",
    "#         features = {\n",
    "#             'mean_f0': mean_f0,\n",
    "#             'std_f0': std_f0,\n",
    "#             'jitter': jitter,\n",
    "#             'shimmer': shimmer,\n",
    "#             'mean_hnr': mean_hnr,\n",
    "#             'std_hnr': std_hnr\n",
    "#         }\n",
    "#         for key, value in features.items():\n",
    "#             if isinstance(value, float) and np.isnan(value):\n",
    "#                 features[key] = 0.0\n",
    "\n",
    "#         return features\n",
    "\n",
    "#     except Exception:\n",
    "#         # This can happen if Praat fails to process a file (e.g., too short, silent)\n",
    "#         return None\n",
    "\n",
    "# def process_and_save_features(bonafide_dirs, spoof_dir, protocol_file, output_csv_path):\n",
    "#     \"\"\"\n",
    "#     Processes audio from bonafide and spoof directories, extracts prosodic features,\n",
    "#     and saves the results to a single CSV file.\n",
    "\n",
    "#     Args:\n",
    "#         bonafide_dirs (list): A list of directories containing all bonafide audio files.\n",
    "#         spoof_dir (str): The directory containing spoofed audio files.\n",
    "#         protocol_file (str): Path to the original ASVspoof protocol file to map attack IDs.\n",
    "#         output_csv_path (str): Path to save the final CSV file.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Prosodic Feature Extraction ---\")\n",
    "\n",
    "#     # 1. Load protocol file to create a mapping from filename to attack_id\n",
    "#     try:\n",
    "#         protocol_df = pd.read_csv(protocol_file, sep=\" \", header=None, names=['speaker_id', 'filename', 'attack_type', 'system_id', 'label'])\n",
    "#         attack_id_map = pd.Series(protocol_df.system_id.values, index=protocol_df.filename).to_dict()\n",
    "#         print(f\"Successfully loaded protocol file for Attack ID mapping.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Protocol file not found at {protocol_file}. Cannot map attack IDs.\")\n",
    "#         return\n",
    "\n",
    "#     # 2. Gather all file paths from the directories\n",
    "#     all_files_to_process = []\n",
    "    \n",
    "#     # Add bonafide files (label = 'bonafide')\n",
    "#     for directory in bonafide_dirs:\n",
    "#         try:\n",
    "#             files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.flac', '.wav'))]\n",
    "#             for f_path in files:\n",
    "#                 all_files_to_process.append({'filepath': f_path, 'label': 'bonafide'})\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Warning: Directory not found: {directory}. Skipping.\")\n",
    "\n",
    "#     # Add spoof files (label = 'spoof')\n",
    "#     try:\n",
    "#         files = [os.path.join(spoof_dir, f) for f in os.listdir(spoof_dir) if f.endswith(('.flac', '.wav'))]\n",
    "#         for f_path in files:\n",
    "#             all_files_to_process.append({'filepath': f_path, 'label': 'spoof'})\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: Directory not found: {spoof_dir}. Skipping.\")\n",
    "\n",
    "#     print(f\"Found {len(all_files_to_process)} total audio files to process.\")\n",
    "\n",
    "#     # 3. Iterate, extract features, and build the results list\n",
    "#     results_list = []\n",
    "#     for file_info in tqdm(all_files_to_process, desc=\"Processing Files\"):\n",
    "#         filepath = file_info['filepath']\n",
    "#         label = file_info['label']\n",
    "        \n",
    "#         prosodic_features = calculate_prosodic_features(filepath)\n",
    "        \n",
    "#         if prosodic_features:\n",
    "#             base_filename = os.path.basename(filepath)\n",
    "            \n",
    "#             # Determine the original filename to look up the attack ID\n",
    "#             if base_filename.startswith('aug_'):\n",
    "#                 # Extracts original name from augmented files, e.g., 'aug_0_LA_T_12345.flac' -> 'LA_T_12345'\n",
    "#                 original_filename_key = '_'.join(base_filename.split('_')[2:]).replace('.flac', '')\n",
    "#             else:\n",
    "#                 # For original files\n",
    "#                 original_filename_key = base_filename.replace('.flac', '')\n",
    "            \n",
    "#             attack_id = attack_id_map.get(original_filename_key, '-') # Default to '-' if not found\n",
    "            \n",
    "#             # Combine all information into one dictionary\n",
    "#             data_row = {\n",
    "#                 'filename': base_filename,\n",
    "#                 'label': label,\n",
    "#                 'attack_id': attack_id,\n",
    "#                 **prosodic_features\n",
    "#             }\n",
    "#             results_list.append(data_row)\n",
    "\n",
    "#     # 4. Convert the list of results into a DataFrame and save\n",
    "#     if not results_list:\n",
    "#         print(\"No features were extracted. Please check your audio directories.\")\n",
    "#         return\n",
    "        \n",
    "#     final_df = pd.DataFrame(results_list)\n",
    "#     os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "#     final_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "#     print(f\"\\n--- Feature Extraction Complete ---\")\n",
    "#     print(f\"Processed {len(final_df)} files successfully.\")\n",
    "#     print(f\"Prosodic features saved to: {output_csv_path}\")\n",
    "\n",
    "# # --- HOW TO RUN THE SCRIPT ---\n",
    "\n",
    "# # ==> IMPORTANT: Update these paths to match your folder structure!\n",
    "\n",
    "# # Path to the original protocol file (used for mapping attack IDs).\n",
    "# PROTOCOL_FILE = \"ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "# # A list of all directories containing genuine (bonafide) audio.\n",
    "# BONAFIDE_DIRS = [\n",
    "#     \"bonafide_audio_train\",            # Original bonafide files\n",
    "#     \"augmented_bonafide\"   # Augmented bonafide files\n",
    "# ]\n",
    "\n",
    "# # The directory containing all spoofed audio files.\n",
    "# SPOOF_AUDIO_DIR = \"spoof_audio_train\"\n",
    "\n",
    "# # Path for the final output CSV file.\n",
    "# OUTPUT_CSV = \"processed_data/prosodic_features_train_augmented.csv\"\n",
    "\n",
    "# process_and_save_features(\n",
    "#     bonafide_dirs=BONAFIDE_DIRS,\n",
    "#     spoof_dir=SPOOF_AUDIO_DIR,\n",
    "#     protocol_file=PROTOCOL_FILE,\n",
    "#     output_csv_path=OUTPUT_CSV\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b69a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Calculates the Equal Error Rate (EER).\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels (0 or 1).\n",
    "        y_score (np.array): Prediction scores or probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        float: The EER value.\n",
    "    \"\"\"\n",
    "    # Ensure y_true and y_score are numpy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    \n",
    "    # The EER is the point on the ROC curve where the false acceptance rate (FAR)\n",
    "    # and false rejection rate (FRR) are equal. FAR is just FPR. FRR is 1 - TPR.\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    \n",
    "    return eer\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, num_classes):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model and calculates EER on the validation set.\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}...\")\n",
    "    model.to(device)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'val_eer': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_eer = float('inf')\n",
    "    best_f1 = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_samples = 0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "\n",
    "        for cqcc, prosodic, labels in train_progress_bar:\n",
    "            cqcc = cqcc.to(device)\n",
    "            prosodic = prosodic.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(cqcc, prosodic)  # Logits\n",
    "            loss = criterion(outputs, labels) ##for Cross Entropy\n",
    "            # loss = criterion(outputs, labels.unsqueeze(1).float()) ## for BCE\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * cqcc.size(0)\n",
    "            \n",
    "            if num_classes == 1:  # Binary with BCEWithLogitsLoss\n",
    "                predicted = (torch.sigmoid(outputs) > 0.3).squeeze().long()\n",
    "            else:  # Multi-class or binary with CrossEntropyLoss\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "            total_train_samples += labels.size(0)\n",
    "            correct_train_predictions += (predicted == labels.squeeze()).sum().item()\n",
    "            train_progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_train_loss = running_train_loss / total_train_samples if total_train_samples > 0 else 0\n",
    "        epoch_train_acc = correct_train_predictions / total_train_samples if total_train_samples > 0 else 0\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val_predictions = 0\n",
    "        total_val_samples = 0\n",
    "        all_val_labels = []\n",
    "        all_val_scores = []  # Scores for the positive class (bona fide)\n",
    "        all_val_preds = []\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosodic, labels in val_progress_bar:\n",
    "                cqcc = cqcc.to(device)\n",
    "                prosodic = prosodic.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(cqcc, prosodic)  # Logits\n",
    "                loss = criterion(outputs, labels) # for CrossEntropy\n",
    "                # loss = criterion(outputs, labels.unsqueeze(1).float()) ##for BCE\n",
    "                running_val_loss += loss.item() * cqcc.size(0)\n",
    "\n",
    "                if num_classes == 1:  # Binary with BCEWithLogitsLoss\n",
    "                    scores = torch.sigmoid(outputs).squeeze() # Probabilities for bona fide\n",
    "                    predicted = (scores > 0.3).long()\n",
    "                else:  # Multi-class or binary with CrossEntropyLoss\n",
    "                    # Assuming class 1 is 'bona fide' (positive class for EER)\n",
    "                    # If your 'bona fide' class is 0, use outputs_probs[:, 0]\n",
    "                    scores = F.softmax(outputs, dim=1)[:, 1] # Probabilities for class 1\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total_val_samples += labels.size(0)\n",
    "                correct_val_predictions += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "                val_progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_val_loss = running_val_loss / total_val_samples if total_val_samples > 0 else 0\n",
    "        epoch_val_acc = correct_val_predictions / total_val_samples if total_val_samples > 0 else 0\n",
    "        \n",
    "        # Calculate EER\n",
    "        val_eer = float('nan')\n",
    "        # EER can only be calculated if there are both positive and negative samples\n",
    "        if len(np.unique(all_val_labels)) > 1:\n",
    "            val_eer = calculate_eer(all_val_labels, all_val_scores)\n",
    "        \n",
    "        # --- Epoch End ---\n",
    "        \n",
    "        # Log metrics\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        history['val_eer'].append(val_eer)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} EER: {val_eer:.4f}\"\n",
    "              )\n",
    "        \n",
    "        # Print confusion matrix for the epoch\n",
    "        cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "        tn, fp, fn, tp= cm.ravel()\n",
    "        f1score = f1_score(all_val_labels, all_val_preds)\n",
    "        print(f\"F1 Score: {f1score}\")\n",
    "\n",
    "        print(f\"Confusion Matrix for Epoch {epoch+1}:\\n{cm}\", tp,tn,fp,fn)\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Epoch {epoch+1}: New best model saved with Val Loss: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "    print(f\"\\nFinished Training. Best model was from Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "936291cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss, accuracy, and EER.\n",
    "\n",
    "    Args:\n",
    "        history (pd.DataFrame): A DataFrame containing the metrics from train_model.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # --- Plotting Loss on the primary y-axis (ax1) ---\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='tab:blue')\n",
    "    ax1.plot(history.index + 1, history['train_loss'], 'b-', label='Train Loss')\n",
    "    ax1.plot(history.index + 1, history['val_loss'], 'b--', label='Validation Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Creating a secondary y-axis for EER and Accuracy ---\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('EER / Accuracy', color='tab:red')\n",
    "    # Plot EER\n",
    "    ax2.plot(history.index + 1, history['val_eer'], 'r-', label='Validation EER')\n",
    "    # Plot Accuracy\n",
    "    ax2.plot(history.index + 1, history['val_acc'], 'g--', label='Validation Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    # Set EER/Accuracy y-axis limits, e.g., from 0 to 1 if they are rates\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics per Epoch')\n",
    "    \n",
    "    # Combine legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e851ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CQCC, Prosodic features, and labels.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosodic_data, labels):\n",
    "        # Ensure data is in torch.Tensor format\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosodic_data = torch.tensor(prosodic_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve one sample from the dataset\n",
    "        cqcc_sample = self.cqcc_data[idx]\n",
    "        prosodic_sample = self.prosodic_data[idx]\n",
    "        label_sample = self.labels[idx]\n",
    "        \n",
    "        # The dataloader will stack these into a batch\n",
    "        return cqcc_sample, prosodic_sample, label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d3b5955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24844, 90, 157])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.load(\"processed_data/cqcc_features_val.npy\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "490d42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# X_train_tensor = torch.tensor(np.load(\"processed_data/cqcc_features.npy\"))\n",
    "# y_train_tensor = prosodicFeaturesLabels\n",
    "# X_val_tensor = torch.tensor(np.load(\"processed_data/cqcc_features_val.npy\"))\n",
    "# y_val_tensor = prosodicFeaturesLabels_val\n",
    "# print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "\n",
    "# y_train_tensor.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7be6e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNnetwork(nn.Module):\n",
    "    def __init__(self, input_features, input_time_steps, num_classes):\n",
    "        super().__init__()\n",
    "        # --- CNN Branch for CQCC Features ---\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(16,32, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # --- Dynamically calculate the flattened size ---\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, input_features, input_time_steps)\n",
    "            dummy_output = self.CNN(dummy_input)\n",
    "            self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # --- Embedding Layer for the CNN Branch ---\n",
    "        # This layer now correctly outputs a 128-dimensional embedding.\n",
    "        # The final classification layer was removed from here.\n",
    "        self.CQCC_embedding_layer = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # --- Embedding Layer for the Prosodic Branch ---\n",
    "        self.ProsodicLayer = nn.Sequential(\n",
    "            nn.Linear(6, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32)\n",
    "        )\n",
    "\n",
    "        # --- Fusion and Classification Layer ---\n",
    "        # This layer takes the concatenated embeddings (128 + 32 = 160)\n",
    "        # and performs the final classification.\n",
    "        self.fusionLayer = nn.Sequential(\n",
    "            nn.Linear(128 + 32, 64), # Input is 160\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_input, prosodic_input):\n",
    "        # Process the CQCC features through the CNN branch\n",
    "        # Input shape: (batch_size, features, time_steps)\n",
    "        x = cqcc_input.unsqueeze(1) # Add channel dimension -> (batch_size, 1, features, time_steps)\n",
    "        x = self.CNN(x)\n",
    "        flattened_x = self.flatten(x)\n",
    "        \n",
    "        # Get the 128-dimensional embedding for CQCC\n",
    "        cqcc_embedding = self.CQCC_embedding_layer(flattened_x) # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Process the prosodic features through its branch\n",
    "        # Get the 32-dimensional embedding for prosodic features\n",
    "        prosodic_embedding = self.ProsodicLayer(prosodic_input) # Shape: (batch_size, 32)\n",
    "        \n",
    "        # Concatenate the embeddings from both branches\n",
    "        concatenated_features = torch.cat([cqcc_embedding, prosodic_embedding], dim=1) # Shape: (batch_size, 160)\n",
    "        \n",
    "        # Pass the fused features to the final classification layer\n",
    "        logits = self.fusionLayer(concatenated_features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c5850e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2970e+02, 1.9315e+01, 1.9433e-02, 1.1098e-01, 9.4594e+00, 6.0118e+00],\n",
       "        [1.5117e+02, 1.8884e+01, 1.3721e-02, 7.7546e-02, 1.5424e+01, 8.8525e+00],\n",
       "        [2.4093e+02, 1.4894e+01, 7.4816e-03, 1.1253e-01, 1.3760e+01, 6.7782e+00],\n",
       "        ...,\n",
       "        [1.0943e+02, 2.1005e+01, 2.0272e-02, 1.6705e-01, 5.6863e+00, 5.1124e+00],\n",
       "        [1.8079e+02, 1.7326e+01, 8.5521e-03, 9.7119e-02, 1.5088e+01, 9.3377e+00],\n",
       "        [1.2182e+02, 1.3714e+01, 1.4913e-02, 1.4035e-01, 1.3382e+01, 7.0352e+00]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prosodicFeatures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f90fa531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25379, 90, 157]) torch.Size([25379])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2199786/3323751370.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
      "/tmp/ipykernel_2199786/3323751370.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.prosodic_data = torch.tensor(prosodic_data, dtype=torch.float32)\n",
      "/tmp/ipykernel_2199786/3323751370.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.2765 Acc: 0.8942 | Val Loss: 4.0315 Acc: 0.0865 EER: 0.8862\n",
      "F1 Score: 0.0069137531177525914\n",
      "Confusion Matrix for Epoch 1:\n",
      "[[ 2070   478]\n",
      " [22217    79]] 79 2070 478 22217\n",
      "Epoch 1: New best model saved with Val Loss: 4.0315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 0.1415 Acc: 0.9413 | Val Loss: 6.7068 Acc: 0.0581 EER: 0.9245\n",
      "F1 Score: 0.0067908832392513056\n",
      "Confusion Matrix for Epoch 2:\n",
      "[[ 1363  1185]\n",
      " [22216    80]] 80 1363 1185 22216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 0.0967 Acc: 0.9636 | Val Loss: 8.2029 Acc: 0.0330 EER: 0.9439\n",
      "F1 Score: 0.010135970333745366\n",
      "Confusion Matrix for Epoch 3:\n",
      "[[  697  1851]\n",
      " [22173   123]] 123 697 1851 22173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 0.0749 Acc: 0.9723 | Val Loss: 9.0178 Acc: 0.0378 EER: 0.9533\n",
      "F1 Score: 0.003584827011254689\n",
      "Confusion Matrix for Epoch 4:\n",
      "[[  897  1651]\n",
      " [22253    43]] 43 897 1651 22253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 0.0578 Acc: 0.9801 | Val Loss: 9.2888 Acc: 0.0291 EER: 0.9541\n",
      "F1 Score: 0.004868388480897763\n",
      "Confusion Matrix for Epoch 5:\n",
      "[[  665  1883]\n",
      " [22237    59]] 59 665 1883 22237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 0.0386 Acc: 0.9876 | Val Loss: 9.5558 Acc: 0.0222 EER: 0.9557\n",
      "F1 Score: 0.0072742133224356345\n",
      "Confusion Matrix for Epoch 6:\n",
      "[[  463  2085]\n",
      " [22207    89]] 89 463 2085 22207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 0.0361 Acc: 0.9891 | Val Loss: 9.7196 Acc: 0.0226 EER: 0.9571\n",
      "F1 Score: 0.007195715278629543\n",
      "Confusion Matrix for Epoch 7:\n",
      "[[  473  2075]\n",
      " [22208    88]] 88 473 2075 22208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 0.0338 Acc: 0.9904 | Val Loss: 10.2573 Acc: 0.0246 EER: 0.9560\n",
      "F1 Score: 0.0046005339905524755\n",
      "Confusion Matrix for Epoch 8:\n",
      "[[  555  1993]\n",
      " [22240    56]] 56 555 1993 22240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 0.0325 Acc: 0.9903 | Val Loss: 9.8572 Acc: 0.0216 EER: 0.9586\n",
      "F1 Score: 0.007917721002367154\n",
      "Confusion Matrix for Epoch 9:\n",
      "[[  439  2109]\n",
      " [22199    97]] 97 439 2109 22199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 0.0297 Acc: 0.9910 | Val Loss: 10.3319 Acc: 0.0241 EER: 0.9578\n",
      "F1 Score: 0.005006565988181221\n",
      "Confusion Matrix for Epoch 10:\n",
      "[[  537  2011]\n",
      " [22235    61]] 61 537 2011 22235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 83\u001b[0m\n\u001b[1;32m     74\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[1;32m     75\u001b[0m     optimizer,\n\u001b[1;32m     76\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# --- 6. Start Training ---\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m history_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m history_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     88\u001b[0m     plot_metrics(history_df)\n",
      "Cell \u001b[0;32mIn[36], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, num_classes)\u001b[0m\n\u001b[1;32m     50\u001b[0m train_progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cqcc, prosodic, labels \u001b[38;5;129;01min\u001b[39;00m train_progress_bar:\n\u001b[0;32m---> 53\u001b[0m     cqcc \u001b[38;5;241m=\u001b[39m \u001b[43mcqcc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     prosodic \u001b[38;5;241m=\u001b[39m prosodic\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(np.load(\"cqcc_features_aligned.npy\"))\n",
    "y_train_tensor = torch.tensor(np.load(\"labels_aligned.npy\"))#prosodicFeaturesLabels\n",
    "X_val_tensor = torch.tensor(np.load(\"processed_data/cqcc_features_val.npy\"))\n",
    "y_val_tensor = prosodicFeaturesLabels_val\n",
    "X_test_tensor = torch.tensor(np.load(\"processed_data/cqcc_features_test.npy\"))\n",
    "y_test_tensor = prosodicFeaturesLabels_test\n",
    "print(X_train_tensor.shape, y_train_tensor.shape)\n",
    "prosodicFeatures_train = torch.tensor(np.load(\"prosody_features_aligned.npy\"))\n",
    "train_dataset = FusionDataset(X_train_tensor, prosodicFeatures_train, y_train_tensor)\n",
    "val_dataset = FusionDataset(X_val_tensor, prosodicFeatures_val, y_val_tensor)\n",
    "test_dataset = FusionDataset(X_test_tensor, prosodicFeatures_test, y_test_tensor)\n",
    "# --- 4. Create TensorDatasets and DataLoaders ---\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# prosodic_train_set = TensorDataset(prosodicFeatures, y_train_tensor)\n",
    "# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# prosodic_val_set = TensorDataset(prosodicFeatures_val, y_train_tensor)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128\n",
    ", shuffle=False) # No need to shuffle validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=32\n",
    ", shuffle=False) # No need to shuffle validation\n",
    "\n",
    "# --- 5. Initialize Model, Criterion (Loss Function), and Optimizer ---\n",
    "# model = LSTMAttentionFusionNetwork(input_features=90, num_classes=2)\n",
    "# model =AttentionFusionCNN(input_features=90,input_time_steps=157, num_classes=2)\n",
    "model =CNNnetwork(input_features=90,input_time_steps=157, num_classes=2)\n",
    "# 1. Calculate the number of samples in each class from the training data\n",
    "# Ensure your labels are integers (0 and 1) for bincount\n",
    "# class_counts = torch.bincount(y_train_tensor.long())\n",
    "# count_class_0 = class_counts[0]\n",
    "# count_class_1 = class_counts[1]\n",
    "\n",
    "# print(f\"Training data contains {count_class_0} samples of class 0 (negative)\")\n",
    "# print(f\"Training data contains {count_class_1} samples of class 1 (positive)\")\n",
    "\n",
    "# 2. Calculate the pos_weight\n",
    "# Handle the case where a class might be missing to avoid division by zero\n",
    "# if count_class_1 > 0:\n",
    "#     pos_weight_value = count_class_0.float() / count_class_1.float()\n",
    "# else:\n",
    "#     pos_weight_value = 1.0 # Default value if no positive samples\n",
    "\n",
    "# print(f\"Calculated pos_weight for BCEWithLogitsLoss: {pos_weight_value:.4f}\")\n",
    "\n",
    "# # 3. Create the weight tensor and move it to the correct device\n",
    "# pos_weight = torch.tensor([pos_weight_value], device=device)\n",
    "\n",
    "\n",
    "# num_samples = len(y_train_tensor)\n",
    "# num_classes = 2\n",
    "# class_counts = torch.bincount(y_train_tensor)\n",
    "\n",
    "# weights = num_samples / (num_classes * class_counts.float())\n",
    "# weights = weights.to(device) # Move to the correct device\n",
    "\n",
    "# For multi-class (or binary if NUM_CLASSES=2 and labels are 0, 1), CrossEntropyLoss is common.\n",
    "# It expects raw logits from the model and integer labels.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# If NUM_CLASSES = 1 (binary with sigmoid output from model), you would use:\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) # if model outputs raw logits\n",
    "#criterion = nn.BCELoss() # if model outputs sigmoid probabilities\n",
    "# And your y_labels_np should be float32 and possibly reshaped for BCE losses.\n",
    "# For this example, we stick to NUM_CLASSES=2 and CrossEntropyLoss.\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode = 'min',\n",
    "    factor = 0.1,\n",
    "    patience = 3,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# --- 6. Start Training ---\n",
    "history_df = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 50, device, num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "if not history_df.empty:\n",
    "    plot_metrics(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    print(f\"\\nTesting on {device}...\")\n",
    "    model.to(device)\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    running_test_loss = 0.0\n",
    "    correct_test_predictions = 0\n",
    "    total_test_samples = 0\n",
    "    all_test_labels = []\n",
    "    all_test_scores = []\n",
    "    all_test_preds = []\n",
    "    test_progress_bar = tqdm(test_loader, desc=\"[Test]\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosodic, labels in test_progress_bar:\n",
    "            cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(cqcc, prosodic)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * cqcc.size(0)\n",
    "\n",
    "            scores = F.softmax(outputs, dim=1)[:, 1] # Probability for class 1\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total_test_samples += labels.size(0)\n",
    "            correct_test_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            all_test_preds.extend(predicted.cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "    # --- Calculate final metrics ---\n",
    "    final_test_loss = running_test_loss / total_test_samples if total_test_samples > 0 else 0\n",
    "    final_test_acc = correct_test_predictions / total_test_samples if total_test_samples > 0 else 0\n",
    "    final_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "    final_eer = float('nan')\n",
    "    if len(np.unique(all_test_labels)) > 1:\n",
    "        final_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "    \n",
    "    cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    print(f\"Test Loss: {final_test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {final_test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {final_f1:.4f}\")\n",
    "    print(f\"Test EER: {final_eer:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "test_model_instance = CNNnetwork(input_features=90,input_time_steps=157, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()       \n",
    "# Load the saved model weights\n",
    "test_model_instance.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "test_model(test_model_instance, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor[0], y_train_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d54a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, epoch, is_final=False):\n",
    "    \"\"\"\n",
    "    Renders a confusion matrix using Seaborn's heatmap.\n",
    "\n",
    "    Args:\n",
    "        cm (numpy.ndarray): The confusion matrix.\n",
    "        class_names (list): A list of class names for the labels.\n",
    "        epoch (int): The current epoch number for the title.\n",
    "        is_final (bool): If True, displays the plot. Otherwise, saves it.\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    \n",
    "    if is_final:\n",
    "        plt.title(f\"Final Confusion Matrix (from Best Epoch {epoch})\")\n",
    "        print(\"Displaying final confusion matrix plot...\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Avoid showing plots for every epoch during training\n",
    "        # Instead, save them to a file if needed\n",
    "        filename = f\"confusion_matrix_epoch_{epoch}.png\"\n",
    "        plt.title(f\"Confusion Matrix (Epoch {epoch})\")\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Saved confusion matrix for epoch {epoch} to {filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# # The PositionalEncoding class remains the same as before\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         position = torch.arange(max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len, 1, d_model)\n",
    "#         pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)\n",
    "# def calculate_eer_from_scores(labels, scores):\n",
    "#     \"\"\"Calculates EER from ground truth labels and classification scores.\"\"\"\n",
    "#     if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "#     if not isinstance(scores, np.ndarray): scores = np.array(scores)\n",
    "\n",
    "#     # Scores for bona fide (positive) and spoof (negative) trials\n",
    "#     bona_fide_scores = scores[labels == 1]\n",
    "#     spoof_scores = scores[labels == 0]\n",
    "\n",
    "#     if len(bona_fide_scores) == 0 or len(spoof_scores) == 0:\n",
    "#         return float('nan')\n",
    "\n",
    "#     # Calculate FAR and FRR at different thresholds\n",
    "#     min_score = min(np.min(bona_fide_scores), np.min(spoof_scores))\n",
    "#     max_score = max(np.max(bona_fide_scores), np.max(spoof_scores))\n",
    "#     thresholds = np.linspace(min_score, max_score, 1000)\n",
    "    \n",
    "#     far, frr = [], [] # False Acceptance Rate, False Rejection Rate\n",
    "#     for t in thresholds:\n",
    "#         far.append(np.sum(spoof_scores > t) / len(spoof_scores))\n",
    "#         frr.append(np.sum(bona_fide_scores <= t) / len(bona_fide_scores))\n",
    "    \n",
    "#     far, frr = np.array(far), np.array(frr)\n",
    "\n",
    "#     # Find the EER using interpolation\n",
    "#     try:\n",
    "#         eer_threshold = brentq(lambda x: interp1d(thresholds, far - frr)(x), min_score, max_score)\n",
    "#         eer = interp1d(thresholds, far)(eer_threshold)\n",
    "#     except (ValueError, RuntimeError):\n",
    "#         eer_index = np.nanargmin(np.abs(far - frr))\n",
    "#         eer = (far[eer_index] + frr[eer_index]) / 2.0\n",
    "#     return eer\n",
    "\n",
    "# # New model that handles both CQCC and Prosodic features\n",
    "# class MultiModalClassifierTransformer(nn.Module):\n",
    "#     # Model definition from previous response...\n",
    "#     def __init__(self, cqcc_input_dim: int, prosodic_input_dim: int, num_classes: int,\n",
    "#                  embedding_dim: int = 256, d_model: int = 256, nhead: int = 8, \n",
    "#                  num_encoder_layers: int = 6, dim_feedforward: int = 1024, dropout: float = 0.3):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "#         self.cqcc_input_projection = nn.Linear(cqcc_input_dim, d_model)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout, batch_first=True\n",
    "#         )\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "#         prosody_embedding_dim = 64\n",
    "#         self.prosody_encoder = nn.Sequential(\n",
    "#             nn.Linear(prosodic_input_dim, 128), nn.ReLU(),\n",
    "#             nn.Linear(128, prosody_embedding_dim)\n",
    "#         )\n",
    "#         self.fusion_head = nn.Sequential(\n",
    "#             nn.LayerNorm(d_model + prosody_embedding_dim),\n",
    "#             nn.Linear(d_model + prosody_embedding_dim, embedding_dim)\n",
    "#         )\n",
    "#         self.classifier_head = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "#     def forward(self, cqcc_src: torch.Tensor, prosodic_src: torch.Tensor, return_embedding=False):\n",
    "#         # *** FIX: Automatically correct CQCC input shape if necessary ***\n",
    "#         # The model expects [batch, seq_len, features]. If the input is\n",
    "#         # [batch, features, seq_len], we transpose it.\n",
    "#         if cqcc_src.shape[-1] != self.cqcc_input_projection.in_features:\n",
    "#             cqcc_src = cqcc_src.transpose(1, 2)\n",
    "\n",
    "#         cqcc_proj = self.cqcc_input_projection(cqcc_src) * math.sqrt(self.d_model)\n",
    "#         batch_size = cqcc_src.shape[0]\n",
    "#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "#         cqcc_with_cls = torch.cat([cls_tokens, cqcc_proj], dim=1)\n",
    "#         cqcc_with_cls = self.pos_encoder(cqcc_with_cls.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "#         transformer_output = self.transformer_encoder(cqcc_with_cls)\n",
    "#         cqcc_embedding = transformer_output[:, 0]\n",
    "#         prosody_embedding = self.prosody_encoder(prosodic_src)\n",
    "#         combined_embedding = torch.cat([cqcc_embedding, prosody_embedding], dim=1)\n",
    "#         final_embedding = self.fusion_head(combined_embedding)\n",
    "#         logits = self.classifier_head(final_embedding)\n",
    "#         if return_embedding:\n",
    "#             return logits, final_embedding\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # def train_classifier_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, num_classes):\n",
    "# #     # Training loop definition from previous response...\n",
    "# #     print(f\"Training on {device}...\")\n",
    "# #     model.to(device)\n",
    "# #     history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_eer': []}\n",
    "# #     best_val_loss = float('inf')\n",
    "# #     best_epoch = 0\n",
    "# #     for epoch in range(num_epochs):\n",
    "# #         model.train()\n",
    "# #         running_train_loss, correct_train_predictions, total_train_samples = 0.0, 0, 0\n",
    "# #         train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "# #         for cqcc, prosodic, labels in train_progress_bar:\n",
    "# #             cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "# #             optimizer.zero_grad()\n",
    "# #             outputs = model(cqcc, prosodic)\n",
    "# #             if num_classes == 1: outputs = outputs.squeeze(1)\n",
    "# #             loss = criterion(outputs, labels.float() if num_classes == 1 else labels)\n",
    "# #             loss.backward()\n",
    "# #             optimizer.step()\n",
    "# #             running_train_loss += loss.item() * cqcc.size(0)\n",
    "# #             if num_classes == 1: predicted = (torch.sigmoid(outputs) > 0.5).long()\n",
    "# #             else: _, predicted = torch.max(outputs.data, 1)\n",
    "# #             total_train_samples += labels.size(0)\n",
    "# #             correct_train_predictions += (predicted == labels).sum().item()\n",
    "# #             train_progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "# #         epoch_train_loss = running_train_loss / total_train_samples\n",
    "# #         epoch_train_acc = correct_train_predictions / total_train_samples\n",
    "# #         history['train_loss'].append(epoch_train_loss)\n",
    "# #         history['train_acc'].append(epoch_train_acc)\n",
    "# #         model.eval()\n",
    "# #         running_val_loss, correct_val_predictions, total_val_samples = 0.0, 0, 0\n",
    "# #         all_val_labels, all_val_scores = [], []\n",
    "# #         val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "# #         with torch.no_grad():\n",
    "# #             for cqcc, prosodic, labels in val_progress_bar:\n",
    "# #                 cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "# #                 outputs = model(cqcc, prosodic)\n",
    "# #                 if num_classes == 1: outputs = outputs.squeeze(1)\n",
    "# #                 loss = criterion(outputs, labels.float() if num_classes == 1 else labels)\n",
    "# #                 running_val_loss += loss.item() * cqcc.size(0)\n",
    "# #                 if num_classes == 1:\n",
    "# #                     scores = torch.sigmoid(outputs)\n",
    "# #                     predicted = (scores > 0.5).long()\n",
    "# #                 else:\n",
    "# #                     scores = F.softmax(outputs, dim=1)[:, 1]\n",
    "# #                     _, predicted = torch.max(outputs.data, 1)\n",
    "# #                 total_val_samples += labels.size(0)\n",
    "# #                 correct_val_predictions += (predicted == labels).sum().item()\n",
    "# #                 all_val_labels.extend(labels.cpu().numpy())\n",
    "# #                 all_val_scores.extend(scores.cpu().numpy())\n",
    "# #         epoch_val_loss = running_val_loss / total_val_samples\n",
    "# #         epoch_val_acc = correct_val_predictions / total_val_samples\n",
    "# #         val_eer = calculate_eer_from_scores(all_val_labels, all_val_scores)\n",
    "# #         history['val_loss'].append(epoch_val_loss)\n",
    "# #         history['val_acc'].append(epoch_val_acc)\n",
    "# #         history['val_eer'].append(val_eer)\n",
    "# #         print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} EER: {val_eer:.4f}\")\n",
    "# #         if epoch_val_loss < best_val_loss:\n",
    "# #             best_val_loss = epoch_val_loss\n",
    "# #             best_epoch = epoch + 1\n",
    "# #             torch.save(model.state_dict(), 'best_classifier_model.pth')\n",
    "# #             print(f\"Epoch {epoch+1}: New best model saved with Val Loss: {best_val_loss:.4f}\")\n",
    "# #         if scheduler:\n",
    "# #             scheduler.step(epoch_val_loss)\n",
    "# #     print(f\"\\nFinished Training. Best model from Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "# #     return pd.DataFrame(history)\n",
    "\n",
    "# def train_classifier_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, num_classes):\n",
    "#     \"\"\"\n",
    "#     Trains a classifier model and evaluates it, including confusion matrix calculation.\n",
    "#     \"\"\"\n",
    "#     print(f\"Training on {device}...\")\n",
    "#     model.to(device)\n",
    "#     history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_eer': []}\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_epoch = 0\n",
    "\n",
    "#     # Define class names for the confusion matrix plot\n",
    "#     if num_classes == 1:\n",
    "#         class_names = ['bona fide', 'spoof'] # Assuming binary classification for anti-spoofing\n",
    "#     else:\n",
    "#         class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_train_loss, correct_train_predictions, total_train_samples = 0.0, 0, 0\n",
    "#         train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "#         for cqcc, prosodic, labels in train_progress_bar:\n",
    "#             cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(cqcc, prosodic)\n",
    "#             if num_classes == 1: outputs = outputs.squeeze(1)\n",
    "#             loss = criterion(outputs, labels.float() if num_classes == 1 else labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_train_loss += loss.item() * cqcc.size(0)\n",
    "#             if num_classes == 1: predicted = (torch.sigmoid(outputs) > 0.5).long()\n",
    "#             else: _, predicted = torch.max(outputs.data, 1)\n",
    "#             total_train_samples += labels.size(0)\n",
    "#             correct_train_predictions += (predicted == labels).sum().item()\n",
    "#             train_progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "#         epoch_train_loss = running_train_loss / total_train_samples\n",
    "#         epoch_train_acc = correct_train_predictions / total_train_samples\n",
    "#         history['train_loss'].append(epoch_train_loss)\n",
    "#         history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "#         # --- Validation Phase ---\n",
    "#         model.eval()\n",
    "#         running_val_loss, correct_val_predictions, total_val_samples = 0.0, 0, 0\n",
    "#         all_val_labels, all_val_scores = [], []\n",
    "        \n",
    "#         # --- ADDED: List to store all validation predictions for the confusion matrix ---\n",
    "#         all_val_preds = []\n",
    "\n",
    "#         val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "#         with torch.no_grad():\n",
    "#             for cqcc, prosodic, labels in val_progress_bar:\n",
    "#                 cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "#                 outputs = model(cqcc, prosodic)\n",
    "#                 if num_classes == 1: outputs = outputs.squeeze(1)\n",
    "#                 loss = criterion(outputs, labels.float() if num_classes == 1 else labels)\n",
    "#                 running_val_loss += loss.item() * cqcc.size(0)\n",
    "\n",
    "#                 if num_classes == 1:\n",
    "#                     scores = torch.sigmoid(outputs)\n",
    "#                     predicted = (scores > 0.5).long()\n",
    "#                 else:\n",
    "#                     scores = F.softmax(outputs, dim=1)[:, 1] # Note: This assumes class 1 is the positive class for EER\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#                 total_val_samples += labels.size(0)\n",
    "#                 correct_val_predictions += (predicted == labels).sum().item()\n",
    "#                 all_val_labels.extend(labels.cpu().numpy())\n",
    "#                 all_val_scores.extend(scores.cpu().numpy())\n",
    "                \n",
    "#                 # --- ADDED: Store predictions for the confusion matrix ---\n",
    "#                 all_val_preds.extend(predicted.cpu().numpy())\n",
    "                \n",
    "#         epoch_val_loss = running_val_loss / total_val_samples\n",
    "#         epoch_val_acc = correct_val_predictions / total_val_samples\n",
    "#         val_eer = calculate_eer_from_scores(all_val_labels, all_val_scores)\n",
    "#         history['val_loss'].append(epoch_val_loss)\n",
    "#         history['val_acc'].append(epoch_val_acc)\n",
    "#         history['val_eer'].append(val_eer)\n",
    "\n",
    "#         # --- ADDED: Calculate and print the confusion matrix for the current epoch ---\n",
    "#         cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "#         print(f\"\\nEpoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} EER: {val_eer:.4f}\")\n",
    "#         print(f\"Confusion Matrix for Epoch {epoch+1}:\\n{cm}\")\n",
    "\n",
    "#         if epoch_val_loss < best_val_loss:\n",
    "#             best_val_loss = epoch_val_loss\n",
    "#             best_epoch = epoch + 1\n",
    "#             torch.save(model.state_dict(), 'best_classifier_model.pth')\n",
    "#             print(f\"Epoch {epoch+1}: New best model saved with Val Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "#         if scheduler:\n",
    "#             scheduler.step(epoch_val_loss)\n",
    "\n",
    "#     print(f\"\\nFinished Training. Best model from Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "#     # --- ADDED: Final Evaluation and Confusion Matrix Plot for the Best Model ---\n",
    "#     print(\"\\n--- Generating Final Report for the Best Model ---\")\n",
    "#     best_model = model\n",
    "#     best_model.load_state_dict(torch.load('best_classifier_model.pth'))\n",
    "#     best_model.to(device)\n",
    "#     best_model.eval()\n",
    "\n",
    "#     final_labels, final_preds = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for cqcc, prosodic, labels in val_loader:\n",
    "#             cqcc, prosodic, labels = cqcc.to(device), prosodic.to(device), labels.to(device)\n",
    "#             outputs = best_model(cqcc, prosodic)\n",
    "#             if num_classes == 1:\n",
    "#                 predicted = (torch.sigmoid(outputs.squeeze(1)) > 0.5).long()\n",
    "#             else:\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#             final_labels.extend(labels.cpu().numpy())\n",
    "#             final_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "#     final_cm = confusion_matrix(final_labels, final_preds)\n",
    "#     plot_confusion_matrix(final_cm, class_names=class_names, epoch=best_epoch, is_final=True)\n",
    "    \n",
    "#     return pd.DataFrame(history)\n",
    "\n",
    "# #==============================================================================\n",
    "# #  MAIN EXECUTION SCRIPT (HELPER FUNCTION TO RUN)\n",
    "# #==============================================================================\n",
    "# def run_training():\n",
    "#     \"\"\"\n",
    "#     A helper function to demonstrate how to set up and run the training process.\n",
    "#     \"\"\"\n",
    "#     # --- Configuration ---\n",
    "#     # Model Hyperparameters\n",
    "#     CQCC_DIM = 90\n",
    "#     PROSODIC_DIM = 6\n",
    "#     SEQ_LEN = 157\n",
    "#     EMBEDDING_DIM = 128\n",
    "#     NUM_CLASSES = 2 # Assuming binary classification: bona fide (1) vs. spoof (0)\n",
    "#     D_MODEL = 256\n",
    "#     N_HEAD = 8\n",
    "#     N_LAYERS = 4\n",
    "    \n",
    "#     # Training Hyperparameters\n",
    "#     NUM_EPOCHS = 50 # Keep it short for a demo run\n",
    "#     LEARNING_RATE = 1e-4\n",
    "#     BATCH_SIZE = 32\n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     # --- Setup for Strategy 2: Classification ---\n",
    "#     print(\"--- Running Training with Classification Strategy ---\")\n",
    "    \n",
    "#     # 1. Instantiate Model\n",
    "#     # Use the classifier version of the model\n",
    "#     model = MultiModalClassifierTransformer(\n",
    "#         cqcc_input_dim=CQCC_DIM,\n",
    "#         prosodic_input_dim=PROSODIC_DIM,\n",
    "#         num_classes=NUM_CLASSES if NUM_CLASSES > 1 else 1, # Output 1 logit for binary case\n",
    "#         embedding_dim=EMBEDDING_DIM,\n",
    "#         d_model=D_MODEL,\n",
    "#         nhead=N_HEAD,\n",
    "#         num_encoder_layers=N_LAYERS\n",
    "#     )\n",
    "    \n",
    "#     # 2. Instantiate Loss Function\n",
    "#     # Use BCEWithLogitsLoss for binary classification, CrossEntropy for multi-class\n",
    "#     if NUM_CLASSES == 1 or NUM_CLASSES == 2:\n",
    "#         criterion = nn.BCEWithLogitsLoss() if NUM_CLASSES == 1 else nn.CrossEntropyLoss()\n",
    "#     else:\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # 3. Instantiate Optimizer and Scheduler\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)\n",
    "\n",
    "#     # 4. Run Training\n",
    "#     history_df = train_classifier_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         criterion=criterion,\n",
    "#         optimizer=optimizer,\n",
    "#         scheduler=scheduler,\n",
    "#         num_epochs=NUM_EPOCHS,\n",
    "#         device=DEVICE,\n",
    "#         num_classes=NUM_CLASSES if NUM_CLASSES > 1 else 1\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\nTraining History:\")\n",
    "#     print(history_df)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not history_df.empty:\n",
    "    plot_metrics(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c42af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data/cqcc_features.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_CSV_PATH = \"processed_data/prosodic_features_and_labels.csv\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data/cqcc_features_val.npy\"\n",
    "PROSODIC_FEATURES_VAL_CSV_PATH = \"processed_data/prosodic_features_and_labels_val.csv\"\n",
    "\n",
    "# --- Model and Training Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Calculates the Equal Error Rate (EER) from the true labels and scores.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for handling 2D CQCC and 1D prosodic features.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        # --- CQCC Branch (Conv2D) ---\n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "\n",
    "        # --- Prosodic Branch ---\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "\n",
    "        # --- Fusion and Classifier ---\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        \n",
    "        prosody_df_train = pd.read_csv(PROSODIC_FEATURES_TRAIN_CSV_PATH)\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        \n",
    "        prosody_df_val = pd.read_csv(PROSODIC_FEATURES_VAL_CSV_PATH)\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        \n",
    "        if len(prosody_df_train) != len(X_cqcc_train) or len(prosody_df_val) != len(X_cqcc_val):\n",
    "            raise ValueError(\"Sample count mismatch between CSV and .npy files.\")\n",
    "\n",
    "        feature_columns = ['mean_f0', 'std_f0', 'jitter', 'shimmer', 'mean_hnr', 'std_hnr']\n",
    "        \n",
    "        X_prosody_train = prosody_df_train[feature_columns].values\n",
    "        y_train = prosody_df_train['label'].values\n",
    "        \n",
    "        X_prosody_val = prosody_df_val[feature_columns].values\n",
    "        y_val = prosody_df_val['label'].values\n",
    "        \n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "        \n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = [] # Collect raw scores for EER\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                \n",
    "                outputs = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Convert lists to numpy arrays for metric calculations\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data/cqcc_features.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_CSV_PATH = \"processed_data/prosodic_features_and_labels.csv\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data/cqcc_features_val.npy\"\n",
    "PROSODIC_FEATURES_VAL_CSV_PATH = \"processed_data/prosodic_features_and_labels_val.csv\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_CSV_PATH = \"processed_data/prosodic_features_and_labels_test.csv\"\n",
    "\n",
    "# --- Model and Training Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plotting losses on the primary y-axis\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Create a second y-axis for EER\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        prosody_df_train = pd.read_csv(PROSODIC_FEATURES_TRAIN_CSV_PATH)\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        prosody_df_val = pd.read_csv(PROSODIC_FEATURES_VAL_CSV_PATH)\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        \n",
    "        if len(prosody_df_train) != len(X_cqcc_train) or len(prosody_df_val) != len(X_cqcc_val):\n",
    "            raise ValueError(\"Sample count mismatch between CSV and .npy files.\")\n",
    "\n",
    "        feature_columns = ['mean_f0', 'std_f0', 'jitter', 'shimmer', 'mean_hnr', 'std_hnr']\n",
    "        X_prosody_train = prosody_df_train[feature_columns].values\n",
    "        y_train = prosody_df_train['label'].values\n",
    "        X_prosody_val = prosody_df_val[feature_columns].values\n",
    "        y_val = prosody_df_val['label'].values\n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        # Store history for plotting\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # Plot and save the training history\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_CSV_PATH = \"processed_data/prosodic_features_and_labels_test.csv\"\n",
    "\n",
    "\n",
    "print(\"\\n--- Starting Final Testing ---\")\n",
    "try:\n",
    "    # 1. Load the full test dataset\n",
    "    print(\"Loading full test data...\")\n",
    "    prosody_df_test_full = pd.read_csv(PROSODIC_FEATURES_TEST_CSV_PATH)\n",
    "    X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "    X_prosody_test_full = prosody_df_test_full[feature_columns].values\n",
    "    y_test_full = prosody_df_test_full['label'].values\n",
    "    print(f\"Loaded {len(y_test_full)} total test samples.\")\n",
    "\n",
    "    # 2. Create a balanced 70,000-sample subset using stratified sampling\n",
    "    NUM_SAMPLES_TO_SELECT = 71200\n",
    "    print(f\"Creating a balanced subset of {NUM_SAMPLES_TO_SELECT} samples...\")\n",
    "\n",
    "    # Use train_test_split to perform a single stratified split to get indices\n",
    "    _, _, _, _, _, selected_indices = train_test_split(\n",
    "        X_cqcc_test_full,\n",
    "        y_test_full,\n",
    "        np.arange(len(y_test_full)), # Pass indices to get a split of indices\n",
    "        test_size=NUM_SAMPLES_TO_SELECT,\n",
    "        stratify=y_test_full,\n",
    "        random_state=42 # Ensures reproducibility\n",
    "    )\n",
    "\n",
    "    # Use the selected indices to create the subset\n",
    "    X_cqcc_test_subset = X_cqcc_test_full[selected_indices]\n",
    "    X_prosody_test_subset = X_prosody_test_full[selected_indices]\n",
    "    y_test_subset = y_test_full[selected_indices]\n",
    "    \n",
    "    print(f\"Balanced subset created. Class distribution: {np.bincount(y_test_subset) / len(y_test_subset)}\")\n",
    "\n",
    "    # 3. Scale the test subset using the *already-fitted* scalers\n",
    "    X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_subset)\n",
    "    nsamples_test, nx_test, ny_test = X_cqcc_test_subset.shape\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_subset.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "    \n",
    "    # 4. Create Test Dataset and DataLoader\n",
    "    test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test_subset)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 5. Load the best model and evaluate\n",
    "    print(\"Loading best model for testing...\")\n",
    "    test_model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    test_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    test_model.eval()\n",
    "\n",
    "    all_test_labels = []\n",
    "    all_test_scores = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            outputs = test_model(cqcc_batch, prosody_batch)\n",
    "            all_test_scores.extend(outputs.cpu().numpy())\n",
    "            all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # 6. Calculate and display final metrics\n",
    "    all_test_labels = np.array(all_test_labels)\n",
    "    all_test_scores = np.array(all_test_scores).flatten()\n",
    "    all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "    test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "    test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "    test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "    test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "    print(\"\\n--- Final Test Results ---\")\n",
    "    print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"EER: {test_eer:.2f}%\")\n",
    "    print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "    print(\"Please ensure your test data files are in the correct paths and format.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data/cqcc_features.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_CSV_PATH = \"processed_data/prosodic_features_and_labels.csv\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data/cqcc_features_val.npy\"\n",
    "PROSODIC_FEATURES_VAL_CSV_PATH = \"processed_data/prosodic_features_and_labels_val.csv\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_CSV_PATH = \"processed_data/prosodic_features_and_labels_test.csv\"\n",
    "\n",
    "# --- Model and Training Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionTransformer_Best.pth\" # Updated model name\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_transformer.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64 # Transformers can be memory-intensive, a smaller batch size might be needed\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# --- NEW: Transformer Hyperparameters ---\n",
    "D_MODEL = 128       # Embedding dimension\n",
    "N_HEAD = 8          # Number of attention heads (must be a divisor of D_MODEL)\n",
    "NUM_ENCODER_LAYERS = 4 # Number of stacked transformer layers\n",
    "DIM_FEEDFORWARD = 512 # Hidden dimension in the feed-forward network\n",
    "TRANSFORMER_DROPOUT = 0.1\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based model to classify audio features.\n",
    "    It fuses prosodic features (as a CLS token) with a sequence of CQCC features.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(AttentionFusionTransformer, self).__init__()\n",
    "        \n",
    "        num_cqcc_coeffs, num_frames = cqcc_input_shape\n",
    "        \n",
    "        # 1. Projection layers to create embeddings\n",
    "        self.cqcc_projection = nn.Linear(num_cqcc_coeffs, d_model)\n",
    "        self.prosody_projection = nn.Linear(prosodic_features, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, num_frames + 1, d_model))\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important!\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # 4. Classifier Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC Input shape: (batch, coeffs, frames)\n",
    "        # Prosody Input shape: (batch, num_features)\n",
    "        \n",
    "        # Permute CQCC to treat frames as sequence: (batch, frames, coeffs)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "\n",
    "        # Project features to the embedding dimension (d_model)\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)  # -> (batch, frames, d_model)\n",
    "        prosody_embed = self.prosody_projection(prosody_x).unsqueeze(1) # -> (batch, 1, d_model)\n",
    "        \n",
    "        # Prepend the prosody embedding as the [CLS] token\n",
    "        # This token will act as the aggregate representation for classification\n",
    "        full_sequence = torch.cat([prosody_embed, cqcc_embed], dim=1) # -> (batch, frames+1, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        full_sequence_with_pos = full_sequence + self.pos_encoder\n",
    "        \n",
    "        # Feed into the Transformer Encoder\n",
    "        transformer_out = self.transformer_encoder(full_sequence_with_pos) # -> (batch, frames+1, d_model)\n",
    "        \n",
    "        # We only use the output of the [CLS] token (the first token) for classification\n",
    "        cls_token_out = transformer_out[:, 0, :] # -> (batch, d_model)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(cls_token_out)\n",
    "        output = torch.sigmoid(logits)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        prosody_df_train = pd.read_csv(PROSODIC_FEATURES_TRAIN_CSV_PATH)\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        prosody_df_val = pd.read_csv(PROSODIC_FEATURES_VAL_CSV_PATH)\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        \n",
    "        if len(prosody_df_train) != len(X_cqcc_train) or len(prosody_df_val) != len(X_cqcc_val):\n",
    "            raise ValueError(\"Sample count mismatch between CSV and .npy files.\")\n",
    "\n",
    "        feature_columns = ['mean_f0', 'std_f0', 'jitter', 'shimmer', 'mean_hnr', 'std_hnr']\n",
    "        X_prosody_train = prosody_df_train[feature_columns].values\n",
    "        y_train = prosody_df_train['label'].values\n",
    "        X_prosody_val = prosody_df_val[feature_columns].values\n",
    "        y_val = prosody_df_val['label'].values\n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # --- MODEL INSTANTIATION CHANGED ---\n",
    "    # model = AttentionFusionCNN(...) # Old model commented out\n",
    "    model = AttentionFusionTransformer(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1],\n",
    "        d_model=D_MODEL,\n",
    "        nhead=N_HEAD,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dim_feedforward=DIM_FEEDFORWARD,\n",
    "        dropout=TRANSFORMER_DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "    \n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # --- Testing Loop ---\n",
    "    print(\"\\n--- Starting Final Testing ---\")\n",
    "    try:\n",
    "        print(\"Loading full test data...\")\n",
    "        prosody_df_test_full = pd.read_csv(PROSODIC_FEATURES_TEST_CSV_PATH)\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_full = prosody_df_test_full[feature_columns].values\n",
    "        y_test_full = prosody_df_test_full['label'].values\n",
    "        print(f\"Loaded {len(y_test_full)} total test samples.\")\n",
    "\n",
    "        NUM_SAMPLES_TO_SELECT = 70000\n",
    "        print(f\"Creating a balanced subset of {NUM_SAMPLES_TO_SELECT} samples...\")\n",
    "        \n",
    "        _, _, _, _, _, selected_indices = train_test_split(\n",
    "            X_cqcc_test_full, y_test_full, np.arange(len(y_test_full)),\n",
    "            test_size=NUM_SAMPLES_TO_SELECT,\n",
    "            stratify=y_test_full,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        X_cqcc_test_subset = X_cqcc_test_full[selected_indices]\n",
    "        X_prosody_test_subset = X_prosody_test_full[selected_indices]\n",
    "        y_test_subset = y_test_full[selected_indices]\n",
    "        \n",
    "        print(f\"Balanced subset created. Class distribution: {np.bincount(y_test_subset) / len(y_test_subset)}\")\n",
    "\n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_subset)\n",
    "        nsamples_test, nx_test, ny_test = X_cqcc_test_subset.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_subset.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test_subset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(\"Loading best model for testing...\")\n",
    "        test_model = AttentionFusionTransformer(\n",
    "            cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "            prosodic_features=X_prosody_train.shape[1],\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEAD,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            dim_feedforward=DIM_FEEDFORWARD,\n",
    "            dropout=TRANSFORMER_DROPOUT\n",
    "        ).to(DEVICE)\n",
    "        test_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        test_model.eval()\n",
    "\n",
    "        all_test_labels = []\n",
    "        all_test_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = test_model(cqcc_batch, prosody_batch)\n",
    "                all_test_scores.extend(outputs.cpu().numpy())\n",
    "                all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        all_test_scores = np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        print(\"Please ensure your test data files are in the correct paths and format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Testing Loop ---\n",
    "# print(\"\\n--- Starting Final Testing ---\")\n",
    "# try:\n",
    "#     print(\"Loading full test data...\")\n",
    "#     prosody_df_test_full = pd.read_csv(PROSODIC_FEATURES_TEST_CSV_PATH)\n",
    "#     X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "#     X_prosody_test_full = prosody_df_test_full[feature_columns].values\n",
    "#     y_test_full = prosody_df_test_full['label'].values\n",
    "#     print(f\"Loaded {len(y_test_full)} total test samples.\")\n",
    "\n",
    "#     NUM_SAMPLES_TO_SELECT = 70000\n",
    "#     print(f\"Creating a balanced subset of {NUM_SAMPLES_TO_SELECT} samples...\")\n",
    "    \n",
    "#     _, _, _, _, _, selected_indices = train_test_split(\n",
    "#         X_cqcc_test_full, y_test_full, np.arange(len(y_test_full)),\n",
    "#         test_size=NUM_SAMPLES_TO_SELECT,\n",
    "#         stratify=y_test_full,\n",
    "#         random_state=42\n",
    "#     )\n",
    "\n",
    "#     X_cqcc_test_subset = X_cqcc_test_full[selected_indices]\n",
    "#     X_prosody_test_subset = X_prosody_test_full[selected_indices]\n",
    "#     y_test_subset = y_test_full[selected_indices]\n",
    "    \n",
    "#     print(f\"Balanced subset created. Class distribution: {np.bincount(y_test_subset) / len(y_test_subset)}\")\n",
    "\n",
    "#     X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_subset)\n",
    "#     nsamples_test, nx_test, ny_test = X_cqcc_test_subset.shape\n",
    "#     X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_subset.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "    \n",
    "#     test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test_subset)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     print(\"Loading best model for testing...\")\n",
    "#     test_model = AttentionFusionTransformer(\n",
    "#         cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "#         prosodic_features=X_prosody_train.shape[1],\n",
    "#         d_model=D_MODEL,\n",
    "#         nhead=N_HEAD,\n",
    "#         num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "#         dim_feedforward=DIM_FEEDFORWARD,\n",
    "#         dropout=TRANSFORMER_DROPOUT\n",
    "#     ).to(DEVICE)\n",
    "#     test_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "#     test_model.eval()\n",
    "\n",
    "#     all_test_labels = []\n",
    "#     all_test_scores = []\n",
    "#     with torch.no_grad():\n",
    "#         for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "#             cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "#             outputs = test_model(cqcc_batch, prosody_batch)\n",
    "#             all_test_scores.extend(outputs.cpu().numpy())\n",
    "#             all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "#     all_test_labels = np.array(all_test_labels)\n",
    "#     all_test_scores = np.array(all_test_scores).flatten()\n",
    "#     all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "#     test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "#     test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "#     test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "#     test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "#     print(\"\\n--- Final Test Results ---\")\n",
    "#     print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "#     print(f\"F1-Score: {test_f1:.4f}\")\n",
    "#     print(f\"EER: {test_eer:.2f}%\")\n",
    "#     print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "# except (FileNotFoundError, ValueError) as e:\n",
    "#     print(f\"Error during testing: {e}\")\n",
    "#     print(\"Please ensure your test data files are in the correct paths and format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"lstm_cross_attention_model_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # Increased for better convergence visualization\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Plotting losses on the primary y-axis (left)\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Create a second y-axis for Accuracy (right)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a third y-axis for EER (right, further out)\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60)) # Offset the third axis\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Combine legends from all axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96]) # Adjust layout to make room for legend\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for the fusion model.\"\"\"\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. LSTM CROSS-ATTENTION MODEL DEFINITION ---\n",
    "\n",
    "class LSTMCrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC features (processed by a CNN) with eGeMAPS LLDs (processed by an LSTM)\n",
    "    using cross-modal attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, embedding_dim):\n",
    "        super(LSTMCrossAttentionFusion, self).__init__()\n",
    "        \n",
    "        self.cqcc_cnn = nn.Sequential(\n",
    "            nn.Conv1d(cqcc_features, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Conv1d(64, embedding_dim, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=egmaps_features,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 # Added dropout to LSTM\n",
    "        )\n",
    "        self.lstm_fc = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 # Added dropout to attention\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        # Input shapes are (batch, features, time)\n",
    "        cqcc_out_cnn = self.cqcc_cnn(cqcc_x).transpose(1, 2) # -> (batch, time, embed_dim)\n",
    "        \n",
    "        egmaps_x_seq = egmaps_x.transpose(1, 2) # -> (batch, time, features)\n",
    "        lstm_out, _ = self.lstm(egmaps_x_seq)\n",
    "        prosody_query = torch.tanh(self.lstm_fc(lstm_out)) # -> (batch, time, embed_dim)\n",
    "        \n",
    "        attended_output, _ = self.cross_attention(\n",
    "            query=prosody_query, key=cqcc_out_cnn, value=cqcc_out_cnn\n",
    "        )\n",
    "        \n",
    "        pooled_output = attended_output.mean(dim=1)\n",
    "        output = self.classifier(pooled_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = LSTMCrossAttentionFusion(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: LSTM Cross-Attention Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        # --- Calculate and Store Metrics ---\n",
    "        # Training metrics\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        # Validation metrics\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        # Log history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss) # Step scheduler on validation loss\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_lstm_cross_attention_model1.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"transformer_encoder_decoder_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64 # Transformers can be memory intensive\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 512 # d_model for the transformer\n",
    "NUM_HEADS = 8       # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. TRANSFORMER ENCODER-DECODER MODEL ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Adds positional information to the input embeddings.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC and eGeMAPS LLDs using a Transformer Encoder-Decoder architecture.\n",
    "    Encoder processes CQCCs. Decoder processes eGeMAPS and attends to the encoder output.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, d_model, nhead, num_encoder_layers, num_decoder_layers, dropout):\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- Feature Projection ---\n",
    "        self.cqcc_projection = nn.Linear(cqcc_features, d_model)\n",
    "        self.egmaps_projection = nn.Linear(egmaps_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # --- Transformer ---\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        # Input shapes are (batch, features, time)\n",
    "        # Transpose to (batch, time, features) for sequence processing\n",
    "        cqcc_x = cqcc_x.transpose(1, 2)\n",
    "        egmaps_x = egmaps_x.transpose(1, 2)\n",
    "        \n",
    "        # 1. Project features to the embedding dimension (d_model)\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)\n",
    "        egmaps_embed = self.egmaps_projection(egmaps_x)\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        # PyTorch Transformer expects (seq_len, batch, features), so we transpose\n",
    "        cqcc_embed = self.pos_encoder(cqcc_embed.transpose(0, 1))\n",
    "        egmaps_embed = self.pos_encoder(egmaps_embed.transpose(0, 1))\n",
    "        \n",
    "        # 3. Pass through the Transformer\n",
    "        # Encoder gets CQCCs (src), Decoder gets eGeMAPS (tgt) and attends to CQCCs (memory)\n",
    "        transformer_out = self.transformer(src=cqcc_embed, tgt=egmaps_embed)\n",
    "        \n",
    "        # Transpose back to (batch, time, features)\n",
    "        transformer_out = transformer_out.transpose(0, 1)\n",
    "\n",
    "        # 4. Pool and Classify\n",
    "        pooled_output = transformer_out.mean(dim=1)\n",
    "        output = self.classifier(pooled_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = TransformerEncoderDecoder(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: Transformer Encoder-Decoder Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_transformer_enc_dec_model.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history_transformer.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"single_stream_transformer_output\")\n",
    "MODEL_OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"single_stream_transformer_output\") \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Transformers can be memory intensive\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 128 # d_model for the transformer\n",
    "NUM_HEADS = 8       # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6 # Can use a deeper single encoder\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. SINGLE-STREAM FUSION TRANSFORMER MODEL ---\n",
    "\n",
    "class SingleStreamFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC and eGeMAPS by concatenating them into a single sequence\n",
    "    and feeding them to a single Transformer Encoder. Uses token-type embeddings\n",
    "    to distinguish between the two modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, d_model, nhead, num_encoder_layers, dropout):\n",
    "        super(SingleStreamFusionTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- Feature Projection ---\n",
    "        self.cqcc_projection = nn.Linear(cqcc_features, d_model)\n",
    "        self.egmaps_projection = nn.Linear(egmaps_features, d_model)\n",
    "        \n",
    "        # --- Special Tokens and Embeddings ---\n",
    "        # CLS token will be learned\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        # Token type embeddings to distinguish CQCC from eGeMAPS\n",
    "        self.token_type_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=d_model) # 0 for CQCC, 1 for eGeMAPS\n",
    "        \n",
    "        # Positional encodings will be added to the combined sequence\n",
    "        # Max length is 1 (CLS) + time_steps (CQCC) + time_steps (eGeMAPS)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1 + time_steps * 2, d_model))\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        # Input shapes are (batch, features, time)\n",
    "        # Transpose to (batch, time, features) for sequence processing\n",
    "        cqcc_x = cqcc_x.transpose(1, 2)\n",
    "        egmaps_x = egmaps_x.transpose(1, 2)\n",
    "        \n",
    "        batch_size = cqcc_x.size(0)\n",
    "        time_steps = cqcc_x.size(1) # Get sequence length\n",
    "        \n",
    "        # 1. Project features to the embedding dimension (d_model)\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)      # (batch, time, d_model)\n",
    "        egmaps_embed = self.egmaps_projection(egmaps_x)  # (batch, time, d_model)\n",
    "        \n",
    "        # 2. Prepare token type embeddings (CORRECTED)\n",
    "        # Create IDs with shape (batch_size, time_steps)\n",
    "        cqcc_type_ids = torch.zeros(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        egmaps_type_ids = torch.ones(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        # Get embeddings from IDs. Shape will be (batch_size, time_steps, d_model)\n",
    "        cqcc_type_embed = self.token_type_embeddings(cqcc_type_ids)\n",
    "        egmaps_type_embed = self.token_type_embeddings(egmaps_type_ids)\n",
    "\n",
    "        # Add token type embeddings to feature embeddings\n",
    "        cqcc_embed += cqcc_type_embed\n",
    "        egmaps_embed += egmaps_type_embed\n",
    "        \n",
    "        # 3. Create the full sequence: [CLS] + CQCC + eGeMAPS\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        full_sequence = torch.cat([cls_tokens, cqcc_embed, egmaps_embed], dim=1)\n",
    "        \n",
    "        # 4. Add positional encoding\n",
    "        full_sequence += self.positional_encoding\n",
    "        \n",
    "        # 5. Pass through the Transformer Encoder\n",
    "        transformer_out = self.transformer_encoder(full_sequence)\n",
    "        \n",
    "        # 6. Use the output of the [CLS] token for classification\n",
    "        cls_output = transformer_out[:, 0, :]\n",
    "        output = self.classifier(cls_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: Single-Stream Fusion Transformer Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history_single_stream.png\"))\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(model, test_loader, device):\n",
    "    print(\"--- Running Inference on Test Set ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_labels, test_scores = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            cqcc_batch, lld_batch = cqcc_batch.to(device), lld_batch.to(device)\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            test_scores.extend(outputs.cpu().numpy())\n",
    "            test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate Final Metrics\n",
    "    test_labels = np.array(test_labels)\n",
    "    test_scores = np.array(test_scores).flatten()\n",
    "    test_preds_binary = (test_scores > 0.5).astype(int)\n",
    "    \n",
    "    test_acc = accuracy_score(test_labels, test_preds_binary) * 100\n",
    "    test_f1 = f1_score(test_labels, test_preds_binary)\n",
    "    test_eer = calculate_eer(test_labels, test_scores)\n",
    "    test_cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"--- Final Test Results ---\")\n",
    "    print(f\"  Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  EER:      {test_eer:.2f}%\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Return Metrics\n",
    "    results = {\n",
    "        'accuracy': test_acc,\n",
    "        'f1_score': test_f1,\n",
    "        'eer': test_eer,\n",
    "        'confusion_matrix': test_cm\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# --- 4. MAIN TEST EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"--- Starting Model Evaluation on Test Set ---\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Test Data ---\")\n",
    "        X_cqcc_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_test.npy\"))\n",
    "        X_lld_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_test.npy\"))\n",
    "        y_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_test.npy\"))\n",
    "        print(f\"✅ Loaded {len(y_test)} test samples.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading test data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Scalers ---\")\n",
    "        scaler_cqcc = joblib.load(os.path.join(MODEL_OUTPUT_DIR, \"scaler_cqcc.joblib\"))\n",
    "        scaler_lld = joblib.load(os.path.join(MODEL_OUTPUT_DIR, \"scaler_lld.joblib\"))\n",
    "        print(\"✅ Scalers loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading scaler files: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"--- Scaling Test Features ---\")\n",
    "    X_lld_test_scaled = scaler_lld.transform(X_lld_test.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_test.shape)\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_test.shape)\n",
    "    \n",
    "    test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_lld_test_scaled, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"--- Loading Best Trained Model ---\")\n",
    "    model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model_path = os.path.join(MODEL_OUTPUT_DIR, \"best_single_stream_transformer_model.pth\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(\"✅ Model weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Model file not found at {model_path}\")\n",
    "        exit()\n",
    "    \n",
    "    # --- Call the evaluation function ---\n",
    "    test_results = evaluate_on_test_set(model=model, test_loader=test_loader, device=DEVICE)\n",
    "    \n",
    "    print(\"\\nEvaluation complete.\")\n",
    "    # You can optionally do something with the results dictionary here\n",
    "    # print(\"Returned metrics dictionary:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.fftpack import dct\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import opensmile\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TEMP_AUDIO_PATH = os.path.join(OUTPUT_DIR, \"temp_5s_audio.wav\") # For temporary audio clips\n",
    "\n",
    "# --- Feature Parameters ---\n",
    "TARGET_SHAPE_CQCC = (128, 157)\n",
    "TARGET_SHAPE_LLD = (23, 157) # eGeMAPS LLDs have 23 features\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5.0\n",
    "\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "\n",
    "def extract_cqcc(y, sr, n_bins=90, n_cqcc=128):\n",
    "    \"\"\"Extracts CQCC features.\"\"\"\n",
    "    try:\n",
    "        cqt = np.abs(librosa.cqt(y=y, sr=sr, n_bins=n_bins, fmin=librosa.note_to_hz('C1')))\n",
    "        log_cqt = np.log(cqt + 1e-6)\n",
    "        cqcc = dct(log_cqt, type=2, axis=0, norm='ortho')\n",
    "        return cqcc[:n_cqcc, :]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pad_or_truncate(array, target_shape):\n",
    "    \"\"\"Pads or truncates a 2D array to a target shape.\"\"\"\n",
    "    padded_array = np.full(target_shape, 0.0, dtype=np.float32) # Pad with 0\n",
    "    copy_shape = tuple(min(c, t) for c, t in zip(array.shape, target_shape))\n",
    "    padded_array[:copy_shape[0], :copy_shape[1]] = array[:copy_shape[0], :copy_shape[1]]\n",
    "    return padded_array\n",
    "\n",
    "def process_data_aligned(directories, label, smile_instance):\n",
    "    \"\"\"\n",
    "    Extracts aligned CQCC and eGeMAPS LLD features.\n",
    "    \"\"\"\n",
    "    cqcc_list, lld_list, labels_list = [], [], []\n",
    "\n",
    "    for directory in directories:\n",
    "        full_dir_path = os.path.join(TEAMMATE_DATA_PATH, directory)\n",
    "        print(f\"\\nProcessing directory: {full_dir_path}\")\n",
    "        if not os.path.isdir(full_dir_path):\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(full_dir_path) if f.endswith(('.flac', '.wav'))]\n",
    "        for filename in tqdm(files, desc=f\"Extracting from {directory}\"):\n",
    "            filepath = os.path.join(full_dir_path, filename)\n",
    "            try:\n",
    "                # 1. Load the 5-second audio clip once\n",
    "                audio_5s, sr = librosa.load(filepath, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "                # 2. Extract CQCC from the 5s clip\n",
    "                cqcc_feats = extract_cqcc(audio_5s, sr, n_cqcc=TARGET_SHAPE_CQCC[0])\n",
    "                if cqcc_feats is None: continue\n",
    "\n",
    "                # 3. Extract LLDs from the same 5s clip\n",
    "                # We need to save the clip to a temporary file for openSMILE to process\n",
    "                sf.write(TEMP_AUDIO_PATH, audio_5s, sr)\n",
    "                lld_df = smile_instance.process_file(TEMP_AUDIO_PATH)\n",
    "                lld_feats = lld_df.values.T # Transpose to get (features, time)\n",
    "\n",
    "                # 4. Pad both feature sets to the target shape\n",
    "                padded_cqcc = pad_or_truncate(cqcc_feats, TARGET_SHAPE_CQCC)\n",
    "                padded_lld = pad_or_truncate(lld_feats, TARGET_SHAPE_LLD)\n",
    "\n",
    "                # 5. Append to lists\n",
    "                cqcc_list.append(padded_cqcc)\n",
    "                lld_list.append(padded_lld)\n",
    "                labels_list.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {filepath}: {e}\")\n",
    "\n",
    "    # Clean up the temporary audio file\n",
    "    if os.path.exists(TEMP_AUDIO_PATH):\n",
    "        os.remove(TEMP_AUDIO_PATH)\n",
    "\n",
    "    return np.array(cqcc_list), np.array(lld_list), np.array(labels_list)\n",
    "\n",
    "\n",
    "# --- 3. MAIN EXECUTION SCRIPT ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Initialize openSMILE for LLDs ---\n",
    "    smile = opensmile.Smile(\n",
    "        feature_set=opensmile.FeatureSet.eGeMAPS,\n",
    "        feature_level=opensmile.FeatureLevel.LowLevelDescriptors, # Use the full name, # Set to LLD\n",
    "    )\n",
    "\n",
    "    # # --- Process Training Data ---\n",
    "    # print(\"--- Processing Training Set ---\")\n",
    "    # cqcc_bf_train, lld_bf_train, labels_bf_train = process_data_aligned(['bonafide_audio_train', 'augmented_bonafide'], 1, smile)\n",
    "    # cqcc_spf_train, lld_spf_train, labels_spf_train = process_data_aligned(['spoof_audio_train'], 0, smile)\n",
    "\n",
    "    # # --- Process Validation Data ---\n",
    "    # print(\"\\n--- Processing Validation Set ---\")\n",
    "    # cqcc_bf_val, lld_bf_val, labels_bf_val = process_data_aligned(['bonafide_audio_val'], 1, smile)\n",
    "    # cqcc_spf_val, lld_spf_val, labels_spf_val = process_data_aligned(['spoof_audio_val'], 0, smile)\n",
    "\n",
    "    # # --- Combine and Save Training Data ---\n",
    "    # X_cqcc_train = np.concatenate((cqcc_bf_train, cqcc_spf_train), axis=0)\n",
    "    # X_lld_train = np.concatenate((lld_bf_train, lld_spf_train), axis=0)\n",
    "    # y_train = np.concatenate((labels_bf_train, labels_spf_train), axis=0)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"cqcc_features_train.npy\"), X_cqcc_train)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"egmaps_lld_features_train.npy\"), X_lld_train)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"labels_train.npy\"), y_train)\n",
    "    # print(f\"\\n✅ Training data saved. Shapes: CQCC={X_cqcc_train.shape}, LLD={X_lld_train.shape}\")\n",
    "\n",
    "    print(\"\\n--- Processing Test Set ---\")\n",
    "    cqcc_bf_test, lld_bf_test, labels_bf_test = process_data_aligned(['bonafide_audio_test'], 1, smile)\n",
    "    cqcc_spf_test, lld_spf_test, labels_spf_test = process_data_aligned(['spoof_audio_test'], 0, smile)\n",
    "\n",
    "    # --- Combine and Save Validation Data ---\n",
    "    # X_cqcc_val = np.concatenate((cqcc_bf_val, cqcc_spf_val), axis=0)\n",
    "    # X_lld_val = np.concatenate((lld_bf_val, lld_spf_val), axis=0)\n",
    "    # y_val = np.concatenate((labels_bf_val, labels_spf_val), axis=0)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"cqcc_features_val.npy\"), X_cqcc_val)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"egmaps_lld_features_val.npy\"), X_lld_val)\n",
    "    # np.save(os.path.join(OUTPUT_DIR, \"labels_dev.npy\"), y_val)\n",
    "    # print(f\"✅ Validation data saved. Shapes: CQCC={X_cqcc_val.shape}, LLD={X_lld_val.shape}\")\n",
    "\n",
    "    X_cqcc_test = np.concatenate((cqcc_bf_test, cqcc_spf_test), axis=0)\n",
    "    X_lld_test = np.concatenate((lld_bf_test, lld_spf_test), axis=0)\n",
    "    y_test = np.concatenate((labels_bf_test, labels_spf_test), axis=0)\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"cqcc_features_test.npy\"), X_cqcc_test)\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"egmaps_lld_features_test.npy\"), X_lld_test)\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"labels_test.npy\"), y_test)\n",
    "    print(f\"✅ Test data saved. Shapes: CQCC={X_cqcc_test.shape}, LLD={X_lld_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate.\n",
    "        test_loader (DataLoader): The DataLoader for the test set.\n",
    "        device (str): The device to run evaluation on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the final test metrics.\n",
    "    \"\"\"\n",
    "    print(\"--- Running Inference on Test Set ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_labels = []\n",
    "    test_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            cqcc_batch, lld_batch = cqcc_batch.to(device), lld_batch.to(device)\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            test_scores.extend(outputs.cpu().numpy())\n",
    "            test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # --- Calculate Final Metrics ---\n",
    "    test_labels = np.array(test_labels)\n",
    "    test_scores = np.array(test_scores).flatten()\n",
    "    test_preds_binary = (test_scores > 0.5).astype(int)\n",
    "    \n",
    "    test_acc = accuracy_score(test_labels, test_preds_binary) * 100\n",
    "    test_f1 = f1_score(test_labels, test_preds_binary)\n",
    "    test_eer = calculate_eer(test_labels, test_scores)\n",
    "    test_cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "    \n",
    "    # --- Print Results ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"--- Final Test Results ---\")\n",
    "    print(f\"  Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  EER:      {test_eer:.2f}%\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # --- Return Metrics ---\n",
    "    results = {\n",
    "        'accuracy': test_acc,\n",
    "        'f1_score': test_f1,\n",
    "        'eer': test_eer,\n",
    "        'confusion_matrix': test_cm\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# --- HOW TO USE IT IN YOUR SCRIPT ---\n",
    "# In your `if __name__ == '__main__':` block, after loading the model:\n",
    "# ... (code to load data, scalers, and model) ...\n",
    "\n",
    "# model.eval() # This is now handled inside the function\n",
    "\n",
    "test_results = evaluate_on_test_set(model=model, test_loader=test_loader, device=DEVICE)\n",
    "print(\"\\nEvaluation complete. Results dictionary:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"single_stream_transformer_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Transformers can be memory intensive\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 128 # d_model for the transformer\n",
    "NUM_HEADS = 8       # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6 # Can use a deeper single encoder\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. SINGLE-STREAM FUSION TRANSFORMER MODEL ---\n",
    "\n",
    "class SingleStreamFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC and eGeMAPS by concatenating them into a single sequence\n",
    "    and feeding them to a single Transformer Encoder. Uses token-type embeddings\n",
    "    to distinguish between the two modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, d_model, nhead, num_encoder_layers, dropout):\n",
    "        super(SingleStreamFusionTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.cqcc_projection = nn.Linear(cqcc_features, d_model)\n",
    "        self.egmaps_projection = nn.Linear(egmaps_features, d_model)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.token_type_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=d_model)\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1 + time_steps * 2, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        cqcc_x = cqcc_x.transpose(1, 2)\n",
    "        egmaps_x = egmaps_x.transpose(1, 2)\n",
    "        \n",
    "        batch_size = cqcc_x.size(0)\n",
    "        time_steps = cqcc_x.size(1)\n",
    "        \n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)\n",
    "        egmaps_embed = self.egmaps_projection(egmaps_x)\n",
    "        \n",
    "        cqcc_type_ids = torch.zeros(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        egmaps_type_ids = torch.ones(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        cqcc_type_embed = self.token_type_embeddings(cqcc_type_ids)\n",
    "        egmaps_type_embed = self.token_type_embeddings(egmaps_type_ids)\n",
    "\n",
    "        cqcc_embed += cqcc_type_embed\n",
    "        egmaps_embed += egmaps_type_embed\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        full_sequence = torch.cat([cls_tokens, cqcc_embed, egmaps_embed], dim=1)\n",
    "        \n",
    "        full_sequence += self.positional_encoding\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(full_sequence)\n",
    "        \n",
    "        cls_output = transformer_out[:, 0, :]\n",
    "        output = self.classifier(cls_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    # --- ADDED: Save the fitted scalers for the test script ---\n",
    "    print(\"--- Saving Scalers ---\")\n",
    "    joblib.dump(scaler_cqcc, os.path.join(OUTPUT_DIR, \"scaler_cqcc.joblib\"))\n",
    "    joblib.dump(scaler_lld, os.path.join(OUTPUT_DIR, \"scaler_lld.joblib\"))\n",
    "    print(f\"✅ Scalers saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: Single-Stream Fusion Transformer Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history_single_stream.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"single_stream_transformer_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Transformers can be memory intensive\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 128 # d_model for the transformer\n",
    "NUM_HEADS = 8       # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6 # Can use a deeper single encoder\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. SINGLE-STREAM FUSION TRANSFORMER MODEL ---\n",
    "\n",
    "class SingleStreamFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC and eGeMAPS by concatenating them into a single sequence\n",
    "    and feeding them to a single Transformer Encoder. Uses token-type embeddings\n",
    "    to distinguish between the two modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, d_model, nhead, num_encoder_layers, dropout):\n",
    "        super(SingleStreamFusionTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.cqcc_projection = nn.Linear(cqcc_features, d_model)\n",
    "        self.egmaps_projection = nn.Linear(egmaps_features, d_model)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.token_type_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=d_model)\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1 + time_steps * 2, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        cqcc_x = cqcc_x.transpose(1, 2)\n",
    "        egmaps_x = egmaps_x.transpose(1, 2)\n",
    "        \n",
    "        batch_size = cqcc_x.size(0)\n",
    "        time_steps = cqcc_x.size(1)\n",
    "        \n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)\n",
    "        egmaps_embed = self.egmaps_projection(egmaps_x)\n",
    "        \n",
    "        cqcc_type_ids = torch.zeros(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        egmaps_type_ids = torch.ones(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        cqcc_type_embed = self.token_type_embeddings(cqcc_type_ids)\n",
    "        egmaps_type_embed = self.token_type_embeddings(egmaps_type_ids)\n",
    "\n",
    "        cqcc_embed += cqcc_type_embed\n",
    "        egmaps_embed += egmaps_type_embed\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        full_sequence = torch.cat([cls_tokens, cqcc_embed, egmaps_embed], dim=1)\n",
    "        \n",
    "        full_sequence += self.positional_encoding\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(full_sequence)\n",
    "        \n",
    "        cls_output = transformer_out[:, 0, :]\n",
    "        output = self.classifier(cls_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    print(\"--- Saving Scalers ---\")\n",
    "    joblib.dump(scaler_cqcc, os.path.join(OUTPUT_DIR, \"scaler_cqcc.joblib\"))\n",
    "    joblib.dump(scaler_lld, os.path.join(OUTPUT_DIR, \"scaler_lld.joblib\"))\n",
    "    print(f\"✅ Scalers saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: Single-Stream Fusion Transformer Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history_single_stream.png\"))\n",
    "    \n",
    "    # --- ADDED: TESTING PORTION ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Starting Final Evaluation on Test Set ---\")\n",
    "    \n",
    "    try:\n",
    "        print(\"--- Loading Test Data ---\")\n",
    "        X_cqcc_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_test.npy\"))\n",
    "        X_lld_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_test.npy\"))\n",
    "        y_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_test.npy\"))\n",
    "        print(f\"✅ Loaded {len(y_test)} test samples.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading test data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Test Features ---\")\n",
    "    X_lld_test_scaled = scaler_lld.transform(X_lld_test.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_test.shape)\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_test.shape)\n",
    "    \n",
    "    test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_lld_test_scaled, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"--- Loading Best Trained Model for Testing ---\")\n",
    "    # Re-initialize the model structure\n",
    "    test_model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model_path = os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\")\n",
    "    try:\n",
    "        test_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(\"✅ Model weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Model file not found at {model_path}\")\n",
    "        exit()\n",
    "\n",
    "    test_model.eval()\n",
    "    \n",
    "    test_labels, test_scores = [], []\n",
    "    with torch.no_grad():\n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "            cqcc_batch, lld_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE)\n",
    "            outputs = test_model(cqcc_batch, lld_batch)\n",
    "            test_scores.extend(outputs.cpu().numpy())\n",
    "            test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    test_labels = np.array(test_labels)\n",
    "    test_scores = np.array(test_scores).flatten()\n",
    "    test_preds_binary = (test_scores > 0.5).astype(int)\n",
    "    \n",
    "    test_acc = accuracy_score(test_labels, test_preds_binary) * 100\n",
    "    test_f1 = f1_score(test_labels, test_preds_binary)\n",
    "    test_eer = calculate_eer(test_labels, test_scores)\n",
    "    test_cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"--- Final Test Results ---\")\n",
    "    print(f\"  Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  EER:      {test_eer:.2f}%\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0950d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "# from scipy.optimize import brentq\n",
    "# from scipy.interpolate import interp1d\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- Configuration ---\n",
    "# # Paths for TRAINING data\n",
    "# CQCC_FEATURES_TRAIN_PATH = \"processed_data/cqcc_features.npy\"\n",
    "# PROSODIC_FEATURES_TRAIN_CSV_PATH = \"processed_data/prosodic_features_and_labels.csv\"\n",
    "\n",
    "# # Paths for VALIDATION data\n",
    "# CQCC_FEATURES_VAL_PATH = \"processed_data/cqcc_features_val.npy\"\n",
    "# PROSODIC_FEATURES_VAL_CSV_PATH = \"processed_data/prosodic_features_and_labels_val.csv\"\n",
    "\n",
    "# # Paths for TEST data\n",
    "# CQCC_FEATURES_TEST_PATH = \"processed_data/cqcc_features_test.npy\"\n",
    "# PROSODIC_FEATURES_TEST_CSV_PATH = \"processed_data/prosodic_features_and_labels_test.csv\"\n",
    "\n",
    "# # --- Model and Analysis Configuration ---\n",
    "# MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "# PLOT_SAVE_PATH = \"saved_models/training_metrics.png\"\n",
    "# ATTENTION_PLOT_PATH = \"saved_models/attention_importance.png\"\n",
    "# ABLATION_PLOT_PATH = \"saved_models/ablation_importance.png\"\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 40\n",
    "# LEARNING_RATE = 1e-4 \n",
    "# WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# os.makedirs(\"saved_models\", exist_ok=True)\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# def calculate_eer(y_true, y_score):\n",
    "#     \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "#     eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "#     return eer * 100\n",
    "\n",
    "# def plot_training_history(history, save_path):\n",
    "#     \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "#     fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "#     color = 'tab:red'\n",
    "#     ax1.set_xlabel('Epochs')\n",
    "#     ax1.set_ylabel('Loss', color=color)\n",
    "#     ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "#     ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "#     ax1.tick_params(axis='y', labelcolor=color)\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "#     ax2 = ax1.twinx()  \n",
    "#     color = 'tab:blue'\n",
    "#     ax2.set_ylabel('EER (%)', color=color)\n",
    "#     ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "#     ax2.tick_params(axis='y', labelcolor=color)\n",
    "#     ax2.legend(loc='upper right')\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.title('Training and Validation Metrics')\n",
    "#     plt.savefig(save_path)\n",
    "#     print(f\"\\nTraining plot saved to {save_path}\")\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# class AudioFeatureDataset(Dataset):\n",
    "#     \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "#     def __init__(self, cqcc_data, prosody_data, labels):\n",
    "#         self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "#         self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "#         self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "# class AttentionFusionCNN(nn.Module):\n",
    "#     \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "#     def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "#         super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "#         self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "#         self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "#         self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "#         self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "#         self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "#         self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "#             dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "#             cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "#         self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "#         self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "#         self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "#         self.prosody_dropout = nn.Dropout(0.4)\n",
    "#         self.prosody_fc2 = nn.Linear(32, 64)\n",
    "#         concatenated_size = 64 + 64\n",
    "#         self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "#         self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "#         self.classifier_bn = nn.BatchNorm1d(64)\n",
    "#         self.classifier_dropout = nn.Dropout(0.5)\n",
    "#         self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, cqcc_x, prosody_x):\n",
    "#         # IMPORTANT: This forward pass now returns attention weights\n",
    "#         cqcc_x = cqcc_x.unsqueeze(1)\n",
    "#         cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "#         cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "#         cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "#         cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "#         cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "#         cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "#         prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "#         prosody_out = self.prosody_dropout(prosody_out)\n",
    "#         prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "#         concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        \n",
    "#         # The attention mechanism here is self-attention on the combined features.\n",
    "#         # The weights will show the importance of each part of the concatenated vector.\n",
    "#         attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "#         fused = concatenated * attention_weights\n",
    "\n",
    "#         x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "#         x = self.classifier_dropout(x)\n",
    "#         output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "#         # Return both the final prediction and the attention weights\n",
    "#         return output, attention_weights\n",
    "\n",
    "# # ==============================================================================\n",
    "# # ANALYSIS FUNCTIONS\n",
    "# # ==============================================================================\n",
    "\n",
    "# def analyze_attention_weights(model, dataloader, feature_names, device, save_path):\n",
    "#     \"\"\"\n",
    "#     Analyzes and visualizes aggregated attention weights for prosodic features.\n",
    "#     \"\"\"\n",
    "#     print(\"\\n--- Running Attention Weight Analysis ---\")\n",
    "#     model.eval()\n",
    "#     # The attention is on the concatenated vector (64 CQCC + N prosodic features expanded to 64)\n",
    "#     # We are interested in the second half of the attention weights\n",
    "#     num_prosodic_features = len(feature_names)\n",
    "#     attention_scores = np.zeros(64) # Attention weights for the prosody branch\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "#             cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "#             _, weights = model(cqcc, prosody)\n",
    "#             # weights shape: [batch_size, 128 (64+64)]\n",
    "#             # We only care about the weights applied to the prosodic part of the vector\n",
    "#             prosody_attention_weights = weights[:, 64:]\n",
    "#             attention_scores += prosody_attention_weights.sum(dim=0).cpu().numpy()\n",
    "\n",
    "#     # Since the prosody branch is an MLP, we can't directly map these 64 weights back\n",
    "#     # to the original N features. This analysis shows the importance of the learned\n",
    "#     # prosodic representation, but not individual input features.\n",
    "#     # For a more direct analysis, feature ablation is better suited for this model architecture.\n",
    "#     print(\"NOTE: Attention analysis for this model shows importance of the *learned prosodic representation*.\")\n",
    "#     print(\"Feature ablation is recommended for analyzing original input feature importance.\")\n",
    "    \n",
    "#     # We can still plot the importance of the learned 64 prosodic dimensions\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.bar(range(64), attention_scores, color='purple')\n",
    "#     plt.xlabel('Dimension of Learned Prosodic Representation')\n",
    "#     plt.ylabel('Aggregated Attention Score')\n",
    "#     plt.title('Importance of Learned Prosodic Feature Dimensions')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path.replace(\".png\", \"_learned_dims.png\"))\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "#     \"\"\"\n",
    "#     Performs feature ablation to measure EER increase.\n",
    "#     \"\"\"\n",
    "#     print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "#     def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "#         model.eval()\n",
    "#         all_labels, all_scores = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for cqcc, prosody, labels in dataloader:\n",
    "#                 cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "#                 if feature_to_ablate is not None:\n",
    "#                     prosody[:, feature_to_ablate] = 0.0 # Zero out the feature\n",
    "                \n",
    "#                 outputs, _ = model(cqcc, prosody)\n",
    "#                 all_scores.extend(outputs.cpu().numpy())\n",
    "#                 all_labels.extend(labels.cpu().numpy())\n",
    "#         return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "#     baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "#     print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "#     eer_increases = {}\n",
    "#     for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "#         ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "#         eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "#     sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "#     print(\"\\nFeature Importance based on EER Increase:\")\n",
    "#     for feature, increase in sorted_features:\n",
    "#         print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "#     names = [item[0] for item in sorted_features]\n",
    "#     increases = [item[1] for item in sorted_features]\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.barh(names, increases, color='salmon')\n",
    "#     plt.xlabel('EER Increase (%)')\n",
    "#     plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path)\n",
    "#     print(f\"\\nAblation plot saved to {save_path}\")\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # MAIN EXECUTION BLOCK\n",
    "# # ==============================================================================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     try:\n",
    "#         print(\"--- Loading and Preparing Data ---\")\n",
    "#         prosody_df_train = pd.read_csv(PROSODIC_FEATURES_TRAIN_CSV_PATH)\n",
    "#         X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "#         prosody_df_val = pd.read_csv(PROSODIC_FEATURES_VAL_CSV_PATH)\n",
    "#         X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        \n",
    "#         if len(prosody_df_train) != len(X_cqcc_train) or len(prosody_df_val) != len(X_cqcc_val):\n",
    "#             raise ValueError(\"Sample count mismatch between CSV and .npy files.\")\n",
    "\n",
    "#         feature_columns = ['mean_f0', 'std_f0', 'jitter', 'shimmer', 'mean_hnr', 'std_hnr']\n",
    "#         X_prosody_train = prosody_df_train[feature_columns].values\n",
    "#         y_train = prosody_df_train['label'].values\n",
    "#         X_prosody_val = prosody_df_val[feature_columns].values\n",
    "#         y_val = prosody_df_val['label'].values\n",
    "#         print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "#     except (FileNotFoundError, ValueError) as e:\n",
    "#         print(f\"Error loading data: {e}\")\n",
    "#         exit()\n",
    "\n",
    "#     print(\"--- Scaling Data ---\")\n",
    "#     scaler_prosody = StandardScaler()\n",
    "#     X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "#     X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "#     scaler_cqcc = StandardScaler()\n",
    "#     nsamples, nx, ny = X_cqcc_train.shape\n",
    "#     X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "#     nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "#     X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "#     print(\"Scaling complete.\")\n",
    "\n",
    "#     train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "#     val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#     model = AttentionFusionCNN(\n",
    "#         cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "#         prosodic_features=X_prosody_train.shape[1]\n",
    "#     ).to(DEVICE)\n",
    "    \n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "#     print(model)\n",
    "    \n",
    "#     best_val_loss = float('inf')\n",
    "#     history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "#     print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "#             cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs, _ = model(cqcc_batch, prosody_batch) # Ignore weights during training\n",
    "#             loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         all_labels = []\n",
    "#         all_scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "#                 cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "#                 outputs, _ = model(cqcc_batch, prosody_batch) # Ignore weights during validation\n",
    "#                 loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "#                 val_loss += loss.item()\n",
    "#                 all_scores.extend(outputs.cpu().numpy())\n",
    "#                 all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "#         avg_train_loss = running_loss / len(train_loader)\n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "#         all_labels = np.array(all_labels)\n",
    "#         all_scores = np.array(all_scores).flatten()\n",
    "#         all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "#         val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "#         f1 = f1_score(all_labels, all_preds)\n",
    "#         eer = calculate_eer(all_labels, all_scores)\n",
    "#         cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "#         print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "#         print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "#         history['train_loss'].append(avg_train_loss)\n",
    "#         history['val_loss'].append(avg_val_loss)\n",
    "#         history['val_acc'].append(val_accuracy)\n",
    "#         history['f1'].append(f1)\n",
    "#         history['eer'].append(eer)\n",
    "        \n",
    "#         scheduler.step(avg_val_loss)\n",
    "\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "#             print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "#     print(\"\\n--- Training Complete ---\")\n",
    "#     print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "#     plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "#     # ==============================================================================\n",
    "#     # FINAL TESTING AND ANALYSIS\n",
    "#     # ==============================================================================\n",
    "#     print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "#     try:\n",
    "#         print(\"Loading test data...\")\n",
    "#         prosody_df_test_full = pd.read_csv(PROSODIC_FEATURES_TEST_CSV_PATH)\n",
    "#         X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "#         X_prosody_test_full = prosody_df_test_full[feature_columns].values\n",
    "#         y_test_full = prosody_df_test_full['label'].values\n",
    "        \n",
    "#         NUM_SAMPLES_TO_SELECT = min(70000, len(y_test_full))\n",
    "#         print(f\"Creating a balanced subset of {NUM_SAMPLES_TO_SELECT} samples for testing and analysis...\")\n",
    "\n",
    "#         _, _, _, _, _, selected_indices = train_test_split(\n",
    "#             X_cqcc_test_full, y_test_full, np.arange(len(y_test_full)),\n",
    "#             test_size=NUM_SAMPLES_TO_SELECT, stratify=y_test_full, random_state=42\n",
    "#         )\n",
    "\n",
    "#         X_cqcc_test_subset = X_cqcc_test_full[selected_indices]\n",
    "#         X_prosody_test_subset = X_prosody_test_full[selected_indices]\n",
    "#         y_test_subset = y_test_full[selected_indices]\n",
    "        \n",
    "#         X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_subset)\n",
    "#         nsamples_test, nx_test, ny_test = X_cqcc_test_subset.shape\n",
    "#         X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_subset.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "#         test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test_subset)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "#         print(\"Loading best model for testing and analysis...\")\n",
    "#         analysis_model = AttentionFusionCNN(\n",
    "#             cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "#             prosodic_features=X_prosody_train.shape[1]\n",
    "#         ).to(DEVICE)\n",
    "#         analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "#         analysis_model.eval()\n",
    "\n",
    "#         # --- First, get final test metrics ---\n",
    "#         all_test_labels = []\n",
    "#         all_test_scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "#                 cqcc_batch, prosody_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE)\n",
    "#                 outputs, _ = analysis_model(cqcc_batch, prosody_batch)\n",
    "#                 all_test_scores.extend(outputs.cpu().numpy())\n",
    "#                 all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "#         all_test_labels = np.array(all_test_labels)\n",
    "#         all_test_scores = np.array(all_test_scores).flatten()\n",
    "#         all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "#         test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "#         test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "#         test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "#         test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "#         print(\"\\n--- Final Test Results ---\")\n",
    "#         print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "#         print(f\"F1-Score: {test_f1:.4f}\")\n",
    "#         print(f\"EER: {test_eer:.2f}%\")\n",
    "#         print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "#         # --- Now, run analysis functions ---\n",
    "#         analyze_attention_weights(analysis_model, test_loader, feature_columns, DEVICE, ATTENTION_PLOT_PATH)\n",
    "#         perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "\n",
    "#     except (FileNotFoundError, ValueError) as e:\n",
    "#         print(f\"Error during testing/analysis: {e}\")\n",
    "#         print(\"Please ensure your test data files are in the correct paths and format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data/cqcc_features.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_CSV_PATH = \"processed_data/prosodic_features_and_labels.csv\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data/cqcc_features_val.npy\"\n",
    "PROSODIC_FEATURES_VAL_CSV_PATH = \"processed_data/prosodic_features_and_labels_val.csv\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_CSV_PATH = \"processed_data/prosodic_features_and_labels_test.csv\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # IMPORTANT: This forward pass now returns attention weights\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        \n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes aggregated attention weights for prosodic features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    attention_scores = np.zeros(64) # Attention weights for the prosody branch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            prosody_attention_weights = weights[:, 64:]\n",
    "            attention_scores += prosody_attention_weights.sum(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"NOTE: Attention analysis for this model shows importance of the *learned prosodic representation*.\")\n",
    "    print(\"Feature ablation is recommended for analyzing original input feature importance.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(64), attention_scores, color='purple')\n",
    "    plt.xlabel('Dimension of Learned Prosodic Representation')\n",
    "    plt.ylabel('Aggregated Attention Score')\n",
    "    plt.title('Importance of Learned Prosodic Feature Dimensions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".png\", \"_learned_dims.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Performs feature ablation to measure EER increase.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody[:, feature_to_ablate] = 0.0 # Zero out the feature\n",
    "                \n",
    "                outputs, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Method 3: Uses SHAP to explain model predictions for prosodic features.\n",
    "    This is computationally intensive and is run on a subset of data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "\n",
    "    # Get a small batch of data for SHAP analysis\n",
    "    # SHAP needs a background dataset to integrate over, and a test set to explain\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    # We'll explain the predictions for another batch\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    # SHAP's KernelExplainer needs a function that takes a numpy array.\n",
    "    # We create a wrapper that takes only the prosodic features (as a numpy array),\n",
    "    # combines them with a fixed background CQCC sample, and returns the model's prediction.\n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        # Number of samples SHAP is currently testing\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        \n",
    "        # Convert prosodic numpy array to a tensor\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        \n",
    "        # Use a single CQCC sample as a fixed background for all predictions\n",
    "        # and expand it to match the batch size of the prosody samples\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device) # Take the first sample\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(cqcc_tensor, prosody_tensor)\n",
    "        \n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    # Create the explainer\n",
    "    # We use the background prosody data to initialize the explainer\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    \n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    # Calculate SHAP values for the test prosody data\n",
    "    # Using a small number of samples for demonstration purposes\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    \n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    # The output of shap_values for a single-output model might be a list\n",
    "    # We take the first element if it is\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    # Create the summary plot\n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    \n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        prosody_df_train = pd.read_csv(PROSODIC_FEATURES_TRAIN_CSV_PATH)\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        prosody_df_val = pd.read_csv(PROSODIC_FEATURES_VAL_CSV_PATH)\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        \n",
    "        if len(prosody_df_train) != len(X_cqcc_train) or len(prosody_df_val) != len(X_cqcc_val):\n",
    "            raise ValueError(\"Sample count mismatch between CSV and .npy files.\")\n",
    "\n",
    "        feature_columns = ['mean_f0', 'std_f0', 'jitter', 'shimmer', 'mean_hnr', 'std_hnr']\n",
    "        X_prosody_train = prosody_df_train[feature_columns].values\n",
    "        y_train = prosody_df_train['label'].values\n",
    "        X_prosody_val = prosody_df_val[feature_columns].values\n",
    "        y_val = prosody_df_val['label'].values\n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(cqcc_batch, prosody_batch) # Ignore weights during training\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs, _ = model(cqcc_batch, prosody_batch) # Ignore weights during validation\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # FINAL TESTING AND ANALYSIS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "    try:\n",
    "        print(\"Loading test data...\")\n",
    "        prosody_df_test_full = pd.read_csv(PROSODIC_FEATURES_TEST_CSV_PATH)\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_full = prosody_df_test_full[feature_columns].values\n",
    "        y_test_full = prosody_df_test_full['label'].values\n",
    "        \n",
    "        NUM_SAMPLES_TO_SELECT = min(70000, len(y_test_full))\n",
    "        print(f\"Creating a balanced subset of {NUM_SAMPLES_TO_SELECT} samples for testing and analysis...\")\n",
    "\n",
    "        _, _, _, _, _, selected_indices = train_test_split(\n",
    "            X_cqcc_test_full, y_test_full, np.arange(len(y_test_full)),\n",
    "            test_size=NUM_SAMPLES_TO_SELECT, stratify=y_test_full, random_state=42\n",
    "        )\n",
    "\n",
    "        X_cqcc_test_subset = X_cqcc_test_full[selected_indices]\n",
    "        X_prosody_test_subset = X_prosody_test_full[selected_indices]\n",
    "        y_test_subset = y_test_full[selected_indices]\n",
    "        \n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_subset)\n",
    "        nsamples_test, nx_test, ny_test = X_cqcc_test_subset.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_subset.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test_subset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(\"Loading best model for testing and analysis...\")\n",
    "        analysis_model = AttentionFusionCNN(\n",
    "            cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "            prosodic_features=X_prosody_train.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        # --- First, get final test metrics ---\n",
    "        all_test_labels = []\n",
    "        all_test_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "                cqcc_batch, prosody_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE)\n",
    "                outputs, _ = analysis_model(cqcc_batch, prosody_batch)\n",
    "                all_test_scores.extend(outputs.cpu().numpy())\n",
    "                all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        all_test_scores = np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "        # --- Now, run analysis functions ---\n",
    "        analyze_attention_weights(analysis_model, test_loader, feature_columns, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        print(\"Please ensure your test data files are in the correct paths and format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # IMPORTANT: This forward pass now returns attention weights\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        \n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes aggregated attention weights for prosodic features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    attention_scores = np.zeros(64) # Attention weights for the prosody branch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            prosody_attention_weights = weights[:, 64:]\n",
    "            attention_scores += prosody_attention_weights.sum(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"NOTE: Attention analysis for this model shows importance of the *learned prosodic representation*.\")\n",
    "    print(\"Feature ablation is recommended for analyzing original input feature importance.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(64), attention_scores, color='purple')\n",
    "    plt.xlabel('Dimension of Learned Prosodic Representation')\n",
    "    plt.ylabel('Aggregated Attention Score')\n",
    "    plt.title('Importance of Learned Prosodic Feature Dimensions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".png\", \"_learned_dims.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Performs feature ablation to measure EER increase.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody[:, feature_to_ablate] = 0.0 # Zero out the feature\n",
    "                \n",
    "                outputs, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10)) # Increased figure height for more features\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Method 3: Uses SHAP to explain model predictions for prosodic features.\n",
    "    This is computationally intensive and is run on a subset of data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(cqcc_tensor, prosody_tensor)\n",
    "        \n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    \n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    \n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10)) # Ensure plot is large enough\n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    \n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        # Load all data from .npy files\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # The LLD prosodic features are 3D (samples, features, time).\n",
    "        # This model expects 2D summary statistics for prosody.\n",
    "        # We convert the 3D data to 2D by taking the mean across the time axis.\n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        # Assumes shape is (samples, features, time), so we take mean over axis 2\n",
    "        X_prosody_train = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        if not (len(X_cqcc_train) == len(X_prosody_train) == len(y_train)):\n",
    "            raise ValueError(\"Sample count mismatch in training files.\")\n",
    "        if not (len(X_cqcc_val) == len(X_prosody_val) == len(y_val)):\n",
    "            raise ValueError(\"Sample count mismatch in validation files.\")\n",
    "\n",
    "        # Generate generic feature names since they are not in the .npy file\n",
    "        num_prosodic_features = X_prosody_train.shape[1]\n",
    "        feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "        \n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "        print(f\"Using {num_prosodic_features} prosodic features.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    # This now works because X_prosody_train is 2D\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    # This scaling method flattens feature and time dimensions together.\n",
    "    # It's a form of instance-level normalization.\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # FINAL TESTING AND ANALYSIS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "    try:\n",
    "        print(\"Loading test data...\")\n",
    "        X_cqcc_test = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # Convert 3D test prosodic features to 2D to match training\n",
    "        X_prosody_test = np.mean(X_prosody_test_3d, axis=2)\n",
    "        \n",
    "        print(f\"Loaded {len(y_test)} test samples.\")\n",
    "        \n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test)\n",
    "        nsamples_test, nx_test, ny_test = X_cqcc_test.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(\"Loading best model for testing and analysis...\")\n",
    "        analysis_model = AttentionFusionCNN(\n",
    "            cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "            prosodic_features=X_prosody_train.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        all_test_labels = []\n",
    "        all_test_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "                cqcc_batch, prosody_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE)\n",
    "                outputs, _ = analysis_model(cqcc_batch, prosody_batch)\n",
    "                all_test_scores.extend(outputs.cpu().numpy())\n",
    "                all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        all_test_scores = np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "        analyze_attention_weights(analysis_model, test_loader, feature_columns, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths and format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101272df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure this path is correct for your environment\n",
    "TEAMMATE_DATA_PATH = '/mount/studenten/arbeitsdaten-studenten1/team-lab-phonetics/2025/student_directories/AuFa/'\n",
    "PREPROCESSED_DATA_DIR = os.path.join(TEAMMATE_DATA_PATH, \"processed_data_aligned_lld\")\n",
    "OUTPUT_DIR = os.path.join(TEAMMATE_DATA_PATH, \"single_stream_transformer_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Transformers can be memory intensive\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "CQCC_SHAPE = (128, 157)\n",
    "EGMAPS_LLD_SHAPE = (23, 157)\n",
    "EMBEDDING_DIM = 128 # d_model for the transformer\n",
    "NUM_HEADS = 8      # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6 # Can use a deeper single encoder\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# --- Analysis Configuration ---\n",
    "ABLATION_PLOT_PATH = os.path.join(OUTPUT_DIR, \"ablation_importance.png\")\n",
    "SHAP_PLOT_PATH = os.path.join(OUTPUT_DIR, \"shap_importance.png\")\n",
    "\n",
    "\n",
    "# --- 2. UTILITY FUNCTIONS & DATASET CLASS ---\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER). Returns -1 if calculation fails.\"\"\"\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        return eer * 100\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -1.0\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves a comprehensive training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', color=color, fontsize=14)\n",
    "    ax1.plot(epochs_range, history['train_loss'], color=color, linestyle='--', marker='o', label='Train Loss')\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, linestyle='-', marker='o', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color, fontsize=14)\n",
    "    ax2.plot(epochs_range, history['train_acc'], color=color, linestyle='--', marker='s', label='Train Accuracy')\n",
    "    ax2.plot(epochs_range, history['val_acc'], color=color, linestyle='-', marker='s', label='Val Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    color = 'tab:green'\n",
    "    ax3.set_ylabel('EER (%)', color=color, fontsize=14)\n",
    "    ax3.plot(epochs_range, history['val_eer'], color=color, linestyle=':', marker='^', label='Val EER')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax3.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\n📈 Training plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, cqcc_data, egmaps_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.egmaps_data = torch.tensor(egmaps_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.egmaps_data[idx], self.labels[idx]\n",
    "\n",
    "# --- ADDED: ANALYSIS FUNCTIONS ---\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Performs feature ablation on the 3D LLD data to measure EER increase.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    # Zero out the specified feature channel across all time steps\n",
    "                    # Input shape is (batch, features, time)\n",
    "                    prosody[:, feature_to_ablate, :] = 0.0 \n",
    "                \n",
    "                outputs = model(cqcc, prosody)\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Uses SHAP to explain model predictions based on the mean of LLD features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "\n",
    "    background_cqcc, background_prosody_3d, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody_3d, _ = next(iter(dataloader))\n",
    "    \n",
    "    # SHAP works best with 2D data, so we'll analyze the mean of the LLDs\n",
    "    background_prosody_2d = np.mean(background_prosody_3d.numpy(), axis=2)\n",
    "    test_prosody_2d = np.mean(test_prosody_3d.numpy(), axis=2)\n",
    "\n",
    "    def model_wrapper(prosodic_features_2d_numpy):\n",
    "        num_samples = prosodic_features_2d_numpy.shape[0]\n",
    "        \n",
    "        # Expand the 2D summary stats back to a 3D sequence by repeating\n",
    "        # This is an approximation to make the data compatible with the model\n",
    "        time_steps = EGMAPS_LLD_SHAPE[1]\n",
    "        prosody_3d_numpy = np.repeat(prosodic_features_2d_numpy[:, :, np.newaxis], time_steps, axis=2)\n",
    "        prosody_tensor = torch.from_numpy(prosody_3d_numpy).float().to(device)\n",
    "        \n",
    "        # Use a fixed CQCC background for all SHAP predictions\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(cqcc_tensor, prosody_tensor)\n",
    "        \n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody_2d)\n",
    "    \n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody_2d, nsamples=100)\n",
    "    \n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, test_prosody_2d, feature_names=feature_names, show=False)\n",
    "    \n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# --- 3. SINGLE-STREAM FUSION TRANSFORMER MODEL ---\n",
    "\n",
    "class SingleStreamFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses CQCC and eGeMAPS by concatenating them into a single sequence\n",
    "    and feeding them to a single Transformer Encoder. Uses token-type embeddings\n",
    "    to distinguish between the two modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_features, egmaps_features, time_steps, d_model, nhead, num_encoder_layers, dropout):\n",
    "        super(SingleStreamFusionTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.cqcc_projection = nn.Linear(cqcc_features, d_model)\n",
    "        self.egmaps_projection = nn.Linear(egmaps_features, d_model)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.token_type_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=d_model)\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1 + time_steps * 2, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, egmaps_x):\n",
    "        # Input shapes are expected to be (batch, features, time)\n",
    "        # Transpose to (batch, time, features) for linear projection\n",
    "        cqcc_x = cqcc_x.transpose(1, 2)\n",
    "        egmaps_x = egmaps_x.transpose(1, 2)\n",
    "        \n",
    "        batch_size = cqcc_x.size(0)\n",
    "        time_steps = cqcc_x.size(1)\n",
    "        \n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x)\n",
    "        egmaps_embed = self.egmaps_projection(egmaps_x)\n",
    "        \n",
    "        cqcc_type_ids = torch.zeros(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        egmaps_type_ids = torch.ones(batch_size, time_steps, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        cqcc_type_embed = self.token_type_embeddings(cqcc_type_ids)\n",
    "        egmaps_type_embed = self.token_type_embeddings(egmaps_type_ids)\n",
    "\n",
    "        cqcc_embed += cqcc_type_embed\n",
    "        egmaps_embed += egmaps_type_embed\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        full_sequence = torch.cat([cls_tokens, cqcc_embed, egmaps_embed], dim=1)\n",
    "        \n",
    "        full_sequence += self.positional_encoding\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(full_sequence)\n",
    "        \n",
    "        cls_output = transformer_out[:, 0, :]\n",
    "        output = self.classifier(cls_output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# --- 4. MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Data ---\")\n",
    "        X_cqcc_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_train.npy\"))\n",
    "        X_lld_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_train.npy\"))\n",
    "        y_train = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_train.npy\"))\n",
    "        X_cqcc_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_dev.npy\"))\n",
    "        X_lld_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_dev.npy\"))\n",
    "        y_val = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_dev.npy\"))\n",
    "        \n",
    "        # Define the 23 eGeMAPS LLD feature names\n",
    "        feature_columns = [\n",
    "            'F0semitoneFrom27.5Hz_sma3nz', 'jitterLocal_sma3nz', 'shimmerLocaldB_sma3nz',\n",
    "            'Loudness_sma3', 'HNRdBACF_sma3nz', 'logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz', 'F1frequency_sma3nz', 'F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz', 'F2frequency_sma3nz', 'F2bandwidth_sma3nz',\n",
    "            'F2amplitudeLogRelF0_sma3nz', 'F3frequency_sma3nz', 'F3bandwidth_sma3nz',\n",
    "            'F3amplitudeLogRelF0_sma3nz', 'alphaRatio_sma3', 'hammarbergIndex_sma3',\n",
    "            'slope0-500_sma3', 'slope500-1500_sma3', 'spectralFlux_sma3',\n",
    "            'mfcc1_sma3', 'mfcc2_sma3'\n",
    "        ]\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Features ---\")\n",
    "    # Reshape to 2D for scaler: (samples * time, features)\n",
    "    scaler_lld = StandardScaler().fit(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0]))\n",
    "    X_lld_train_scaled = scaler_lld.transform(X_lld_train.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_train.shape)\n",
    "    X_lld_val_scaled = scaler_lld.transform(X_lld_val.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_val.shape)\n",
    "    \n",
    "    scaler_cqcc = StandardScaler().fit(X_cqcc_train.reshape(-1, CQCC_SHAPE[0]))\n",
    "    X_cqcc_train_scaled = scaler_cqcc.transform(X_cqcc_train.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_train.shape)\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_val.shape)\n",
    "    \n",
    "    print(\"--- Saving Scalers ---\")\n",
    "    joblib.dump(scaler_cqcc, os.path.join(OUTPUT_DIR, \"scaler_cqcc.joblib\"))\n",
    "    joblib.dump(scaler_lld, os.path.join(OUTPUT_DIR, \"scaler_lld.joblib\"))\n",
    "    print(f\"✅ Scalers saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_lld_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_lld_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    best_val_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_eer': []}\n",
    "    \n",
    "    print(f\"\\n--- Starting Training: Single-Stream Fusion Transformer Model ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_labels, train_preds = [], []\n",
    "        \n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cqcc_batch, lld_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_labels, val_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, lld_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]  \"):\n",
    "                cqcc_batch, lld_batch, labels_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs = model(cqcc_batch, lld_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_scores.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_preds_binary = (np.array(train_preds) > 0.5).astype(int).flatten()\n",
    "        train_acc = accuracy_score(train_labels, train_preds_binary) * 100\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_scores = np.array(val_scores).flatten()\n",
    "        val_preds_binary = (val_scores > 0.5).astype(int)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_binary) * 100\n",
    "        val_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        val_eer = calculate_eer(val_labels, val_scores)\n",
    "        cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_eer'].append(val_eer)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val EER: {val_eer:.2f}%\")\n",
    "        print(\"  Validation Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if val_eer > 0 and val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\"))\n",
    "            print(f\"  -> ✅ New best model saved with EER: {best_val_eer:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    plot_training_history(history, os.path.join(OUTPUT_DIR, \"training_history_single_stream.png\"))\n",
    "    \n",
    "    # --- FINAL EVALUATION AND ANALYSIS ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Starting Final Evaluation and Analysis on Test Set ---\")\n",
    "    \n",
    "    try:\n",
    "        print(\"--- Loading Test Data ---\")\n",
    "        X_cqcc_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"cqcc_features_test.npy\"))\n",
    "        X_lld_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"egmaps_lld_features_test.npy\"))\n",
    "        y_test = np.load(os.path.join(PREPROCESSED_DATA_DIR, \"labels_test.npy\"))\n",
    "        print(f\"✅ Loaded {len(y_test)} test samples.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error loading test data files: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Test Features ---\")\n",
    "    X_lld_test_scaled = scaler_lld.transform(X_lld_test.reshape(-1, EGMAPS_LLD_SHAPE[0])).reshape(X_lld_test.shape)\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(-1, CQCC_SHAPE[0])).reshape(X_cqcc_test.shape)\n",
    "    \n",
    "    test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_lld_test_scaled, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"--- Loading Best Trained Model for Testing ---\")\n",
    "    analysis_model = SingleStreamFusionTransformer(\n",
    "        cqcc_features=CQCC_SHAPE[0],\n",
    "        egmaps_features=EGMAPS_LLD_SHAPE[0],\n",
    "        time_steps=CQCC_SHAPE[1],\n",
    "        d_model=EMBEDDING_DIM,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model_path = os.path.join(OUTPUT_DIR, \"best_single_stream_transformer_model.pth\")\n",
    "    try:\n",
    "        analysis_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(\"✅ Model weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Model file not found at {model_path}\")\n",
    "        exit()\n",
    "\n",
    "    analysis_model.eval()\n",
    "    \n",
    "    test_labels, test_scores = [], []\n",
    "    with torch.no_grad():\n",
    "        for cqcc_batch, lld_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "            cqcc_batch, lld_batch = cqcc_batch.to(DEVICE), lld_batch.to(DEVICE)\n",
    "            outputs = analysis_model(cqcc_batch, lld_batch)\n",
    "            test_scores.extend(outputs.cpu().numpy())\n",
    "            test_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    test_labels = np.array(test_labels)\n",
    "    test_scores = np.array(test_scores).flatten()\n",
    "    test_preds_binary = (test_scores > 0.5).astype(int)\n",
    "    \n",
    "    test_acc = accuracy_score(test_labels, test_preds_binary) * 100\n",
    "    test_f1 = f1_score(test_labels, test_preds_binary)\n",
    "    test_eer = calculate_eer(test_labels, test_scores)\n",
    "    test_cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"--- Final Test Results ---\")\n",
    "    print(f\"  Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  EER:        {test_eer:.2f}%\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # --- Run Analysis ---\n",
    "    perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "    analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # IMPORTANT: This forward pass now returns attention weights\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        \n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes aggregated attention weights for prosodic features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    attention_scores = np.zeros(64) # Attention weights for the prosody branch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            prosody_attention_weights = weights[:, 64:]\n",
    "            attention_scores += prosody_attention_weights.sum(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"NOTE: Attention analysis for this model shows importance of the *learned prosodic representation*.\")\n",
    "    print(\"Feature ablation is recommended for analyzing original input feature importance.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(64), attention_scores, color='purple')\n",
    "    plt.xlabel('Dimension of Learned Prosodic Representation')\n",
    "    plt.ylabel('Aggregated Attention Score')\n",
    "    plt.title('Importance of Learned Prosodic Feature Dimensions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".png\", \"_learned_dims.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Performs feature ablation to measure EER increase.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody[:, feature_to_ablate] = 0.0 # Zero out the feature\n",
    "                \n",
    "                outputs, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10)) # Increased figure height for more features\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Method 3: Uses SHAP to explain model predictions for prosodic features.\n",
    "    This is computationally intensive and is run on a subset of data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(cqcc_tensor, prosody_tensor)\n",
    "        \n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    \n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    \n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10)) # Ensure plot is large enough\n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    \n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        # Load all data from .npy files\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # The LLD prosodic features are 3D (samples, features, time).\n",
    "        # This model expects 2D summary statistics for prosody.\n",
    "        # We convert the 3D data to 2D by taking the mean across the time axis.\n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        # Assumes shape is (samples, features, time), so we take mean over axis 2\n",
    "        X_prosody_train = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        if not (len(X_cqcc_train) == len(X_prosody_train) == len(y_train)):\n",
    "            raise ValueError(\"Sample count mismatch in training files.\")\n",
    "        if not (len(X_cqcc_val) == len(X_prosody_val) == len(y_val)):\n",
    "            raise ValueError(\"Sample count mismatch in validation files.\")\n",
    "\n",
    "        # --- UPDATED: Define the 23 eGeMAPS LLD feature names ---\n",
    "        # This list corresponds to the standard eGeMAPS v01b LLD set.\n",
    "        # The script calculates the mean of these, so these are \"mean LLDs\".\n",
    "        # IMPORTANT: Please verify this order matches your feature extraction script.\n",
    "        feature_columns = [\n",
    "            'F0semitoneFrom27.5Hz_sma3nz_amean',\n",
    "            'jitterLocal_sma3nz_amean',\n",
    "            'shimmerLocaldB_sma3nz_amean',\n",
    "            'Loudness_sma3_amean',\n",
    "            'HNRdBACF_sma3nz_amean',\n",
    "            'logRelF0-H1-H2_sma3nz_amean',\n",
    "            'logRelF0-H1-A3_sma3nz_amean',\n",
    "            'F1frequency_sma3nz_amean',\n",
    "            'F1bandwidth_sma3nz_amean',\n",
    "            'F1amplitudeLogRelF0_sma3nz_amean',\n",
    "            'F2frequency_sma3nz_amean',\n",
    "            'F2bandwidth_sma3nz_amean',\n",
    "            'F2amplitudeLogRelF0_sma3nz_amean',\n",
    "            'F3frequency_sma3nz_amean',\n",
    "            'F3bandwidth_sma3nz_amean',\n",
    "            'F3amplitudeLogRelF0_sma3nz_amean',\n",
    "            'alphaRatio_sma3_amean',\n",
    "            'hammarbergIndex_sma3_amean',\n",
    "            'slope0-500_sma3_amean',\n",
    "            'slope500-1500_sma3_amean',\n",
    "            'spectralFlux_sma3_amean',\n",
    "            'mfcc1_sma3_amean',\n",
    "            'mfcc2_sma3_amean'\n",
    "        ]\n",
    "        \n",
    "        num_prosodic_features = X_prosody_train.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            print(f\"Warning: Provided feature name count ({len(feature_columns)}) does not match data ({num_prosodic_features}). Using generic names.\")\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "        print(f\"Using {num_prosodic_features} prosodic features.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    # This now works because X_prosody_train is 2D\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    # This scaling method flattens feature and time dimensions together.\n",
    "    # It's a form of instance-level normalization.\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # FINAL TESTING AND ANALYSIS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "    try:\n",
    "        print(\"Loading test data...\")\n",
    "        X_cqcc_test = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # Convert 3D test prosodic features to 2D to match training\n",
    "        X_prosody_test = np.mean(X_prosody_test_3d, axis=2)\n",
    "        \n",
    "        print(f\"Loaded {len(y_test)} test samples.\")\n",
    "        \n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test)\n",
    "        nsamples_test, nx_test, ny_test = X_cqcc_test.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(\"Loading best model for testing and analysis...\")\n",
    "        analysis_model = AttentionFusionCNN(\n",
    "            cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "            prosodic_features=X_prosody_train.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        all_test_labels = []\n",
    "        all_test_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "                cqcc_batch, prosody_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE)\n",
    "                outputs, _ = analysis_model(cqcc_batch, prosody_batch)\n",
    "                all_test_scores.extend(outputs.cpu().numpy())\n",
    "                all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        all_test_scores = np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "        analyze_attention_weights(analysis_model, test_loader, feature_columns, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths and format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# from scipy.fftpack import dct\n",
    "# from tqdm import tqdm\n",
    "# import soundfile as sf\n",
    "# import opensmile\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # --- Initialize openSMILE for LLDs ---\n",
    "#     smile = opensmile.Smile(\n",
    "#         feature_set=opensmile.FeatureSet.eGeMAPS,\n",
    "#         feature_level=opensmile.FeatureLevel.LowLevelDescriptors, # Use the full name, # Set to LLD\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb713a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in smile.feature_names:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in smile.feature_set:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification Snippet ---\n",
    "\n",
    "# 1. Initialize openSMILE (as you already do)\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPS,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")\n",
    "\n",
    "# 2. Get the list of feature names directly from the instance\n",
    "feature_names_from_smile = smile.feature_names\n",
    "\n",
    "# 3. Process a SINGLE audio file to get the DataFrame\n",
    "#    Replace with a real path to one of your audio files.\n",
    "try:\n",
    "    single_audio_file_path = \"audio_test/LA_E_1002903.flac\" \n",
    "    lld_df = smile.process_file(single_audio_file_path)\n",
    "\n",
    "    print(\"--- Verifying Feature Names and Values ---\")\n",
    "    \n",
    "    # 4. Iterate through the feature names and get the values\n",
    "    for feature_name in feature_names_from_smile:\n",
    "        # Get the corresponding column (a pandas Series) from the DataFrame\n",
    "        feature_values = lld_df[feature_name]\n",
    "        \n",
    "        # Print the name and the first 3 values for that feature\n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        print(f\"  First 3 values: {feature_values.head(3).values}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Verification skipped: Could not find the example file at '{single_audio_file_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during verification: {e}\")\n",
    "\n",
    "# --- End of Verification Snippet ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AttentionFusionCNN_2D_PyTorch_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_23feat.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "class AttentionFusionCNN(nn.Module):\n",
    "    \"\"\"PyTorch implementation using Conv2D for CQCC features.\"\"\"\n",
    "    def __init__(self, cqcc_input_shape, prosodic_features):\n",
    "        super(AttentionFusionCNN, self).__init__()\n",
    "        \n",
    "        self.cqcc_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn1 = nn.BatchNorm2d(16)\n",
    "        self.cqcc_pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.cqcc_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.cqcc_bn2 = nn.BatchNorm2d(32)\n",
    "        self.cqcc_pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_cqcc = torch.zeros(1, 1, *cqcc_input_shape)\n",
    "            dummy_out = self.cqcc_pool2(self.cqcc_bn2(self.cqcc_conv2(self.cqcc_pool1(self.cqcc_bn1(self.cqcc_conv1(dummy_cqcc))))))\n",
    "            cqcc_flat_size = dummy_out.numel()\n",
    "            \n",
    "        self.cqcc_fc = nn.Linear(cqcc_flat_size, 64)\n",
    "        self.prosody_fc1 = nn.Linear(prosodic_features, 32)\n",
    "        self.prosody_bn1 = nn.BatchNorm1d(32)\n",
    "        self.prosody_dropout = nn.Dropout(0.4)\n",
    "        self.prosody_fc2 = nn.Linear(32, 64)\n",
    "        concatenated_size = 64 + 64\n",
    "        self.attention = nn.Linear(concatenated_size, concatenated_size)\n",
    "        self.classifier_fc1 = nn.Linear(concatenated_size, 64)\n",
    "        self.classifier_bn = nn.BatchNorm1d(64)\n",
    "        self.classifier_dropout = nn.Dropout(0.5)\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # IMPORTANT: This forward pass now returns attention weights\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn1(self.cqcc_conv1(cqcc_x)))\n",
    "        cqcc_out = self.cqcc_pool1(cqcc_out)\n",
    "        cqcc_out = torch.relu(self.cqcc_bn2(self.cqcc_conv2(cqcc_out)))\n",
    "        cqcc_out = self.cqcc_pool2(cqcc_out)\n",
    "        cqcc_out = torch.flatten(cqcc_out, 1)\n",
    "        cqcc_branch_out = torch.relu(self.cqcc_fc(cqcc_out))\n",
    "\n",
    "        prosody_out = torch.relu(self.prosody_bn1(self.prosody_fc1(prosody_x)))\n",
    "        prosody_out = self.prosody_dropout(prosody_out)\n",
    "        prosody_branch_out = torch.relu(self.prosody_fc2(prosody_out))\n",
    "\n",
    "        concatenated = torch.cat([cqcc_branch_out, prosody_branch_out], dim=1)\n",
    "        \n",
    "        attention_weights = torch.softmax(self.attention(concatenated), dim=1)\n",
    "        fused = concatenated * attention_weights\n",
    "\n",
    "        x = torch.relu(self.classifier_bn(self.classifier_fc1(fused)))\n",
    "        x = self.classifier_dropout(x)\n",
    "        output = torch.sigmoid(self.output_fc(x))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes aggregated attention weights for prosodic features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    attention_scores = np.zeros(64) # Attention weights for the prosody branch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            prosody_attention_weights = weights[:, 64:]\n",
    "            attention_scores += prosody_attention_weights.sum(dim=0).cpu().numpy()\n",
    "\n",
    "    print(\"NOTE: Attention analysis for this model shows importance of the *learned prosodic representation*.\")\n",
    "    print(\"Feature ablation is recommended for analyzing original input feature importance.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(64), attention_scores, color='purple')\n",
    "    plt.xlabel('Dimension of Learned Prosodic Representation')\n",
    "    plt.ylabel('Aggregated Attention Score')\n",
    "    plt.title('Importance of Learned Prosodic Feature Dimensions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".png\", \"_learned_dims.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Performs feature ablation to measure EER increase.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    \n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody[:, feature_to_ablate] = 0.0 # Zero out the feature\n",
    "                \n",
    "                outputs, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    \n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "        \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10)) # Increased figure height for more features\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    \"\"\"\n",
    "    Method 3: Uses SHAP to explain model predictions for prosodic features.\n",
    "    This is computationally intensive and is run on a subset of data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(cqcc_tensor, prosody_tensor)\n",
    "        \n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    \n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    \n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10)) # Ensure plot is large enough\n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    \n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        # Load all data from .npy files\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # The LLD prosodic features are 3D (samples, features, time).\n",
    "        # This model expects 2D summary statistics for prosody.\n",
    "        # We convert the 3D data to 2D by taking the mean across the time axis.\n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        # Assumes shape is (samples, features, time), so we take mean over axis 2\n",
    "        X_prosody_train = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        if not (len(X_cqcc_train) == len(X_prosody_train) == len(y_train)):\n",
    "            raise ValueError(\"Sample count mismatch in training files.\")\n",
    "        if not (len(X_cqcc_val) == len(X_prosody_val) == len(y_val)):\n",
    "            raise ValueError(\"Sample count mismatch in validation files.\")\n",
    "\n",
    "        # --- UPDATED: Define the 23 feature names based on user verification ---\n",
    "        feature_columns = [\n",
    "            'Loudness_sma3',\n",
    "            'alphaRatio_sma3',\n",
    "            'hammarbergIndex_sma3',\n",
    "            'slope0-500_sma3',\n",
    "            'slope500-1500_sma3',\n",
    "            'spectralFlux_sma3',\n",
    "            'mfcc1_sma3',\n",
    "            'mfcc2_sma3',\n",
    "            'mfcc3_sma3',\n",
    "            'mfcc4_sma3',\n",
    "            'F0semitoneFrom27.5Hz_sma3nz',\n",
    "            'jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz',\n",
    "            'HNRdBACF_sma3nz',\n",
    "            'logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz',\n",
    "            'F1frequency_sma3nz',\n",
    "            'F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz',\n",
    "            'F2frequency_sma3nz',\n",
    "            'F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz',\n",
    "            'F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        \n",
    "        num_prosodic_features = X_prosody_train.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            print(f\"Warning: Provided feature name count ({len(feature_columns)}) does not match data ({num_prosodic_features}). Using generic names.\")\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "        print(f\"Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")\n",
    "        print(f\"Using {num_prosodic_features} prosodic features.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    # This now works because X_prosody_train is 2D\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "\n",
    "    # This scaling method flattens feature and time dimensions together.\n",
    "    # It's a form of instance-level normalization.\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    nsamples, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(nsamples, -1)).reshape(nsamples, nx, ny)\n",
    "    nsamples_val, nx_val, ny_val = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsamples_val, -1)).reshape(nsamples_val, nx_val, ny_val)\n",
    "    print(\"Scaling complete.\")\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AttentionFusionCNN(\n",
    "        cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "        prosodic_features=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for cqcc_batch, prosody_batch, labels_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "            loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in val_loader:\n",
    "                cqcc_batch, prosody_batch, labels_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "                outputs, _ = model(cqcc_batch, prosody_batch)\n",
    "                loss = criterion(outputs, labels_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_scores = np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        print(\"Validation Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['f1'].append(f1)\n",
    "        history['eer'].append(eer)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # FINAL TESTING AND ANALYSIS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "    try:\n",
    "        print(\"Loading test data...\")\n",
    "        X_cqcc_test = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        \n",
    "        # --- FIX for ValueError ---\n",
    "        # Convert 3D test prosodic features to 2D to match training\n",
    "        X_prosody_test = np.mean(X_prosody_test_3d, axis=2)\n",
    "        \n",
    "        print(f\"Loaded {len(y_test)} test samples.\")\n",
    "        \n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test)\n",
    "        nsamples_test, nx_test, ny_test = X_cqcc_test.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(nsamples_test, -1)).reshape(nsamples_test, nx_test, ny_test)\n",
    "        \n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(\"Loading best model for testing and analysis...\")\n",
    "        analysis_model = AttentionFusionCNN(\n",
    "            cqcc_input_shape=(X_cqcc_train.shape[1], X_cqcc_train.shape[2]),\n",
    "            prosodic_features=X_prosody_train.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        all_test_labels = []\n",
    "        all_test_scores = []\n",
    "        with torch.no_grad():\n",
    "            for cqcc_batch, prosody_batch, labels_batch in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "                cqcc_batch, prosody_batch = cqcc_batch.to(DEVICE), prosody_batch.to(DEVICE)\n",
    "                outputs, _ = analysis_model(cqcc_batch, prosody_batch)\n",
    "                all_test_scores.extend(outputs.cpu().numpy())\n",
    "                all_test_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        all_test_scores = np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "        analyze_attention_weights(analysis_model, test_loader, feature_columns, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        print(\"Please ensure your .npy data files are in the correct paths and format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 25379 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/20 | Train Loss: 0.2859, Train Acc: 0.8764 | Val Loss: 0.1732, Val EER: 0.1205, Val F1: 0.9556\n",
      "-> New best model saved with EER: 0.1205\n",
      "Epoch 2/20 | Train Loss: 0.1424, Train Acc: 0.9406 | Val Loss: 0.1453, Val EER: 0.0922, Val F1: 0.9673\n",
      "-> New best model saved with EER: 0.0922\n",
      "Epoch 3/20 | Train Loss: 0.1012, Train Acc: 0.9607 | Val Loss: 0.1181, Val EER: 0.0824, Val F1: 0.9748\n",
      "-> New best model saved with EER: 0.0824\n",
      "Epoch 4/20 | Train Loss: 0.0872, Train Acc: 0.9679 | Val Loss: 0.1073, Val EER: 0.0742, Val F1: 0.9772\n",
      "-> New best model saved with EER: 0.0742\n",
      "Epoch 5/20 | Train Loss: 0.0714, Train Acc: 0.9749 | Val Loss: 0.0888, Val EER: 0.0648, Val F1: 0.9816\n",
      "-> New best model saved with EER: 0.0648\n",
      "Epoch 6/20 | Train Loss: 0.0601, Train Acc: 0.9803 | Val Loss: 0.1081, Val EER: 0.0649, Val F1: 0.9808\n",
      "Epoch 7/20 | Train Loss: 0.0547, Train Acc: 0.9803 | Val Loss: 0.1054, Val EER: 0.0671, Val F1: 0.9801\n",
      "Epoch 8/20 | Train Loss: 0.0467, Train Acc: 0.9840 | Val Loss: 0.1080, Val EER: 0.0631, Val F1: 0.9809\n",
      "-> New best model saved with EER: 0.0631\n",
      "Epoch 9/20 | Train Loss: 0.0436, Train Acc: 0.9844 | Val Loss: 0.0946, Val EER: 0.0625, Val F1: 0.9835\n",
      "-> New best model saved with EER: 0.0625\n",
      "Epoch 10/20 | Train Loss: 0.0417, Train Acc: 0.9856 | Val Loss: 0.1379, Val EER: 0.0712, Val F1: 0.9756\n",
      "Epoch 11/20 | Train Loss: 0.0351, Train Acc: 0.9883 | Val Loss: 0.0735, Val EER: 0.0526, Val F1: 0.9865\n",
      "-> New best model saved with EER: 0.0526\n",
      "Epoch 12/20 | Train Loss: 0.0387, Train Acc: 0.9869 | Val Loss: 0.0762, Val EER: 0.0571, Val F1: 0.9856\n",
      "Epoch 13/20 | Train Loss: 0.0340, Train Acc: 0.9887 | Val Loss: 0.1911, Val EER: 0.0840, Val F1: 0.9706\n",
      "Epoch 14/20 | Train Loss: 0.0312, Train Acc: 0.9893 | Val Loss: 0.1087, Val EER: 0.0603, Val F1: 0.9821\n",
      "Epoch 15/20 | Train Loss: 0.0257, Train Acc: 0.9916 | Val Loss: 0.1353, Val EER: 0.0632, Val F1: 0.9814\n",
      "Epoch 16/20 | Train Loss: 0.0255, Train Acc: 0.9914 | Val Loss: 0.1808, Val EER: 0.0691, Val F1: 0.9744\n",
      "Epoch 17/20 | Train Loss: 0.0272, Train Acc: 0.9913 | Val Loss: 0.1027, Val EER: 0.0615, Val F1: 0.9841\n",
      "Epoch 18/20 | Train Loss: 0.0227, Train Acc: 0.9921 | Val Loss: 0.0967, Val EER: 0.0550, Val F1: 0.9859\n",
      "Epoch 19/20 | Train Loss: 0.0229, Train Acc: 0.9924 | Val Loss: 0.1445, Val EER: 0.0655, Val F1: 0.9808\n",
      "Epoch 20/20 | Train Loss: 0.0225, Train Acc: 0.9932 | Val Loss: 0.2023, Val EER: 0.0852, Val F1: 0.9739\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0526\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/3507294116.py:288: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.4989\n",
      "Test EER: 0.1180\n",
      "Test F1-Score: 0.9186\n"
     ]
    }
   ],
   "source": [
    "#CNN + MLP with Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# --- Paste the CrossAttentionModel class definition here ---\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that uses cross-attention to fuse prosodic features and CQCC features.\n",
    "    \n",
    "    1. A CNN processes the CQCC spectrogram to extract spatial features.\n",
    "    2. An MLP processes the 1D prosodic feature vector.\n",
    "    3. The processed prosodic features act as a query to attend to the CQCC feature map.\n",
    "    4. The attended CQCC features are combined with the prosody features for final classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_shape, prosody_feature_dim, embed_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_shape (tuple): The shape of the input CQCC (channels, features, frames), e.g., (1, 90, 157).\n",
    "            prosody_feature_dim (int): The number of prosodic features, e.g., 6.\n",
    "            embed_dim (int): The dimensionality of the common embedding space for attention.\n",
    "        \"\"\"\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # --- 1. CNN for CQCC Feature Extraction ---\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "        )\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embed_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        cqcc_map = self.cnn_extractor(cqcc_x)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "        batch_size, _, H, W = cqcc_map.shape\n",
    "        cqcc_seq = cqcc_map.view(batch_size, self.embed_dim, -1)\n",
    "        cqcc_seq = cqcc_seq.permute(0, 2, 1)\n",
    "        query = prosody_query.unsqueeze(1)\n",
    "        attention_scores = torch.bmm(query, cqcc_seq.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (self.embed_dim ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_context = torch.bmm(attention_weights, cqcc_seq)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "        fused_features = torch.cat([attention_context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        # Load the combined CSV file using pandas\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        # --- FIX STARTS HERE ---\n",
    "        # A robust way to extract labels and features from the dataframe\n",
    "        \n",
    "        # 1. Extract labels and convert to a NumPy array\n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        # 2. Identify all columns that are NOT features\n",
    "        metadata_cols = ['label'] # Start with the label column\n",
    "        # Add other potential metadata columns if they exist in the dataframe\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        # 3. Drop all metadata columns to get the prosodic features, then convert to a NumPy array\n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "        # --- FIX ENDS HERE ---\n",
    "\n",
    "        # The assertion check is crucial for catching data generation errors\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        # .iloc is not needed here because self.prosody_data is now a NumPy array\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files (all .npy)\n",
    "    TRAIN_CQCC_FILE = 'cqcc_features_aligned.npy'\n",
    "    TRAIN_PROSODY_FILE = 'prosody_features_aligned.npy'\n",
    "    TRAIN_LABELS_FILE = 'labels_aligned.npy'\n",
    "    \n",
    "    # Validation data files (1 .npy, 1 combined .csv)\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files (1 .npy, 1 combined .csv)\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetNPY(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_shape = (1, cqcc_sample.shape[0], cqcc_sample.shape[1])\n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    model = CrossAttentionModel(\n",
    "        cqcc_shape=cqcc_shape,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            # Use the same dataset class for test set as for validation set\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d31b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 25379 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/50 | Train Loss: 0.4109, Train Acc: 0.8736 | Val Loss: 0.3418, Val EER: 0.4134, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.4134\n",
      "Epoch 2/50 | Train Loss: 0.3500, Train Acc: 0.8979 | Val Loss: 0.2936, Val EER: 0.2117, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.2117\n",
      "Epoch 3/50 | Train Loss: 0.2784, Train Acc: 0.8969 | Val Loss: 0.2378, Val EER: 0.1468, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1468\n",
      "Epoch 4/50 | Train Loss: 0.2190, Train Acc: 0.8938 | Val Loss: 0.1803, Val EER: 0.1329, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1329\n",
      "Epoch 5/50 | Train Loss: 0.1902, Train Acc: 0.8959 | Val Loss: 0.1721, Val EER: 0.1240, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1240\n",
      "Epoch 6/50 | Train Loss: 0.1713, Train Acc: 0.9017 | Val Loss: 0.1955, Val EER: 0.1056, Val F1: 0.9584\n",
      "-> New best model saved with EER: 0.1056\n",
      "Epoch 7/50 | Train Loss: 0.1494, Train Acc: 0.9290 | Val Loss: 0.1780, Val EER: 0.0907, Val F1: 0.9523\n",
      "-> New best model saved with EER: 0.0907\n",
      "Epoch 8/50 | Train Loss: 0.1207, Train Acc: 0.9545 | Val Loss: 0.1701, Val EER: 0.0859, Val F1: 0.9687\n",
      "-> New best model saved with EER: 0.0859\n",
      "Epoch 9/50 | Train Loss: 0.1004, Train Acc: 0.9627 | Val Loss: 0.3591, Val EER: 0.0977, Val F1: 0.8733\n",
      "Epoch 10/50 | Train Loss: 0.0867, Train Acc: 0.9689 | Val Loss: 0.1622, Val EER: 0.0788, Val F1: 0.9698\n",
      "-> New best model saved with EER: 0.0788\n",
      "Epoch 11/50 | Train Loss: 0.0766, Train Acc: 0.9737 | Val Loss: 0.1130, Val EER: 0.0718, Val F1: 0.9777\n",
      "-> New best model saved with EER: 0.0718\n",
      "Epoch 12/50 | Train Loss: 0.0691, Train Acc: 0.9737 | Val Loss: 0.1377, Val EER: 0.0747, Val F1: 0.9640\n",
      "Epoch 13/50 | Train Loss: 0.0592, Train Acc: 0.9792 | Val Loss: 0.1583, Val EER: 0.0597, Val F1: 0.9723\n",
      "-> New best model saved with EER: 0.0597\n",
      "Epoch 14/50 | Train Loss: 0.0561, Train Acc: 0.9815 | Val Loss: 0.1390, Val EER: 0.0675, Val F1: 0.9661\n",
      "Epoch 15/50 | Train Loss: 0.0505, Train Acc: 0.9822 | Val Loss: 0.2877, Val EER: 0.0743, Val F1: 0.9260\n",
      "Epoch 16/50 | Train Loss: 0.0437, Train Acc: 0.9856 | Val Loss: 0.0944, Val EER: 0.0557, Val F1: 0.9827\n",
      "-> New best model saved with EER: 0.0557\n",
      "Epoch 17/50 | Train Loss: 0.0399, Train Acc: 0.9863 | Val Loss: 0.1400, Val EER: 0.0553, Val F1: 0.9777\n",
      "-> New best model saved with EER: 0.0553\n",
      "Epoch 18/50 | Train Loss: 0.0356, Train Acc: 0.9882 | Val Loss: 0.0808, Val EER: 0.0526, Val F1: 0.9837\n",
      "-> New best model saved with EER: 0.0526\n",
      "Epoch 19/50 | Train Loss: 0.0338, Train Acc: 0.9892 | Val Loss: 0.1593, Val EER: 0.0491, Val F1: 0.9749\n",
      "-> New best model saved with EER: 0.0491\n",
      "Epoch 20/50 | Train Loss: 0.0291, Train Acc: 0.9907 | Val Loss: 0.2599, Val EER: 0.0546, Val F1: 0.9675\n",
      "Epoch 21/50 | Train Loss: 0.0279, Train Acc: 0.9917 | Val Loss: 0.0748, Val EER: 0.0448, Val F1: 0.9866\n",
      "-> New best model saved with EER: 0.0448\n",
      "Epoch 22/50 | Train Loss: 0.0264, Train Acc: 0.9916 | Val Loss: 0.1320, Val EER: 0.0476, Val F1: 0.9794\n",
      "Epoch 23/50 | Train Loss: 0.0220, Train Acc: 0.9928 | Val Loss: 0.2004, Val EER: 0.0459, Val F1: 0.9727\n",
      "Epoch 24/50 | Train Loss: 0.0215, Train Acc: 0.9933 | Val Loss: 0.2004, Val EER: 0.0538, Val F1: 0.9731\n",
      "Epoch 25/50 | Train Loss: 0.0203, Train Acc: 0.9937 | Val Loss: 0.2477, Val EER: 0.0495, Val F1: 0.9685\n",
      "Epoch 26/50 | Train Loss: 0.0179, Train Acc: 0.9941 | Val Loss: 0.0719, Val EER: 0.0475, Val F1: 0.9857\n",
      "Epoch 27/50 | Train Loss: 0.0189, Train Acc: 0.9942 | Val Loss: 0.1668, Val EER: 0.0467, Val F1: 0.9767\n",
      "Epoch 28/50 | Train Loss: 0.0168, Train Acc: 0.9951 | Val Loss: 0.1126, Val EER: 0.0467, Val F1: 0.9830\n",
      "Epoch 29/50 | Train Loss: 0.0148, Train Acc: 0.9954 | Val Loss: 0.1229, Val EER: 0.0420, Val F1: 0.9826\n",
      "-> New best model saved with EER: 0.0420\n",
      "Epoch 30/50 | Train Loss: 0.0131, Train Acc: 0.9957 | Val Loss: 0.1140, Val EER: 0.0396, Val F1: 0.9843\n",
      "-> New best model saved with EER: 0.0396\n",
      "Epoch 31/50 | Train Loss: 0.0137, Train Acc: 0.9959 | Val Loss: 0.7669, Val EER: 0.0938, Val F1: 0.9511\n",
      "Epoch 32/50 | Train Loss: 0.0127, Train Acc: 0.9961 | Val Loss: 0.1215, Val EER: 0.0412, Val F1: 0.9837\n",
      "Epoch 33/50 | Train Loss: 0.0110, Train Acc: 0.9965 | Val Loss: 0.0726, Val EER: 0.0391, Val F1: 0.9892\n",
      "-> New best model saved with EER: 0.0391\n",
      "Epoch 34/50 | Train Loss: 0.0103, Train Acc: 0.9967 | Val Loss: 0.1253, Val EER: 0.0440, Val F1: 0.9849\n",
      "Epoch 35/50 | Train Loss: 0.0129, Train Acc: 0.9959 | Val Loss: 0.2031, Val EER: 0.0424, Val F1: 0.9763\n",
      "Epoch 36/50 | Train Loss: 0.0079, Train Acc: 0.9978 | Val Loss: 0.2849, Val EER: 0.0592, Val F1: 0.9713\n",
      "Epoch 37/50 | Train Loss: 0.0079, Train Acc: 0.9975 | Val Loss: 0.0839, Val EER: 0.0451, Val F1: 0.9867\n",
      "Epoch 38/50 | Train Loss: 0.0104, Train Acc: 0.9968 | Val Loss: 0.3746, Val EER: 0.0563, Val F1: 0.9663\n",
      "Epoch 39/50 | Train Loss: 0.0100, Train Acc: 0.9971 | Val Loss: 0.3508, Val EER: 0.0404, Val F1: 0.9663\n",
      "Epoch 40/50 | Train Loss: 0.0094, Train Acc: 0.9973 | Val Loss: 0.1000, Val EER: 0.0357, Val F1: 0.9866\n",
      "-> New best model saved with EER: 0.0357\n",
      "Epoch 41/50 | Train Loss: 0.0075, Train Acc: 0.9977 | Val Loss: 0.0811, Val EER: 0.0345, Val F1: 0.9890\n",
      "-> New best model saved with EER: 0.0345\n",
      "Epoch 42/50 | Train Loss: 0.0086, Train Acc: 0.9971 | Val Loss: 0.0636, Val EER: 0.0349, Val F1: 0.9904\n",
      "Epoch 43/50 | Train Loss: 0.0072, Train Acc: 0.9980 | Val Loss: 0.2720, Val EER: 0.0426, Val F1: 0.9718\n",
      "Epoch 44/50 | Train Loss: 0.0060, Train Acc: 0.9982 | Val Loss: 0.0912, Val EER: 0.0361, Val F1: 0.9881\n",
      "Epoch 45/50 | Train Loss: 0.0052, Train Acc: 0.9985 | Val Loss: 0.0647, Val EER: 0.0377, Val F1: 0.9901\n",
      "Epoch 46/50 | Train Loss: 0.0077, Train Acc: 0.9976 | Val Loss: 0.4424, Val EER: 0.0612, Val F1: 0.9651\n",
      "Epoch 47/50 | Train Loss: 0.0064, Train Acc: 0.9982 | Val Loss: 0.0617, Val EER: 0.0312, Val F1: 0.9912\n",
      "-> New best model saved with EER: 0.0312\n",
      "Epoch 48/50 | Train Loss: 0.0068, Train Acc: 0.9978 | Val Loss: 0.2347, Val EER: 0.0636, Val F1: 0.9771\n",
      "Epoch 49/50 | Train Loss: 0.0060, Train Acc: 0.9980 | Val Loss: 0.0816, Val EER: 0.0330, Val F1: 0.9893\n",
      "Epoch 50/50 | Train Loss: 0.0052, Train Acc: 0.9983 | Val Loss: 0.0851, Val EER: 0.0424, Val F1: 0.9894\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0312\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/4258021584.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.7309\n",
      "Test EER: 0.1127\n",
      "Test F1-Score: 0.9157\n"
     ]
    }
   ],
   "source": [
    "#CNN + MLP (No Attention No Augmentation)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# --- CNN + MLP Fusion Model (No Attention) ---\n",
    "class CNNMLPFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a CNN and an MLP, without attention.\n",
    "    \n",
    "    1. A CNN processes the CQCC spectrogram.\n",
    "    2. The output of the CNN is flattened to a 1D vector.\n",
    "    3. An MLP processes the 1D prosodic feature vector.\n",
    "    4. The two resulting vectors are concatenated.\n",
    "    5. A final classifier head makes the prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_shape, prosody_feature_dim, cnn_embed_dim=128, prosody_embed_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_shape (tuple): The shape of the input CQCC (channels, features, frames).\n",
    "            prosody_feature_dim (int): The number of prosodic features.\n",
    "            cnn_embed_dim (int): The output channels of the last CNN layer.\n",
    "            prosody_embed_dim (int): The output dimension of the prosody MLP.\n",
    "        \"\"\"\n",
    "        super(CNNMLPFusionModel, self).__init__()\n",
    "        \n",
    "        # --- 1. CNN for CQCC Feature Extraction ---\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=cnn_embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(cnn_embed_dim),\n",
    "        )\n",
    "        \n",
    "        # Use Adaptive Pooling to handle variable input sizes and create a fixed-size output\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, prosody_embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input dimension is the sum of the flattened CNN output and the prosody MLP output\n",
    "        classifier_input_dim = cnn_embed_dim + prosody_embed_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # Add a channel dimension for the CNN\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        \n",
    "        # Process CQCC features\n",
    "        cqcc_out = self.cnn_extractor(cqcc_x)\n",
    "        cqcc_pooled = self.adaptive_pool(cqcc_out)\n",
    "        cqcc_flat = self.flatten(cqcc_pooled)\n",
    "        \n",
    "        # Process prosodic features\n",
    "        prosody_out = self.prosody_mlp(prosody_x)\n",
    "        \n",
    "        # Concatenate the feature vectors\n",
    "        fused_features = torch.cat([cqcc_flat, prosody_out], dim=1)\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files (all .npy)\n",
    "    TRAIN_CQCC_FILE = 'cqcc_features_aligned.npy'\n",
    "    TRAIN_PROSODY_FILE = 'prosody_features_aligned.npy'\n",
    "    TRAIN_LABELS_FILE = 'labels_aligned.npy'\n",
    "    \n",
    "    # Validation data files (1 .npy, 1 combined .csv)\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files (1 .npy, 1 combined .csv)\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetNPY(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_shape = (1, cqcc_sample.shape[0], cqcc_sample.shape[1])\n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    model = CNNMLPFusionModel(\n",
    "        cqcc_shape=cqcc_shape,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a773b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 25379 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/20 | Train Loss: 0.2983, Train Acc: 0.8854 | Val Loss: 0.1771, Val EER: 0.1328, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1328\n",
      "Epoch 2/20 | Train Loss: 0.1622, Train Acc: 0.9120 | Val Loss: 0.1385, Val EER: 0.0858, Val F1: 0.9641\n",
      "-> New best model saved with EER: 0.0858\n",
      "Epoch 3/20 | Train Loss: 0.0913, Train Acc: 0.9655 | Val Loss: 0.1122, Val EER: 0.0714, Val F1: 0.9799\n",
      "-> New best model saved with EER: 0.0714\n",
      "Epoch 4/20 | Train Loss: 0.0482, Train Acc: 0.9841 | Val Loss: 0.1796, Val EER: 0.0730, Val F1: 0.9821\n",
      "Epoch 5/20 | Train Loss: 0.0342, Train Acc: 0.9901 | Val Loss: 0.1498, Val EER: 0.0521, Val F1: 0.9817\n",
      "-> New best model saved with EER: 0.0521\n",
      "Epoch 6/20 | Train Loss: 0.0293, Train Acc: 0.9912 | Val Loss: 0.1175, Val EER: 0.0600, Val F1: 0.9860\n",
      "Epoch 7/20 | Train Loss: 0.0191, Train Acc: 0.9948 | Val Loss: 0.1849, Val EER: 0.0608, Val F1: 0.9784\n",
      "Epoch 8/20 | Train Loss: 0.0173, Train Acc: 0.9950 | Val Loss: 0.1603, Val EER: 0.0599, Val F1: 0.9848\n",
      "Epoch 9/20 | Train Loss: 0.0133, Train Acc: 0.9960 | Val Loss: 0.1095, Val EER: 0.0512, Val F1: 0.9876\n",
      "-> New best model saved with EER: 0.0512\n",
      "Epoch 10/20 | Train Loss: 0.0107, Train Acc: 0.9973 | Val Loss: 0.0984, Val EER: 0.0522, Val F1: 0.9886\n",
      "Epoch 11/20 | Train Loss: 0.0113, Train Acc: 0.9968 | Val Loss: 0.0827, Val EER: 0.0412, Val F1: 0.9899\n",
      "-> New best model saved with EER: 0.0412\n",
      "Epoch 12/20 | Train Loss: 0.0100, Train Acc: 0.9972 | Val Loss: 0.2305, Val EER: 0.0628, Val F1: 0.9804\n",
      "Epoch 13/20 | Train Loss: 0.0114, Train Acc: 0.9967 | Val Loss: 0.0765, Val EER: 0.0443, Val F1: 0.9899\n",
      "Epoch 14/20 | Train Loss: 0.0088, Train Acc: 0.9972 | Val Loss: 0.1225, Val EER: 0.0530, Val F1: 0.9890\n",
      "Epoch 15/20 | Train Loss: 0.0071, Train Acc: 0.9978 | Val Loss: 0.1105, Val EER: 0.0534, Val F1: 0.9888\n",
      "Epoch 16/20 | Train Loss: 0.0057, Train Acc: 0.9982 | Val Loss: 0.1853, Val EER: 0.0616, Val F1: 0.9871\n",
      "Epoch 17/20 | Train Loss: 0.0081, Train Acc: 0.9978 | Val Loss: 0.2299, Val EER: 0.0773, Val F1: 0.9659\n",
      "Epoch 18/20 | Train Loss: 0.0046, Train Acc: 0.9987 | Val Loss: 0.1464, Val EER: 0.0593, Val F1: 0.9843\n",
      "Epoch 19/20 | Train Loss: 0.0075, Train Acc: 0.9977 | Val Loss: 0.1106, Val EER: 0.0514, Val F1: 0.9895\n",
      "Epoch 20/20 | Train Loss: 0.0046, Train Acc: 0.9985 | Val Loss: 0.1393, Val EER: 0.0542, Val F1: 0.9861\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0412\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/2589069090.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.7963\n",
      "Test EER: 0.1177\n",
      "Test F1-Score: 0.9049\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# --- LSTM + MLP Fusion Model (No Attention) ---\n",
    "class LSTMMMLPFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using an LSTM and an MLP.\n",
    "    \n",
    "    1. An LSTM processes the CQCC spectrogram as a sequence of feature vectors.\n",
    "    2. An MLP processes the 1D prosodic feature vector.\n",
    "    3. The final hidden state of the LSTM and the output of the MLP are concatenated.\n",
    "    4. A final classifier head makes the prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, lstm_hidden_dim=128, lstm_layers=2, prosody_embed_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_feature_dim (int): The number of CQCC coefficients (input feature size for LSTM).\n",
    "            prosody_feature_dim (int): The number of prosodic features.\n",
    "            lstm_hidden_dim (int): The hidden dimension size of the LSTM.\n",
    "            lstm_layers (int): The number of LSTM layers.\n",
    "            prosody_embed_dim (int): The output dimension of the prosody MLP.\n",
    "        \"\"\"\n",
    "        super(LSTMMMLPFusionModel, self).__init__()\n",
    "        \n",
    "        # --- 1. LSTM for CQCC Feature Extraction ---\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cqcc_feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,  # Crucial for (batch, seq, feature) input format\n",
    "            bidirectional=True, # Use a bidirectional LSTM to capture context from both directions\n",
    "            dropout=0.2 if lstm_layers > 1 else 0 # Add dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, prosody_embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input dimension is the sum of the LSTM output and the prosody MLP output\n",
    "        # LSTM hidden_dim * 2 because it's bidirectional\n",
    "        classifier_input_dim = (lstm_hidden_dim * 2) + prosody_embed_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames)\n",
    "        # Permute for LSTM: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # Process CQCC features through LSTM\n",
    "        # We only need the final hidden state, not the full output sequence\n",
    "        # The `_` holds the output of all time steps\n",
    "        _, (h_n, _) = self.lstm(cqcc_x)\n",
    "        \n",
    "        # h_n shape is (num_layers * num_directions, batch, hidden_dim)\n",
    "        # We concatenate the final hidden states of the forward and backward LSTMs\n",
    "        # The last forward layer is at index -2, the last backward layer is at index -1\n",
    "        lstm_out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        \n",
    "        # Process prosodic features\n",
    "        prosody_out = self.prosody_mlp(prosody_x)\n",
    "        \n",
    "        # Concatenate the feature vectors\n",
    "        fused_features = torch.cat([lstm_out, prosody_out], dim=1)\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files (all .npy)\n",
    "    TRAIN_CQCC_FILE = 'cqcc_features_aligned.npy'\n",
    "    TRAIN_PROSODY_FILE = 'prosody_features_aligned.npy'\n",
    "    TRAIN_LABELS_FILE = 'labels_aligned.npy'\n",
    "    \n",
    "    # Validation data files (1 .npy, 1 combined .csv)\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files (1 .npy, 1 combined .csv)\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetNPY(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    # For LSTM, the input feature dimension is the number of CQCC coefficients\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    model = LSTMMMLPFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "515faa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 25379 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/20 | Train Loss: 0.3634, Train Acc: 0.8882 | Val Loss: 0.2445, Val EER: 0.1932, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1932\n",
      "Epoch 2/20 | Train Loss: 0.2145, Train Acc: 0.9044 | Val Loss: 0.1677, Val EER: 0.1122, Val F1: 0.9616\n",
      "-> New best model saved with EER: 0.1122\n",
      "Epoch 3/20 | Train Loss: 0.1276, Train Acc: 0.9487 | Val Loss: 0.1380, Val EER: 0.0746, Val F1: 0.9722\n",
      "-> New best model saved with EER: 0.0746\n",
      "Epoch 4/20 | Train Loss: 0.0890, Train Acc: 0.9692 | Val Loss: 0.1241, Val EER: 0.0675, Val F1: 0.9793\n",
      "-> New best model saved with EER: 0.0675\n",
      "Epoch 5/20 | Train Loss: 0.0615, Train Acc: 0.9808 | Val Loss: 0.0983, Val EER: 0.0631, Val F1: 0.9783\n",
      "-> New best model saved with EER: 0.0631\n",
      "Epoch 6/20 | Train Loss: 0.0468, Train Acc: 0.9851 | Val Loss: 0.1315, Val EER: 0.0557, Val F1: 0.9827\n",
      "-> New best model saved with EER: 0.0557\n",
      "Epoch 7/20 | Train Loss: 0.0371, Train Acc: 0.9890 | Val Loss: 0.1407, Val EER: 0.0635, Val F1: 0.9835\n",
      "Epoch 8/20 | Train Loss: 0.0232, Train Acc: 0.9931 | Val Loss: 0.1015, Val EER: 0.0453, Val F1: 0.9871\n",
      "-> New best model saved with EER: 0.0453\n",
      "Epoch 9/20 | Train Loss: 0.0183, Train Acc: 0.9945 | Val Loss: 0.0894, Val EER: 0.0407, Val F1: 0.9878\n",
      "-> New best model saved with EER: 0.0407\n",
      "Epoch 10/20 | Train Loss: 0.0144, Train Acc: 0.9957 | Val Loss: 0.1489, Val EER: 0.0495, Val F1: 0.9855\n",
      "Epoch 11/20 | Train Loss: 0.0160, Train Acc: 0.9958 | Val Loss: 0.0923, Val EER: 0.0400, Val F1: 0.9885\n",
      "-> New best model saved with EER: 0.0400\n",
      "Epoch 12/20 | Train Loss: 0.0115, Train Acc: 0.9965 | Val Loss: 0.2680, Val EER: 0.0522, Val F1: 0.9763\n",
      "Epoch 13/20 | Train Loss: 0.0171, Train Acc: 0.9957 | Val Loss: 0.1019, Val EER: 0.0388, Val F1: 0.9879\n",
      "-> New best model saved with EER: 0.0388\n",
      "Epoch 14/20 | Train Loss: 0.0087, Train Acc: 0.9976 | Val Loss: 0.0895, Val EER: 0.0369, Val F1: 0.9890\n",
      "-> New best model saved with EER: 0.0369\n",
      "Epoch 15/20 | Train Loss: 0.0074, Train Acc: 0.9981 | Val Loss: 0.1067, Val EER: 0.0569, Val F1: 0.9874\n",
      "Epoch 16/20 | Train Loss: 0.0094, Train Acc: 0.9973 | Val Loss: 0.1185, Val EER: 0.0415, Val F1: 0.9869\n",
      "Epoch 17/20 | Train Loss: 0.0100, Train Acc: 0.9975 | Val Loss: 0.1635, Val EER: 0.0483, Val F1: 0.9828\n",
      "Epoch 18/20 | Train Loss: 0.0070, Train Acc: 0.9980 | Val Loss: 0.1364, Val EER: 0.0443, Val F1: 0.9862\n",
      "Epoch 19/20 | Train Loss: 0.0104, Train Acc: 0.9975 | Val Loss: 0.0791, Val EER: 0.0437, Val F1: 0.9899\n",
      "Epoch 20/20 | Train Loss: 0.0027, Train Acc: 0.9994 | Val Loss: 0.1824, Val EER: 0.0454, Val F1: 0.9863\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0369\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/3206541370.py:357: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.5919\n",
      "Test EER: 0.1050\n",
      "Test F1-Score: 0.9240\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# --- LSTM + Cross-Attention Fusion Model ---\n",
    "class LSTMAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using an LSTM and cross-attention.\n",
    "    \n",
    "    1. An LSTM processes the CQCC spectrogram as a sequence of feature vectors.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the LSTM's output sequence.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, lstm_hidden_dim=128, lstm_layers=2, prosody_embed_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_feature_dim (int): The number of CQCC coefficients.\n",
    "            prosody_feature_dim (int): The number of prosodic features.\n",
    "            lstm_hidden_dim (int): The hidden dimension size of the LSTM.\n",
    "            lstm_layers (int): The number of LSTM layers.\n",
    "            prosody_embed_dim (int): The output dimension of the prosody MLP.\n",
    "        \"\"\"\n",
    "        super(LSTMAttentionFusionModel, self).__init__()\n",
    "        \n",
    "        # The LSTM's output dimension will be hidden_dim * 2 because it's bidirectional\n",
    "        self.lstm_output_dim = lstm_hidden_dim * 2\n",
    "        \n",
    "        # --- 1. LSTM for CQCC Feature Extraction ---\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cqcc_feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        # This MLP will create the query for the attention mechanism\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, prosody_embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Attention Mechanism Layers ---\n",
    "        # We need linear layers to project the query and keys to a common dimension if they differ.\n",
    "        # Here, we'll design it so the prosody_embed_dim and lstm_output_dim are compatible.\n",
    "        # For simplicity, we'll use a linear layer to make the LSTM output match the prosody query dimension.\n",
    "        self.key_projection = nn.Linear(self.lstm_output_dim, prosody_embed_dim)\n",
    "\n",
    "        # --- 4. Classifier Head ---\n",
    "        # The input is the attention context vector + the original prosody query vector\n",
    "        classifier_input_dim = prosody_embed_dim + prosody_embed_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for LSTM: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC through LSTM to get the full sequence output\n",
    "        # lstm_out shape: (batch, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(cqcc_x)\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        # prosody_query shape: (batch, prosody_embed_dim)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        # Project LSTM output to create keys and values for attention\n",
    "        # keys shape: (batch, seq_len, prosody_embed_dim)\n",
    "        keys = self.key_projection(lstm_out)\n",
    "        # values are the original lstm output\n",
    "        values = lstm_out\n",
    "        \n",
    "        # Reshape query for batch matrix multiplication\n",
    "        # query shape: (batch, 1, prosody_embed_dim)\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        # (batch, 1, embed_dim) * (batch, embed_dim, seq_len) -> (batch, 1, seq_len)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5) # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector: Weights * V\n",
    "        # (batch, 1, seq_len) * (batch, seq_len, lstm_output_dim) -> (batch, 1, lstm_output_dim)\n",
    "        context = torch.bmm(attention_weights, values)\n",
    "        # Squeeze to remove the sequence dimension: (batch, lstm_output_dim)\n",
    "        context = context.squeeze(1)\n",
    "\n",
    "        # Project the context to match the prosody embedding dimension for concatenation\n",
    "        # This step was missing in the previous attention model, let's make it more robust.\n",
    "        # We'll use a linear layer for this. Let's define it in __init__\n",
    "        # For simplicity in this version, we will assume the context vector can be fused directly\n",
    "        # with a projected version of itself. Let's refine the classifier input.\n",
    "        \n",
    "        # Let's redefine the classifier input to be simpler and more direct:\n",
    "        # We will concatenate the context vector with the original prosody query.\n",
    "        # The context vector's size is self.lstm_output_dim.\n",
    "        # The prosody query's size is prosody_embed_dim.\n",
    "        \n",
    "        # Re-calculating classifier input dimension based on what we have:\n",
    "        # classifier_input_dim = self.lstm_output_dim + prosody_embed_dim\n",
    "        # Let's adjust the classifier in __init__ for this.\n",
    "        \n",
    "        # For this implementation, let's stick to the original plan and adjust the classifier input dynamically\n",
    "        # Let's create a new classifier head here for clarity, or better, fix the __init__ part.\n",
    "        \n",
    "        # Let's fix the architecture to be more robust from the start.\n",
    "        # The context vector has size `lstm_output_dim`. The query has size `prosody_embed_dim`.\n",
    "        # We will concatenate these two.\n",
    "        \n",
    "        # Let's adjust the classifier input dimension in __init__\n",
    "        # classifier_input_dim = self.lstm_output_dim + prosody_embed_dim\n",
    "        # And the classifier head needs to be updated accordingly.\n",
    "\n",
    "        # Let's assume the model is re-initialized with the correct dimensions.\n",
    "        # The logic below is correct if the classifier is initialized with:\n",
    "        # classifier_input_dim = (lstm_hidden_dim * 2) + prosody_embed_dim\n",
    "        \n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files (all .npy)\n",
    "    TRAIN_CQCC_FILE = 'cqcc_features_aligned.npy'\n",
    "    TRAIN_PROSODY_FILE = 'prosody_features_aligned.npy'\n",
    "    TRAIN_LABELS_FILE = 'labels_aligned.npy'\n",
    "    \n",
    "    # Validation data files (1 .npy, 1 combined .csv)\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files (1 .npy, 1 combined .csv)\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetNPY(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    # For LSTM, the input feature dimension is the number of CQCC coefficients\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    # Use the new Attention model\n",
    "    model = LSTMAttentionFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Let's fix the classifier input dimension inside the model's __init__\n",
    "    # This was a point of confusion before. Let's make it explicit.\n",
    "    lstm_hidden_dim = 128 # Must match the default in the model\n",
    "    prosody_embed_dim = 64 # Must match the default in the model\n",
    "    classifier_input_dim = (lstm_hidden_dim * 2) + prosody_embed_dim\n",
    "    \n",
    "    # Re-initializing the classifier head with the correct input dimension\n",
    "    model.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ac7416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 25379 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/20 | Train Loss: 0.3807, Train Acc: 0.8912 | Val Loss: 0.3380, Val EER: 0.4075, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.4075\n",
      "Epoch 2/20 | Train Loss: 0.3514, Train Acc: 0.8980 | Val Loss: 0.3463, Val EER: 0.3831, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.3831\n",
      "Epoch 3/20 | Train Loss: 0.3364, Train Acc: 0.8983 | Val Loss: 0.3437, Val EER: 0.3619, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.3619\n",
      "Epoch 4/20 | Train Loss: 0.3290, Train Acc: 0.8983 | Val Loss: 0.3175, Val EER: 0.3787, Val F1: 0.9459\n",
      "Epoch 5/20 | Train Loss: 0.3220, Train Acc: 0.8983 | Val Loss: 0.3173, Val EER: 0.3367, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.3367\n",
      "Epoch 6/20 | Train Loss: 0.3053, Train Acc: 0.8985 | Val Loss: 0.2688, Val EER: 0.2539, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.2539\n",
      "Epoch 7/20 | Train Loss: 0.2664, Train Acc: 0.8982 | Val Loss: 0.2551, Val EER: 0.1860, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1860\n",
      "Epoch 8/20 | Train Loss: 0.2203, Train Acc: 0.8980 | Val Loss: 0.1944, Val EER: 0.1538, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1538\n",
      "Epoch 9/20 | Train Loss: 0.2063, Train Acc: 0.8986 | Val Loss: 0.2038, Val EER: 0.1509, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1509\n",
      "Epoch 10/20 | Train Loss: 0.1988, Train Acc: 0.8986 | Val Loss: 0.1913, Val EER: 0.1319, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1319\n",
      "Epoch 11/20 | Train Loss: 0.1921, Train Acc: 0.8992 | Val Loss: 0.2028, Val EER: 0.1293, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1293\n",
      "Epoch 12/20 | Train Loss: 0.1845, Train Acc: 0.9036 | Val Loss: 0.2594, Val EER: 0.1519, Val F1: 0.8912\n",
      "Epoch 13/20 | Train Loss: 0.1646, Train Acc: 0.9208 | Val Loss: 0.1500, Val EER: 0.0977, Val F1: 0.9635\n",
      "-> New best model saved with EER: 0.0977\n",
      "Epoch 14/20 | Train Loss: 0.1555, Train Acc: 0.9299 | Val Loss: 0.3082, Val EER: 0.1231, Val F1: 0.8851\n",
      "Epoch 15/20 | Train Loss: 0.2072, Train Acc: 0.9238 | Val Loss: 0.3057, Val EER: 0.3321, Val F1: 0.9476\n",
      "Epoch 16/20 | Train Loss: 0.2906, Train Acc: 0.8994 | Val Loss: 0.3065, Val EER: 0.3336, Val F1: 0.9464\n",
      "Epoch 17/20 | Train Loss: 0.2428, Train Acc: 0.9073 | Val Loss: 0.1784, Val EER: 0.1221, Val F1: 0.9531\n",
      "Epoch 18/20 | Train Loss: 0.1796, Train Acc: 0.9258 | Val Loss: 0.1604, Val EER: 0.1002, Val F1: 0.9609\n",
      "Epoch 19/20 | Train Loss: 0.1530, Train Acc: 0.9360 | Val Loss: 0.1482, Val EER: 0.1019, Val F1: 0.9632\n",
      "Epoch 20/20 | Train Loss: 0.1381, Train Acc: 0.9443 | Val Loss: 0.2452, Val EER: 0.1013, Val F1: 0.9286\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0977\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/3084071013.py:284: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.2923\n",
      "Test EER: 0.1903\n",
      "Test F1-Score: 0.9011\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Transformer + MLP Fusion Model (No Attention) ---\n",
    "class TransformerFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a Transformer and an MLP without attention.\n",
    "    \n",
    "    1. A Transformer Encoder processes the CQCC spectrogram as a sequence.\n",
    "    2. The output of the Transformer is aggregated (here, by taking the first time step's output).\n",
    "    3. An MLP processes the 1D prosodic feature vector.\n",
    "    4. The two resulting vectors are concatenated and passed to a classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, nhead=8, num_encoder_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- 1. CQCC Feature Processing Path ---\n",
    "        self.cqcc_projection = nn.Linear(cqcc_feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the aggregated transformer output (size d_model) + the prosody MLP output (size d_model)\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for Transformer: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC features\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x) * math.sqrt(self.d_model)\n",
    "        cqcc_pos = self.pos_encoder(cqcc_embed.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        transformer_out = self.transformer_encoder(cqcc_pos)\n",
    "        \n",
    "        # Aggregate the transformer output. We'll take the output of the first time step.\n",
    "        transformer_aggregated = transformer_out[:, 0, :]\n",
    "        \n",
    "        # 2. Process prosodic features\n",
    "        prosody_out = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # 3. Fusion and Classification\n",
    "        fused_features = torch.cat([transformer_aggregated, prosody_out], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files (all .npy)\n",
    "    TRAIN_CQCC_FILE = 'cqcc_features_aligned.npy'\n",
    "    TRAIN_PROSODY_FILE = 'prosody_features_aligned.npy'\n",
    "    TRAIN_LABELS_FILE = 'labels_aligned.npy'\n",
    "    \n",
    "    # Validation data files (1 .npy, 1 combined .csv)\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files (1 .npy, 1 combined .csv)\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetNPY(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    # Use the new Transformer Fusion model (no attention)\n",
    "    model = TransformerFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "380a341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 46019 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 0.6667, Train Acc: 0.5971 | Val Loss: 0.4529, Val EER: 0.3771, Val F1: 0.9410\n",
      "-> New best model saved with EER: 0.3771\n",
      "Epoch 2/40 | Train Loss: 0.4516, Train Acc: 0.7836 | Val Loss: 0.3912, Val EER: 0.3795, Val F1: 0.9458\n",
      "Epoch 3/40 | Train Loss: 0.3993, Train Acc: 0.8165 | Val Loss: 0.3424, Val EER: 0.3699, Val F1: 0.9460\n",
      "-> New best model saved with EER: 0.3699\n",
      "Epoch 4/40 | Train Loss: 0.3499, Train Acc: 0.8434 | Val Loss: 0.2746, Val EER: 0.2561, Val F1: 0.9463\n",
      "-> New best model saved with EER: 0.2561\n",
      "Epoch 5/40 | Train Loss: 0.3102, Train Acc: 0.8606 | Val Loss: 0.2366, Val EER: 0.1966, Val F1: 0.9497\n",
      "-> New best model saved with EER: 0.1966\n",
      "Epoch 6/40 | Train Loss: 0.2729, Train Acc: 0.8784 | Val Loss: 0.2289, Val EER: 0.2009, Val F1: 0.9531\n",
      "Epoch 7/40 | Train Loss: 0.2462, Train Acc: 0.8944 | Val Loss: 0.2610, Val EER: 0.2414, Val F1: 0.9495\n",
      "Epoch 8/40 | Train Loss: 0.2203, Train Acc: 0.9044 | Val Loss: 0.1926, Val EER: 0.1515, Val F1: 0.9556\n",
      "-> New best model saved with EER: 0.1515\n",
      "Epoch 9/40 | Train Loss: 0.2070, Train Acc: 0.9103 | Val Loss: 0.2371, Val EER: 0.1770, Val F1: 0.9548\n",
      "Epoch 10/40 | Train Loss: 0.2097, Train Acc: 0.9080 | Val Loss: 0.1829, Val EER: 0.1409, Val F1: 0.9606\n",
      "-> New best model saved with EER: 0.1409\n",
      "Epoch 11/40 | Train Loss: 0.2643, Train Acc: 0.8840 | Val Loss: 0.2211, Val EER: 0.1447, Val F1: 0.9455\n",
      "Epoch 12/40 | Train Loss: 0.2261, Train Acc: 0.9016 | Val Loss: 0.2498, Val EER: 0.1983, Val F1: 0.9535\n",
      "Epoch 13/40 | Train Loss: 0.1855, Train Acc: 0.9209 | Val Loss: 0.2649, Val EER: 0.1887, Val F1: 0.9511\n",
      "Epoch 14/40 | Train Loss: 0.1782, Train Acc: 0.9229 | Val Loss: 0.2090, Val EER: 0.1616, Val F1: 0.9572\n",
      "Epoch 15/40 | Train Loss: 0.1682, Train Acc: 0.9262 | Val Loss: 0.1984, Val EER: 0.1529, Val F1: 0.9599\n",
      "Epoch 16/40 | Train Loss: 0.1524, Train Acc: 0.9342 | Val Loss: 0.1823, Val EER: 0.1165, Val F1: 0.9563\n",
      "-> New best model saved with EER: 0.1165\n",
      "Epoch 17/40 | Train Loss: 0.1458, Train Acc: 0.9372 | Val Loss: 0.1839, Val EER: 0.1201, Val F1: 0.9595\n",
      "Epoch 18/40 | Train Loss: 0.1464, Train Acc: 0.9378 | Val Loss: 0.2015, Val EER: 0.1081, Val F1: 0.9493\n",
      "-> New best model saved with EER: 0.1081\n",
      "Epoch 19/40 | Train Loss: 0.1318, Train Acc: 0.9445 | Val Loss: 0.2862, Val EER: 0.1644, Val F1: 0.9394\n",
      "Epoch 20/40 | Train Loss: 0.1255, Train Acc: 0.9467 | Val Loss: 0.1431, Val EER: 0.0950, Val F1: 0.9670\n",
      "-> New best model saved with EER: 0.0950\n",
      "Epoch 21/40 | Train Loss: 0.1118, Train Acc: 0.9537 | Val Loss: 0.2086, Val EER: 0.0895, Val F1: 0.9476\n",
      "-> New best model saved with EER: 0.0895\n",
      "Epoch 22/40 | Train Loss: 0.1122, Train Acc: 0.9538 | Val Loss: 0.1846, Val EER: 0.0865, Val F1: 0.9541\n",
      "-> New best model saved with EER: 0.0865\n",
      "Epoch 23/40 | Train Loss: 0.1006, Train Acc: 0.9585 | Val Loss: 0.2443, Val EER: 0.0937, Val F1: 0.9403\n",
      "Epoch 24/40 | Train Loss: 0.0986, Train Acc: 0.9604 | Val Loss: 0.1345, Val EER: 0.0792, Val F1: 0.9689\n",
      "-> New best model saved with EER: 0.0792\n",
      "Epoch 25/40 | Train Loss: 0.0944, Train Acc: 0.9621 | Val Loss: 0.1556, Val EER: 0.0836, Val F1: 0.9616\n",
      "Epoch 26/40 | Train Loss: 0.0932, Train Acc: 0.9636 | Val Loss: 0.1236, Val EER: 0.0801, Val F1: 0.9731\n",
      "Epoch 27/40 | Train Loss: 0.0874, Train Acc: 0.9663 | Val Loss: 0.1184, Val EER: 0.0781, Val F1: 0.9718\n",
      "-> New best model saved with EER: 0.0781\n",
      "Epoch 28/40 | Train Loss: 0.0778, Train Acc: 0.9688 | Val Loss: 0.0879, Val EER: 0.0608, Val F1: 0.9814\n",
      "-> New best model saved with EER: 0.0608\n",
      "Epoch 29/40 | Train Loss: 0.0726, Train Acc: 0.9722 | Val Loss: 0.1129, Val EER: 0.0700, Val F1: 0.9743\n",
      "Epoch 30/40 | Train Loss: 0.1292, Train Acc: 0.9501 | Val Loss: 2.1012, Val EER: 0.2732, Val F1: 0.5398\n",
      "Epoch 31/40 | Train Loss: 0.1464, Train Acc: 0.9383 | Val Loss: 0.3957, Val EER: 0.1099, Val F1: 0.9248\n",
      "Epoch 32/40 | Train Loss: 0.1115, Train Acc: 0.9544 | Val Loss: 0.1679, Val EER: 0.0862, Val F1: 0.9517\n",
      "Epoch 33/40 | Train Loss: 0.1007, Train Acc: 0.9593 | Val Loss: 0.2531, Val EER: 0.0687, Val F1: 0.9405\n",
      "Epoch 34/40 | Train Loss: 0.0907, Train Acc: 0.9640 | Val Loss: 0.1286, Val EER: 0.0557, Val F1: 0.9697\n",
      "-> New best model saved with EER: 0.0557\n",
      "Epoch 35/40 | Train Loss: 0.1938, Train Acc: 0.9176 | Val Loss: 0.3883, Val EER: 0.2170, Val F1: 0.9472\n",
      "Epoch 36/40 | Train Loss: 0.1945, Train Acc: 0.9198 | Val Loss: 0.3201, Val EER: 0.1958, Val F1: 0.9512\n",
      "Epoch 37/40 | Train Loss: 0.2061, Train Acc: 0.9141 | Val Loss: 0.5858, Val EER: 0.2982, Val F1: 0.9347\n",
      "Epoch 38/40 | Train Loss: 0.2080, Train Acc: 0.9136 | Val Loss: 0.6703, Val EER: 0.4458, Val F1: 0.9370\n",
      "Epoch 39/40 | Train Loss: 0.1981, Train Acc: 0.9167 | Val Loss: 0.5891, Val EER: 0.4239, Val F1: 0.9396\n",
      "Epoch 40/40 | Train Loss: 0.1872, Train Acc: 0.9218 | Val Loss: 0.6767, Val EER: 0.4056, Val F1: 0.9450\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0557\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/740938358.py:307: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.3534\n",
      "Test EER: 0.0934\n",
      "Test F1-Score: 0.9274\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Transformer + Cross-Attention Fusion Model ---\n",
    "class TransformerAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a Transformer and cross-attention.\n",
    "    \n",
    "    1. A Transformer Encoder processes the CQCC spectrogram as a sequence.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the Transformer's output sequence.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, nhead=8, num_encoder_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerAttentionFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- 1. CQCC Feature Processing Path ---\n",
    "        self.cqcc_projection = nn.Linear(cqcc_feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model), # Output dimension must match d_model for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector (size d_model) + the prosody query (size d_model)\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for Transformer: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC features\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x) * math.sqrt(self.d_model)\n",
    "        # Permute for positional encoding: (batch, seq, feature) -> (seq, batch, feature)\n",
    "        cqcc_pos = self.pos_encoder(cqcc_embed.permute(1, 0, 2))\n",
    "        # Permute back for transformer encoder: (seq, batch, feature) -> (batch, seq, feature)\n",
    "        transformer_out = self.transformer_encoder(cqcc_pos.permute(1, 0, 2)) # This is our Key and Value\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        keys = transformer_out\n",
    "        values = transformer_out\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5) # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector by applying weights to values\n",
    "        context = torch.bmm(attention_weights, values)\n",
    "        context = context.squeeze(1) # Remove the sequence dimension\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data/cqcc_features.npy'\n",
    "    TRAIN_COMBINED_FILE = 'processed_data/prosodic_features_and_labels.csv'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5 # Added weight decay\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetCombinedCSV(TRAIN_CQCC_FILE, TRAIN_COMBINED_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    # Use the Transformer Cross-Attention model\n",
    "    model = TransformerAttentionFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # Add weight_decay to the optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Step the scheduler based on the validation EER\n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c2248fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 46019 samples.\n",
      "Validation data loaded: 24844 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/40 | Train Loss: 0.5205, Train Acc: 0.7115 | Val Loss: 0.2788, Val EER: 0.1479, Val F1: 0.9151\n",
      "-> New best model saved with EER: 0.1479\n",
      "Epoch 2/40 | Train Loss: 0.2232, Train Acc: 0.8957 | Val Loss: 0.2119, Val EER: 0.1246, Val F1: 0.9216\n",
      "-> New best model saved with EER: 0.1246\n",
      "Epoch 3/40 | Train Loss: 0.1855, Train Acc: 0.9092 | Val Loss: 0.1841, Val EER: 0.1229, Val F1: 0.9381\n",
      "-> New best model saved with EER: 0.1229\n",
      "Epoch 4/40 | Train Loss: 0.1726, Train Acc: 0.9147 | Val Loss: 0.1909, Val EER: 0.1126, Val F1: 0.9256\n",
      "-> New best model saved with EER: 0.1126\n",
      "Epoch 5/40 | Train Loss: 0.1657, Train Acc: 0.9190 | Val Loss: 0.2011, Val EER: 0.1103, Val F1: 0.9244\n",
      "-> New best model saved with EER: 0.1103\n",
      "Epoch 6/40 | Train Loss: 0.1550, Train Acc: 0.9258 | Val Loss: 0.1730, Val EER: 0.1095, Val F1: 0.9459\n",
      "-> New best model saved with EER: 0.1095\n",
      "Epoch 7/40 | Train Loss: 0.1508, Train Acc: 0.9276 | Val Loss: 0.1705, Val EER: 0.1054, Val F1: 0.9463\n",
      "-> New best model saved with EER: 0.1054\n",
      "Epoch 8/40 | Train Loss: 0.1447, Train Acc: 0.9330 | Val Loss: 0.1575, Val EER: 0.1103, Val F1: 0.9569\n",
      "Epoch 9/40 | Train Loss: 0.1403, Train Acc: 0.9353 | Val Loss: 0.1752, Val EER: 0.1020, Val F1: 0.9457\n",
      "-> New best model saved with EER: 0.1020\n",
      "Epoch 10/40 | Train Loss: 0.1342, Train Acc: 0.9377 | Val Loss: 0.1547, Val EER: 0.1072, Val F1: 0.9567\n",
      "Epoch 11/40 | Train Loss: 0.1292, Train Acc: 0.9398 | Val Loss: 0.1722, Val EER: 0.1003, Val F1: 0.9476\n",
      "-> New best model saved with EER: 0.1003\n",
      "Epoch 12/40 | Train Loss: 0.1258, Train Acc: 0.9419 | Val Loss: 0.1585, Val EER: 0.0983, Val F1: 0.9525\n",
      "-> New best model saved with EER: 0.0983\n",
      "Epoch 13/40 | Train Loss: 0.1216, Train Acc: 0.9456 | Val Loss: 0.1458, Val EER: 0.1060, Val F1: 0.9620\n",
      "Epoch 14/40 | Train Loss: 0.1196, Train Acc: 0.9465 | Val Loss: 0.1420, Val EER: 0.0973, Val F1: 0.9615\n",
      "-> New best model saved with EER: 0.0973\n",
      "Epoch 15/40 | Train Loss: 0.1144, Train Acc: 0.9493 | Val Loss: 0.1470, Val EER: 0.0937, Val F1: 0.9590\n",
      "-> New best model saved with EER: 0.0937\n",
      "Epoch 16/40 | Train Loss: 0.1135, Train Acc: 0.9498 | Val Loss: 0.1385, Val EER: 0.0948, Val F1: 0.9641\n",
      "Epoch 17/40 | Train Loss: 0.1072, Train Acc: 0.9526 | Val Loss: 0.1365, Val EER: 0.0929, Val F1: 0.9657\n",
      "-> New best model saved with EER: 0.0929\n",
      "Epoch 18/40 | Train Loss: 0.1061, Train Acc: 0.9532 | Val Loss: 0.1330, Val EER: 0.0926, Val F1: 0.9655\n",
      "-> New best model saved with EER: 0.0926\n",
      "Epoch 19/40 | Train Loss: 0.1039, Train Acc: 0.9545 | Val Loss: 0.1315, Val EER: 0.0920, Val F1: 0.9675\n",
      "-> New best model saved with EER: 0.0920\n",
      "Epoch 20/40 | Train Loss: 0.1016, Train Acc: 0.9567 | Val Loss: 0.1362, Val EER: 0.0907, Val F1: 0.9644\n",
      "-> New best model saved with EER: 0.0907\n",
      "Epoch 21/40 | Train Loss: 0.0986, Train Acc: 0.9586 | Val Loss: 0.1837, Val EER: 0.0828, Val F1: 0.9464\n",
      "-> New best model saved with EER: 0.0828\n",
      "Epoch 22/40 | Train Loss: 0.0945, Train Acc: 0.9588 | Val Loss: 0.1274, Val EER: 0.0878, Val F1: 0.9668\n",
      "Epoch 23/40 | Train Loss: 0.0930, Train Acc: 0.9604 | Val Loss: 0.1353, Val EER: 0.0916, Val F1: 0.9663\n",
      "Epoch 24/40 | Train Loss: 0.0913, Train Acc: 0.9615 | Val Loss: 0.1467, Val EER: 0.0922, Val F1: 0.9671\n",
      "Epoch 25/40 | Train Loss: 0.0885, Train Acc: 0.9630 | Val Loss: 0.1244, Val EER: 0.0861, Val F1: 0.9697\n",
      "Epoch 26/40 | Train Loss: 0.0909, Train Acc: 0.9604 | Val Loss: 0.1219, Val EER: 0.0838, Val F1: 0.9703\n",
      "Epoch 27/40 | Train Loss: 0.0856, Train Acc: 0.9633 | Val Loss: 0.1245, Val EER: 0.0856, Val F1: 0.9699\n",
      "Epoch 28/40 | Train Loss: 0.0788, Train Acc: 0.9668 | Val Loss: 0.1277, Val EER: 0.0877, Val F1: 0.9694\n",
      "Epoch 29/40 | Train Loss: 0.0779, Train Acc: 0.9674 | Val Loss: 0.1285, Val EER: 0.0879, Val F1: 0.9700\n",
      "Epoch 30/40 | Train Loss: 0.0758, Train Acc: 0.9677 | Val Loss: 0.1276, Val EER: 0.0871, Val F1: 0.9699\n",
      "Epoch 31/40 | Train Loss: 0.0765, Train Acc: 0.9678 | Val Loss: 0.1305, Val EER: 0.0880, Val F1: 0.9699\n",
      "Epoch 32/40 | Train Loss: 0.0764, Train Acc: 0.9676 | Val Loss: 0.1348, Val EER: 0.0889, Val F1: 0.9696\n",
      "Epoch 33/40 | Train Loss: 0.0759, Train Acc: 0.9691 | Val Loss: 0.1341, Val EER: 0.0899, Val F1: 0.9700\n",
      "Epoch 34/40 | Train Loss: 0.0741, Train Acc: 0.9693 | Val Loss: 0.1325, Val EER: 0.0897, Val F1: 0.9698\n",
      "Epoch 35/40 | Train Loss: 0.0751, Train Acc: 0.9688 | Val Loss: 0.1319, Val EER: 0.0888, Val F1: 0.9700\n",
      "Epoch 36/40 | Train Loss: 0.0740, Train Acc: 0.9687 | Val Loss: 0.1323, Val EER: 0.0892, Val F1: 0.9700\n",
      "Epoch 37/40 | Train Loss: 0.0735, Train Acc: 0.9700 | Val Loss: 0.1337, Val EER: 0.0894, Val F1: 0.9699\n",
      "Epoch 38/40 | Train Loss: 0.0733, Train Acc: 0.9690 | Val Loss: 0.1334, Val EER: 0.0889, Val F1: 0.9699\n",
      "Epoch 39/40 | Train Loss: 0.0741, Train Acc: 0.9690 | Val Loss: 0.1317, Val EER: 0.0887, Val F1: 0.9701\n",
      "Epoch 40/40 | Train Loss: 0.0739, Train Acc: 0.9697 | Val Loss: 0.1332, Val EER: 0.0889, Val F1: 0.9698\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0828\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/2089772013.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.8347\n",
      "Test EER: 0.1621\n",
      "Test F1-Score: 0.8691\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- CNN + Cross-Attention Fusion Model ---\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that uses cross-attention to fuse prosodic features and CQCC features.\n",
    "    \n",
    "    1. A CNN processes the CQCC spectrogram to extract spatial features.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the CNN's output feature map.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_shape, prosody_feature_dim, embed_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_shape (tuple): The shape of the input CQCC (channels, features, frames).\n",
    "            prosody_feature_dim (int): The number of prosodic features.\n",
    "            embed_dim (int): The dimensionality of the common embedding space for attention.\n",
    "        \"\"\"\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # --- 1. CNN for CQCC Feature Extraction ---\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "        )\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, embed_dim), # Output dimension must match embed_dim for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector (size embed_dim) + the prosody query (size embed_dim)\n",
    "        classifier_input_dim = embed_dim + embed_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # Add a channel dimension to CQCC input for the CNN\n",
    "        # Shape: (batch, features, frames) -> (batch, 1, features, frames)\n",
    "        cqcc_x = cqcc_x.unsqueeze(1)\n",
    "        \n",
    "        # 1. Process inputs through their respective networks\n",
    "        # cqcc_map shape: (batch, embed_dim, H', W')\n",
    "        cqcc_map = self.cnn_extractor(cqcc_x)\n",
    "        \n",
    "        # prosody_query shape: (batch, embed_dim)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # --- 2. Cross-Attention Mechanism ---\n",
    "        batch_size, _, H, W = cqcc_map.shape\n",
    "        \n",
    "        # Flatten the spatial dimensions of the CQCC map to create a sequence of features\n",
    "        # This will be our 'value' and 'key' in the attention mechanism.\n",
    "        # Shape: (batch, embed_dim, H', W') -> (batch, embed_dim, H'*W')\n",
    "        cqcc_seq = cqcc_map.view(batch_size, self.embed_dim, -1)\n",
    "        \n",
    "        # Reshape for attention calculation: (batch, H'*W', embed_dim)\n",
    "        keys = cqcc_seq.permute(0, 2, 1)\n",
    "        values = keys # In this case, keys and values are the same\n",
    "        \n",
    "        # The prosody vector is our 'query'. We need to expand it for matrix multiplication.\n",
    "        # Shape: (batch, embed_dim) -> (batch, 1, embed_dim)\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention scores (Query * Key^T)\n",
    "        # Q: (batch, 1, embed_dim), K^T: (batch, embed_dim, H'*W') -> Scores: (batch, 1, H'*W')\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        \n",
    "        # Scale the scores\n",
    "        attention_scores = attention_scores / (self.embed_dim ** 0.5)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply weights to the CQCC sequence (Value) to get the context vector\n",
    "        # Weights: (batch, 1, H'*W'), V: (batch, H'*W', embed_dim) -> Context: (batch, 1, embed_dim)\n",
    "        context = torch.bmm(attention_weights, values)\n",
    "        \n",
    "        # Remove the middle dimension: (batch, 1, embed_dim) -> (batch, embed_dim)\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        # --- 3. Final Classification ---\n",
    "        # Concatenate the attention output with the original processed prosody vector\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        \n",
    "        # Pass the fused features through the final classifier\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data/cqcc_features.npy'\n",
    "    TRAIN_COMBINED_FILE = 'processed_data/prosodic_features_and_labels.csv'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5 # Added weight decay\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetCombinedCSV(TRAIN_CQCC_FILE, TRAIN_COMBINED_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_shape = (1, cqcc_sample.shape[0], cqcc_sample.shape[1])\n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    # Use the CNN Cross-Attention model\n",
    "    model = CrossAttentionModel(\n",
    "        cqcc_shape=cqcc_shape,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # Add weight_decay to the optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Step the scheduler based on the validation EER\n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e898e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training data loaded: 46019 samples.\n",
      "Validation data loaded: 24844 samples.\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 0.4166, Train Acc: 0.8100 | Val Loss: 0.2086, Val EER: 0.0789, Val F1: 0.9593\n",
      "-> New best model saved with EER: 0.0789\n",
      "Epoch 2/40 | Train Loss: 0.1128, Train Acc: 0.9708 | Val Loss: 0.1313, Val EER: 0.0639, Val F1: 0.9700\n",
      "-> New best model saved with EER: 0.0639\n",
      "Epoch 3/40 | Train Loss: 0.0696, Train Acc: 0.9834 | Val Loss: 0.1063, Val EER: 0.0722, Val F1: 0.9849\n",
      "Epoch 4/40 | Train Loss: 0.0590, Train Acc: 0.9855 | Val Loss: 0.1127, Val EER: 0.0632, Val F1: 0.9821\n",
      "-> New best model saved with EER: 0.0632\n",
      "Epoch 5/40 | Train Loss: 0.0407, Train Acc: 0.9897 | Val Loss: 0.1549, Val EER: 0.0954, Val F1: 0.9832\n",
      "Epoch 6/40 | Train Loss: 0.0323, Train Acc: 0.9914 | Val Loss: 0.1436, Val EER: 0.0836, Val F1: 0.9838\n",
      "Epoch 7/40 | Train Loss: 0.0314, Train Acc: 0.9926 | Val Loss: 0.1613, Val EER: 0.0896, Val F1: 0.9850\n",
      "Epoch 8/40 | Train Loss: 0.0222, Train Acc: 0.9944 | Val Loss: 0.1408, Val EER: 0.0808, Val F1: 0.9856\n",
      "Epoch 9/40 | Train Loss: 0.0181, Train Acc: 0.9956 | Val Loss: 0.1011, Val EER: 0.0447, Val F1: 0.9832\n",
      "-> New best model saved with EER: 0.0447\n",
      "Epoch 10/40 | Train Loss: 0.0204, Train Acc: 0.9942 | Val Loss: 0.1325, Val EER: 0.0861, Val F1: 0.9844\n",
      "Epoch 11/40 | Train Loss: 0.0126, Train Acc: 0.9965 | Val Loss: 0.1829, Val EER: 0.0738, Val F1: 0.9829\n",
      "Epoch 12/40 | Train Loss: 0.0110, Train Acc: 0.9964 | Val Loss: 0.1567, Val EER: 0.0789, Val F1: 0.9838\n",
      "Epoch 13/40 | Train Loss: 0.0125, Train Acc: 0.9963 | Val Loss: 0.1121, Val EER: 0.0718, Val F1: 0.9862\n",
      "Epoch 14/40 | Train Loss: 0.0095, Train Acc: 0.9973 | Val Loss: 0.1347, Val EER: 0.0648, Val F1: 0.9861\n",
      "Epoch 15/40 | Train Loss: 0.0141, Train Acc: 0.9961 | Val Loss: 0.1434, Val EER: 0.0528, Val F1: 0.9851\n",
      "Epoch 16/40 | Train Loss: 0.0029, Train Acc: 0.9992 | Val Loss: 0.1846, Val EER: 0.0604, Val F1: 0.9834\n",
      "Epoch 17/40 | Train Loss: 0.0024, Train Acc: 0.9994 | Val Loss: 0.1976, Val EER: 0.0638, Val F1: 0.9836\n",
      "Epoch 18/40 | Train Loss: 0.0020, Train Acc: 0.9994 | Val Loss: 0.1736, Val EER: 0.0597, Val F1: 0.9854\n",
      "Epoch 19/40 | Train Loss: 0.0013, Train Acc: 0.9997 | Val Loss: 0.2000, Val EER: 0.0620, Val F1: 0.9840\n",
      "Epoch 20/40 | Train Loss: 0.0016, Train Acc: 0.9996 | Val Loss: 0.2051, Val EER: 0.0628, Val F1: 0.9841\n",
      "Epoch 21/40 | Train Loss: 0.0014, Train Acc: 0.9997 | Val Loss: 0.2013, Val EER: 0.0640, Val F1: 0.9845\n",
      "Epoch 22/40 | Train Loss: 0.0011, Train Acc: 0.9997 | Val Loss: 0.2190, Val EER: 0.0664, Val F1: 0.9838\n",
      "Epoch 23/40 | Train Loss: 0.0011, Train Acc: 0.9998 | Val Loss: 0.2113, Val EER: 0.0651, Val F1: 0.9841\n",
      "Epoch 24/40 | Train Loss: 0.0010, Train Acc: 0.9998 | Val Loss: 0.2131, Val EER: 0.0650, Val F1: 0.9841\n",
      "Epoch 25/40 | Train Loss: 0.0013, Train Acc: 0.9997 | Val Loss: 0.2215, Val EER: 0.0670, Val F1: 0.9838\n",
      "Epoch 26/40 | Train Loss: 0.0011, Train Acc: 0.9998 | Val Loss: 0.2214, Val EER: 0.0671, Val F1: 0.9841\n",
      "Epoch 27/40 | Train Loss: 0.0008, Train Acc: 0.9998 | Val Loss: 0.2241, Val EER: 0.0675, Val F1: 0.9841\n",
      "Epoch 28/40 | Train Loss: 0.0010, Train Acc: 0.9999 | Val Loss: 0.2237, Val EER: 0.0675, Val F1: 0.9841\n",
      "Epoch 29/40 | Train Loss: 0.0011, Train Acc: 0.9998 | Val Loss: 0.2258, Val EER: 0.0682, Val F1: 0.9841\n",
      "Epoch 30/40 | Train Loss: 0.0012, Train Acc: 0.9997 | Val Loss: 0.2264, Val EER: 0.0686, Val F1: 0.9840\n",
      "Epoch 31/40 | Train Loss: 0.0009, Train Acc: 0.9998 | Val Loss: 0.2263, Val EER: 0.0682, Val F1: 0.9840\n",
      "Epoch 32/40 | Train Loss: 0.0011, Train Acc: 0.9998 | Val Loss: 0.2264, Val EER: 0.0682, Val F1: 0.9841\n",
      "Epoch 33/40 | Train Loss: 0.0009, Train Acc: 0.9998 | Val Loss: 0.2287, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 34/40 | Train Loss: 0.0012, Train Acc: 0.9998 | Val Loss: 0.2287, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 35/40 | Train Loss: 0.0011, Train Acc: 0.9998 | Val Loss: 0.2285, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 36/40 | Train Loss: 0.0009, Train Acc: 0.9998 | Val Loss: 0.2285, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 37/40 | Train Loss: 0.0008, Train Acc: 0.9998 | Val Loss: 0.2285, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 38/40 | Train Loss: 0.0007, Train Acc: 0.9999 | Val Loss: 0.2285, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 39/40 | Train Loss: 0.0010, Train Acc: 0.9998 | Val Loss: 0.2285, Val EER: 0.0682, Val F1: 0.9839\n",
      "Epoch 40/40 | Train Loss: 0.0009, Train Acc: 0.9998 | Val Loss: 0.2286, Val EER: 0.0682, Val F1: 0.9839\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0447\n",
      "\n",
      "--- Starting Testing ---\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/3258654864.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.6690\n",
      "Test EER: 0.1052\n",
      "Test F1-Score: 0.9066\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- LSTM + Cross-Attention Fusion Model ---\n",
    "class LSTMAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a BiLSTM and cross-attention.\n",
    "    \n",
    "    1. A BiLSTM processes the CQCC spectrogram as a sequence of feature vectors.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the BiLSTM's output sequence.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, lstm_hidden_dim=128, lstm_layers=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cqcc_feature_dim (int): The number of CQCC coefficients.\n",
    "            prosody_feature_dim (int): The number of prosodic features.\n",
    "            lstm_hidden_dim (int): The hidden dimension size of the LSTM.\n",
    "            lstm_layers (int): The number of LSTM layers.\n",
    "        \"\"\"\n",
    "        super(LSTMAttentionFusionModel, self).__init__()\n",
    "        \n",
    "        self.lstm_output_dim = lstm_hidden_dim * 2 # Times 2 for bidirectional\n",
    "        \n",
    "        # --- 1. BiLSTM for CQCC Feature Extraction ---\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cqcc_feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        # This MLP creates the query for the attention mechanism.\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, self.lstm_output_dim), # Output must match LSTM output dim for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector + the prosody query vector.\n",
    "        # Both have size self.lstm_output_dim.\n",
    "        classifier_input_dim = self.lstm_output_dim + self.lstm_output_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for LSTM: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC through LSTM to get the full sequence output (keys and values)\n",
    "        # lstm_out shape: (batch, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(cqcc_x)\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        # prosody_query shape: (batch, lstm_output_dim)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        keys = lstm_out\n",
    "        values = lstm_out\n",
    "        \n",
    "        # Reshape query for batch matrix multiplication\n",
    "        # query shape: (batch, 1, lstm_output_dim)\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        # (batch, 1, embed_dim) * (batch, embed_dim, seq_len) -> (batch, 1, seq_len)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5) # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector: Weights * V\n",
    "        # (batch, 1, seq_len) * (batch, seq_len, lstm_output_dim) -> (batch, 1, lstm_output_dim)\n",
    "        context = torch.bmm(attention_weights, values)\n",
    "        context = context.squeeze(1) # Remove the sequence dimension\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Datasets ---\n",
    "class AudioSpoofDatasetNPY(Dataset):\n",
    "    \"\"\"Custom Dataset for loading all data from .npy files (for training).\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more training feature files not found.\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Training data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "class AudioSpoofDatasetCombinedCSV(Dataset):\n",
    "    \"\"\"Custom Dataset for val/test: CQCC from .npy, prosody and labels from one combined .csv.\"\"\"\n",
    "    def __init__(self, cqcc_file, combined_csv_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, combined_csv_file]):\n",
    "            raise FileNotFoundError(f\"Validation or Test feature files not found: {cqcc_file}, {combined_csv_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        combined_data = pd.read_csv(combined_csv_file)\n",
    "        \n",
    "        if 'label' not in combined_data.columns:\n",
    "            raise ValueError(\"The combined CSV file must contain a 'label' column.\")\n",
    "        self.labels = combined_data['label'].values\n",
    "        \n",
    "        metadata_cols = ['label']\n",
    "        if 'filename' in combined_data.columns:\n",
    "            metadata_cols.append('filename')\n",
    "        if 'attack_id' in combined_data.columns:\n",
    "            metadata_cols.append('attack_id')\n",
    "        \n",
    "        self.prosody_data = combined_data.drop(columns=metadata_cols).values\n",
    "\n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data/cqcc_features.npy'\n",
    "    TRAIN_COMBINED_FILE = 'processed_data/prosodic_features_and_labels.csv'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data/cqcc_features_val.npy'\n",
    "    VAL_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_val.csv'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data/cqcc_features_test.npy'\n",
    "    TEST_COMBINED_FILE = 'processed_data/prosodic_features_and_labels_test.csv'\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5 # Added weight decay\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDatasetCombinedCSV(TRAIN_CQCC_FILE, TRAIN_COMBINED_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDatasetCombinedCSV(VAL_CQCC_FILE, VAL_COMBINED_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    # Use the LSTM Cross-Attention model\n",
    "    model = LSTMAttentionFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # Add weight_decay to the optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Step the scheduler based on the validation EER\n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDatasetCombinedCSV(TEST_CQCC_FILE, TEST_COMBINED_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "281f9fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CQCC data shape: (46019, 128, 157)\n",
      "Prosody data shape: (46019, 23, 157)\n",
      "Labels shape: (46019,)\n",
      "Training data loaded: 46019 samples.\n",
      "CQCC data shape: (24844, 128, 157)\n",
      "Prosody data shape: (24844, 23, 157)\n",
      "Labels shape: (24844,)\n",
      "Validation data loaded: 24844 samples.\n",
      "CQCC feature dimension: 128\n",
      "Prosody feature dimension: 3611\n",
      "Prosody sample shape: torch.Size([23, 157])\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/40 | Train Loss: 0.4842, Train Acc: 0.7736 | Val Loss: 0.5272, Val EER: 0.1462, Val F1: 0.4273\n",
      "-> New best model saved with EER: 0.1462\n",
      "Epoch 2/40 | Train Loss: 0.1555, Train Acc: 0.9392 | Val Loss: 0.2303, Val EER: 0.1083, Val F1: 0.6217\n",
      "-> New best model saved with EER: 0.1083\n",
      "Epoch 3/40 | Train Loss: 0.1001, Train Acc: 0.9625 | Val Loss: 0.5237, Val EER: 0.1122, Val F1: 0.4946\n",
      "Epoch 4/40 | Train Loss: 0.0775, Train Acc: 0.9718 | Val Loss: 0.2234, Val EER: 0.1090, Val F1: 0.6642\n",
      "Epoch 5/40 | Train Loss: 0.0613, Train Acc: 0.9781 | Val Loss: 0.1393, Val EER: 0.0738, Val F1: 0.7781\n",
      "-> New best model saved with EER: 0.0738\n",
      "Epoch 6/40 | Train Loss: 0.0470, Train Acc: 0.9842 | Val Loss: 0.1116, Val EER: 0.0738, Val F1: 0.8268\n",
      "-> New best model saved with EER: 0.0738\n",
      "Epoch 7/40 | Train Loss: 0.0373, Train Acc: 0.9877 | Val Loss: 0.1827, Val EER: 0.0597, Val F1: 0.7655\n",
      "-> New best model saved with EER: 0.0597\n",
      "Epoch 8/40 | Train Loss: 0.0305, Train Acc: 0.9896 | Val Loss: 0.1579, Val EER: 0.0840, Val F1: 0.7796\n",
      "Epoch 9/40 | Train Loss: 0.0270, Train Acc: 0.9907 | Val Loss: 0.0929, Val EER: 0.0557, Val F1: 0.8735\n",
      "-> New best model saved with EER: 0.0557\n",
      "Epoch 10/40 | Train Loss: 0.0204, Train Acc: 0.9931 | Val Loss: 0.1348, Val EER: 0.0553, Val F1: 0.8711\n",
      "-> New best model saved with EER: 0.0553\n",
      "Epoch 11/40 | Train Loss: 0.0180, Train Acc: 0.9938 | Val Loss: 0.0876, Val EER: 0.0490, Val F1: 0.8872\n",
      "-> New best model saved with EER: 0.0490\n",
      "Epoch 12/40 | Train Loss: 0.0149, Train Acc: 0.9952 | Val Loss: 0.0943, Val EER: 0.0424, Val F1: 0.9012\n",
      "-> New best model saved with EER: 0.0424\n",
      "Epoch 13/40 | Train Loss: 0.0129, Train Acc: 0.9957 | Val Loss: 0.1311, Val EER: 0.0479, Val F1: 0.8809\n",
      "Epoch 14/40 | Train Loss: 0.0120, Train Acc: 0.9959 | Val Loss: 0.1641, Val EER: 0.0502, Val F1: 0.8378\n",
      "Epoch 15/40 | Train Loss: 0.0124, Train Acc: 0.9959 | Val Loss: 0.2215, Val EER: 0.0601, Val F1: 0.8024\n",
      "Epoch 16/40 | Train Loss: 0.0161, Train Acc: 0.9950 | Val Loss: 0.1062, Val EER: 0.0432, Val F1: 0.8958\n",
      "Epoch 17/40 | Train Loss: 0.0080, Train Acc: 0.9974 | Val Loss: 0.1798, Val EER: 0.0728, Val F1: 0.8512\n",
      "Epoch 18/40 | Train Loss: 0.0080, Train Acc: 0.9972 | Val Loss: 0.1635, Val EER: 0.0572, Val F1: 0.8824\n",
      "Epoch 19/40 | Train Loss: 0.0031, Train Acc: 0.9992 | Val Loss: 0.1234, Val EER: 0.0455, Val F1: 0.8945\n",
      "Epoch 20/40 | Train Loss: 0.0025, Train Acc: 0.9992 | Val Loss: 0.1516, Val EER: 0.0436, Val F1: 0.8874\n",
      "Epoch 21/40 | Train Loss: 0.0022, Train Acc: 0.9993 | Val Loss: 0.1353, Val EER: 0.0412, Val F1: 0.8947\n",
      "-> New best model saved with EER: 0.0412\n",
      "Epoch 22/40 | Train Loss: 0.0016, Train Acc: 0.9995 | Val Loss: 0.1635, Val EER: 0.0443, Val F1: 0.8825\n",
      "Epoch 23/40 | Train Loss: 0.0016, Train Acc: 0.9995 | Val Loss: 0.1470, Val EER: 0.0440, Val F1: 0.8966\n",
      "Epoch 24/40 | Train Loss: 0.0016, Train Acc: 0.9994 | Val Loss: 0.1561, Val EER: 0.0412, Val F1: 0.8933\n",
      "-> New best model saved with EER: 0.0412\n",
      "Epoch 25/40 | Train Loss: 0.0018, Train Acc: 0.9994 | Val Loss: 0.1385, Val EER: 0.0420, Val F1: 0.9031\n",
      "Epoch 26/40 | Train Loss: 0.0018, Train Acc: 0.9994 | Val Loss: 0.1370, Val EER: 0.0389, Val F1: 0.9025\n",
      "-> New best model saved with EER: 0.0389\n",
      "Epoch 27/40 | Train Loss: 0.0017, Train Acc: 0.9994 | Val Loss: 0.1528, Val EER: 0.0396, Val F1: 0.9025\n",
      "Epoch 28/40 | Train Loss: 0.0010, Train Acc: 0.9997 | Val Loss: 0.2072, Val EER: 0.0375, Val F1: 0.8817\n",
      "-> New best model saved with EER: 0.0375\n",
      "Epoch 29/40 | Train Loss: 0.0011, Train Acc: 0.9996 | Val Loss: 0.1934, Val EER: 0.0400, Val F1: 0.8919\n",
      "Epoch 30/40 | Train Loss: 0.0016, Train Acc: 0.9995 | Val Loss: 0.2111, Val EER: 0.0370, Val F1: 0.8814\n",
      "-> New best model saved with EER: 0.0370\n",
      "Epoch 31/40 | Train Loss: 0.0012, Train Acc: 0.9996 | Val Loss: 0.2021, Val EER: 0.0444, Val F1: 0.8924\n",
      "Epoch 32/40 | Train Loss: 0.0014, Train Acc: 0.9996 | Val Loss: 0.1826, Val EER: 0.0413, Val F1: 0.8989\n",
      "Epoch 33/40 | Train Loss: 0.0013, Train Acc: 0.9996 | Val Loss: 0.2103, Val EER: 0.0384, Val F1: 0.8868\n",
      "Epoch 34/40 | Train Loss: 0.0011, Train Acc: 0.9996 | Val Loss: 0.2111, Val EER: 0.0379, Val F1: 0.8875\n",
      "Epoch 35/40 | Train Loss: 0.0009, Train Acc: 0.9997 | Val Loss: 0.2095, Val EER: 0.0424, Val F1: 0.8913\n",
      "Epoch 36/40 | Train Loss: 0.0010, Train Acc: 0.9996 | Val Loss: 0.2037, Val EER: 0.0404, Val F1: 0.8942\n",
      "Epoch 37/40 | Train Loss: 0.0010, Train Acc: 0.9997 | Val Loss: 0.1970, Val EER: 0.0404, Val F1: 0.8988\n",
      "Epoch 38/40 | Train Loss: 0.0006, Train Acc: 0.9998 | Val Loss: 0.1918, Val EER: 0.0404, Val F1: 0.9003\n",
      "Epoch 39/40 | Train Loss: 0.0008, Train Acc: 0.9997 | Val Loss: 0.2009, Val EER: 0.0409, Val F1: 0.8973\n",
      "Epoch 40/40 | Train Loss: 0.0010, Train Acc: 0.9997 | Val Loss: 0.1983, Val EER: 0.0408, Val F1: 0.8982\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0370\n",
      "\n",
      "--- Starting Testing ---\n",
      "CQCC data shape: (71237, 128, 157)\n",
      "Prosody data shape: (71237, 23, 157)\n",
      "Labels shape: (71237,)\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/311372358.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.5361\n",
      "Test EER: 0.0883\n",
      "Test F1-Score: 0.7114\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Transformer + Cross-Attention Fusion Model ---\n",
    "class TransformerAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a Transformer and cross-attention.\n",
    "    \n",
    "    1. A Transformer Encoder processes the CQCC spectrogram as a sequence.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the Transformer's output sequence.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, nhead=8, num_encoder_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerAttentionFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- 1. CQCC Feature Processing Path ---\n",
    "        self.cqcc_projection = nn.Linear(cqcc_feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        # Updated to handle the actual prosody feature dimension\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256),  # Increased capacity for larger input\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model), # Output dimension must match d_model for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector (size d_model) + the prosody query (size d_model)\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for Transformer: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC features\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x) * math.sqrt(self.d_model)\n",
    "        # Permute for positional encoding: (batch, seq, feature) -> (seq, batch, feature)\n",
    "        cqcc_pos = self.pos_encoder(cqcc_embed.permute(1, 0, 2))\n",
    "        # Permute back for transformer encoder: (seq, batch, feature) -> (batch, seq, feature)\n",
    "        transformer_out = self.transformer_encoder(cqcc_pos.permute(1, 0, 2)) # This is our Key and Value\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        # Flatten prosody features if they have multiple dimensions\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        \n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        keys = transformer_out\n",
    "        values = transformer_out\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5) # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector by applying weights to values\n",
    "        context = torch.bmm(attention_weights, values)\n",
    "        context = context.squeeze(1) # Remove the sequence dimension\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class AudioSpoofDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CQCC, prosody, and labels from .npy files.\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more feature files not found: {cqcc_file}, {prosody_file}, {labels_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "        \n",
    "        # Debug: Print shapes to understand the data structure\n",
    "        print(f\"CQCC data shape: {self.cqcc_data.shape}\")\n",
    "        print(f\"Prosody data shape: {self.prosody_data.shape}\")\n",
    "        print(f\"Labels shape: {self.labels.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Update these paths to your new .npy files\n",
    "    \n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_train.npy'\n",
    "    TRAIN_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_train.npy'\n",
    "    TRAIN_LABELS_FILE = 'processed_data_aligned_lld/labels_train.npy'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_dev.npy'\n",
    "    VAL_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_dev.npy'\n",
    "    VAL_LABELS_FILE = 'processed_data_aligned_lld/labels_dev.npy'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_test.npy'\n",
    "    TEST_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_test.npy'\n",
    "    TEST_LABELS_FILE = 'processed_data_aligned_lld/labels_test.npy'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDataset(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDataset(VAL_CQCC_FILE, VAL_PROSODY_FILE, VAL_LABELS_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    \n",
    "    # Handle prosody dimensions properly\n",
    "    if prosody_sample.dim() > 1:\n",
    "        prosody_dim = prosody_sample.numel()  # Total number of elements if multi-dimensional\n",
    "    else:\n",
    "        prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    print(f\"CQCC feature dimension: {cqcc_dim}\")\n",
    "    print(f\"Prosody feature dimension: {prosody_dim}\")\n",
    "    print(f\"Prosody sample shape: {prosody_sample.shape}\")\n",
    "\n",
    "    # Use the Transformer Cross-Attention model\n",
    "    model = TransformerAttentionFusionModel(\n",
    "        cqcc_feature_dim=cqcc_dim,\n",
    "        prosody_feature_dim=prosody_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDataset(TEST_CQCC_FILE, TEST_PROSODY_FILE, TEST_LABELS_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_model.pth' found to test. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99e6f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CQCC data shape: (46019, 128, 157)\n",
      "Prosody data shape: (46019, 23, 157)\n",
      "Labels shape: (46019,)\n",
      "Training data loaded: 46019 samples.\n",
      "CQCC data shape: (24844, 128, 157)\n",
      "Prosody data shape: (24844, 23, 157)\n",
      "Labels shape: (24844,)\n",
      "Validation data loaded: 24844 samples.\n",
      "CQCC feature dimension: 128\n",
      "Prosody feature dimension: 3611\n",
      "Prosody sample shape: torch.Size([23, 157])\n",
      "Using ResNet-style CNN model\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 0.5494, Train Acc: 0.7297 | Val Loss: 0.3105, Val EER: 0.1133, Val F1: 0.5731\n",
      "-> New best model saved with EER: 0.1133\n",
      "Epoch 2/40 | Train Loss: 0.2009, Train Acc: 0.9183 | Val Loss: 0.1604, Val EER: 0.1115, Val F1: 0.6115\n",
      "-> New best model saved with EER: 0.1115\n",
      "Epoch 3/40 | Train Loss: 0.1628, Train Acc: 0.9327 | Val Loss: 0.1556, Val EER: 0.1022, Val F1: 0.6588\n",
      "-> New best model saved with EER: 0.1022\n",
      "Epoch 4/40 | Train Loss: 0.1555, Train Acc: 0.9367 | Val Loss: 0.1521, Val EER: 0.1019, Val F1: 0.6597\n",
      "-> New best model saved with EER: 0.1019\n",
      "Epoch 5/40 | Train Loss: 0.1448, Train Acc: 0.9399 | Val Loss: 0.1564, Val EER: 0.1024, Val F1: 0.6594\n",
      "Epoch 6/40 | Train Loss: 0.1115, Train Acc: 0.9591 | Val Loss: 0.1228, Val EER: 0.0644, Val F1: 0.8096\n",
      "-> New best model saved with EER: 0.0644\n",
      "Epoch 7/40 | Train Loss: 0.1374, Train Acc: 0.9465 | Val Loss: 0.1361, Val EER: 0.0852, Val F1: 0.7667\n",
      "Epoch 8/40 | Train Loss: 0.0564, Train Acc: 0.9831 | Val Loss: 0.1248, Val EER: 0.0658, Val F1: 0.8410\n",
      "Epoch 9/40 | Train Loss: 0.0348, Train Acc: 0.9897 | Val Loss: 0.1099, Val EER: 0.0584, Val F1: 0.8576\n",
      "-> New best model saved with EER: 0.0584\n",
      "Epoch 10/40 | Train Loss: 0.0252, Train Acc: 0.9925 | Val Loss: 0.1827, Val EER: 0.0677, Val F1: 0.8074\n",
      "Epoch 11/40 | Train Loss: 0.0176, Train Acc: 0.9942 | Val Loss: 0.2734, Val EER: 0.1133, Val F1: 0.7303\n",
      "Epoch 12/40 | Train Loss: 0.0165, Train Acc: 0.9948 | Val Loss: 0.2412, Val EER: 0.0549, Val F1: 0.7560\n",
      "-> New best model saved with EER: 0.0549\n",
      "Epoch 13/40 | Train Loss: 0.0118, Train Acc: 0.9963 | Val Loss: 0.1834, Val EER: 0.0505, Val F1: 0.7848\n",
      "-> New best model saved with EER: 0.0505\n",
      "Epoch 14/40 | Train Loss: 0.0647, Train Acc: 0.9804 | Val Loss: 0.1950, Val EER: 0.0604, Val F1: 0.8236\n",
      "Epoch 15/40 | Train Loss: 0.0536, Train Acc: 0.9853 | Val Loss: 0.2513, Val EER: 0.0801, Val F1: 0.5852\n",
      "Epoch 16/40 | Train Loss: 0.0223, Train Acc: 0.9930 | Val Loss: 0.0956, Val EER: 0.0522, Val F1: 0.8776\n",
      "Epoch 17/40 | Train Loss: 0.0106, Train Acc: 0.9968 | Val Loss: 0.0925, Val EER: 0.0447, Val F1: 0.8913\n",
      "-> New best model saved with EER: 0.0447\n",
      "Epoch 18/40 | Train Loss: 0.0085, Train Acc: 0.9976 | Val Loss: 0.0974, Val EER: 0.0443, Val F1: 0.8897\n",
      "-> New best model saved with EER: 0.0443\n",
      "Epoch 19/40 | Train Loss: 0.0070, Train Acc: 0.9978 | Val Loss: 0.1467, Val EER: 0.0495, Val F1: 0.8580\n",
      "Epoch 20/40 | Train Loss: 0.0056, Train Acc: 0.9982 | Val Loss: 0.1059, Val EER: 0.0432, Val F1: 0.8859\n",
      "-> New best model saved with EER: 0.0432\n",
      "Epoch 21/40 | Train Loss: 0.0047, Train Acc: 0.9987 | Val Loss: 0.1316, Val EER: 0.0440, Val F1: 0.8749\n",
      "Epoch 22/40 | Train Loss: 0.0058, Train Acc: 0.9982 | Val Loss: 0.1264, Val EER: 0.0546, Val F1: 0.8867\n",
      "Epoch 23/40 | Train Loss: 0.0061, Train Acc: 0.9983 | Val Loss: 0.1045, Val EER: 0.0483, Val F1: 0.8977\n",
      "Epoch 24/40 | Train Loss: 0.0034, Train Acc: 0.9989 | Val Loss: 0.1509, Val EER: 0.0431, Val F1: 0.8722\n",
      "-> New best model saved with EER: 0.0431\n",
      "Epoch 25/40 | Train Loss: 0.0048, Train Acc: 0.9988 | Val Loss: 0.1521, Val EER: 0.0475, Val F1: 0.8682\n",
      "Epoch 26/40 | Train Loss: 0.0055, Train Acc: 0.9983 | Val Loss: 0.1541, Val EER: 0.0593, Val F1: 0.8779\n",
      "Epoch 27/40 | Train Loss: 0.0036, Train Acc: 0.9992 | Val Loss: 0.1297, Val EER: 0.0408, Val F1: 0.8770\n",
      "-> New best model saved with EER: 0.0408\n",
      "Epoch 28/40 | Train Loss: 0.0046, Train Acc: 0.9987 | Val Loss: 0.1027, Val EER: 0.0401, Val F1: 0.8980\n",
      "-> New best model saved with EER: 0.0401\n",
      "Epoch 29/40 | Train Loss: 0.0025, Train Acc: 0.9993 | Val Loss: 0.1251, Val EER: 0.0389, Val F1: 0.8973\n",
      "-> New best model saved with EER: 0.0389\n",
      "Epoch 30/40 | Train Loss: 0.0038, Train Acc: 0.9988 | Val Loss: 0.1311, Val EER: 0.0408, Val F1: 0.8758\n",
      "Epoch 31/40 | Train Loss: 0.0030, Train Acc: 0.9991 | Val Loss: 0.1115, Val EER: 0.0483, Val F1: 0.8944\n",
      "Epoch 32/40 | Train Loss: 0.0849, Train Acc: 0.9732 | Val Loss: 0.0960, Val EER: 0.0680, Val F1: 0.8092\n",
      "Epoch 33/40 | Train Loss: 0.0470, Train Acc: 0.9855 | Val Loss: 0.1053, Val EER: 0.0648, Val F1: 0.7958\n",
      "Epoch 34/40 | Train Loss: 0.0278, Train Acc: 0.9930 | Val Loss: 0.2077, Val EER: 0.0703, Val F1: 0.8436\n",
      "Epoch 35/40 | Train Loss: 0.0182, Train Acc: 0.9950 | Val Loss: 0.1775, Val EER: 0.0608, Val F1: 0.8317\n",
      "Epoch 36/40 | Train Loss: 0.0106, Train Acc: 0.9973 | Val Loss: 0.1887, Val EER: 0.0596, Val F1: 0.8183\n",
      "Epoch 37/40 | Train Loss: 0.0110, Train Acc: 0.9972 | Val Loss: 0.2201, Val EER: 0.0651, Val F1: 0.8249\n",
      "Epoch 38/40 | Train Loss: 0.0098, Train Acc: 0.9977 | Val Loss: 0.2507, Val EER: 0.0586, Val F1: 0.8376\n",
      "Epoch 39/40 | Train Loss: 0.0093, Train Acc: 0.9978 | Val Loss: 0.1987, Val EER: 0.0608, Val F1: 0.8454\n",
      "Epoch 40/40 | Train Loss: 0.0104, Train Acc: 0.9972 | Val Loss: 0.2085, Val EER: 0.0610, Val F1: 0.8553\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0389\n",
      "\n",
      "--- Starting Testing ---\n",
      "CQCC data shape: (71237, 128, 157)\n",
      "Prosody data shape: (71237, 23, 157)\n",
      "Labels shape: (71237,)\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/1571906051.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.8916\n",
      "Test EER: 0.1105\n",
      "Test F1-Score: 0.6058\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- CNN + Cross-Attention Fusion Model ---\n",
    "class CNNAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using a CNN and cross-attention.\n",
    "    \n",
    "    1. A CNN processes the CQCC spectrogram as a 2D feature map.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the CNN's output feature maps.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, dropout=0.1):\n",
    "        super(CNNAttentionFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- 1. CNN for CQCC Feature Processing ---\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(cqcc_feature_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            nn.Conv1d(256, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling alternative (if needed)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model), # Output dimension must match d_model for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector (size d_model) + the prosody query (size d_model)\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) - perfect for Conv1d\n",
    "        \n",
    "        # 1. Process CQCC features with CNN\n",
    "        cnn_out = self.cnn_layers(cqcc_x)  # Shape: (batch, d_model, frames)\n",
    "        \n",
    "        # Permute for attention: (batch, d_model, frames) -> (batch, frames, d_model)\n",
    "        cnn_features = cnn_out.permute(0, 2, 1)  # This will be our Keys and Values\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        # Flatten prosody features if they have multiple dimensions\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        \n",
    "        prosody_query = self.prosody_mlp(prosody_x)  # Shape: (batch, d_model)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        keys = cnn_features  # Shape: (batch, frames, d_model)\n",
    "        values = cnn_features  # Shape: (batch, frames, d_model)\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)  # Shape: (batch, 1, d_model)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))  # (batch, 1, frames)\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5)  # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector by applying weights to values\n",
    "        context = torch.bmm(attention_weights, values)  # (batch, 1, d_model)\n",
    "        context = context.squeeze(1)  # Remove the sequence dimension -> (batch, d_model)\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)  # (batch, 2*d_model)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- Alternative CNN model with different architecture ---\n",
    "class CNNAttentionFusionModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative CNN architecture with residual connections and different structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, dropout=0.1):\n",
    "        super(CNNAttentionFusionModelV2, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # --- 1. CNN with Residual Blocks for CQCC ---\n",
    "        self.conv1 = nn.Conv1d(cqcc_feature_dim, 64, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = self._make_res_block(64, 64, kernel_size=3)\n",
    "        self.res_block2 = self._make_res_block(64, 128, kernel_size=3)\n",
    "        self.res_block3 = self._make_res_block(128, 256, kernel_size=3)\n",
    "        \n",
    "        # Final projection to d_model\n",
    "        self.final_conv = nn.Conv1d(256, d_model, kernel_size=1)\n",
    "        self.final_bn = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _make_res_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        \"\"\"Create a residual block.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # 1. Process CQCC features with CNN\n",
    "        x = F.relu(self.bn1(self.conv1(cqcc_x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        identity = x\n",
    "        x = self.res_block1(x)\n",
    "        if x.size(1) == identity.size(1):  # Same channels\n",
    "            x = F.relu(x + identity)\n",
    "        else:\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res_block2(x)\n",
    "        x = F.relu(x)  # Different channels, no residual connection\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res_block3(x)\n",
    "        x = F.relu(x)  # Different channels, no residual connection\n",
    "        \n",
    "        # Final projection\n",
    "        cnn_out = F.relu(self.final_bn(self.final_conv(x)))\n",
    "        \n",
    "        # Permute for attention: (batch, d_model, frames) -> (batch, frames, d_model)\n",
    "        cnn_features = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        # 2. Process prosodic features\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        \n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "        \n",
    "        # 3. Cross-Attention (same as in the first model)\n",
    "        keys = cnn_features\n",
    "        values = cnn_features\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        \n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1)\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class AudioSpoofDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CQCC, prosody, and labels from .npy files.\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more feature files not found: {cqcc_file}, {prosody_file}, {labels_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "        \n",
    "        # Debug: Print shapes to understand the data structure\n",
    "        print(f\"CQCC data shape: {self.cqcc_data.shape}\")\n",
    "        print(f\"Prosody data shape: {self.prosody_data.shape}\")\n",
    "        print(f\"Labels shape: {self.labels.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Update these paths to your new .npy files\n",
    "    \n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_train.npy'\n",
    "    TRAIN_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_train.npy'\n",
    "    TRAIN_LABELS_FILE = 'processed_data_aligned_lld/labels_train.npy'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_dev.npy'\n",
    "    VAL_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_dev.npy'\n",
    "    VAL_LABELS_FILE = 'processed_data_aligned_lld/labels_dev.npy'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_test.npy'\n",
    "    TEST_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_test.npy'\n",
    "    TEST_LABELS_FILE = 'processed_data_aligned_lld/labels_test.npy'\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    USE_RESNET_CNN = True  # Set to True to use the ResNet-style CNN (CNNAttentionFusionModelV2)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDataset(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDataset(VAL_CQCC_FILE, VAL_PROSODY_FILE, VAL_LABELS_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    \n",
    "    # Handle prosody dimensions properly\n",
    "    if prosody_sample.dim() > 1:\n",
    "        prosody_dim = prosody_sample.numel()  # Total number of elements if multi-dimensional\n",
    "    else:\n",
    "        prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    print(f\"CQCC feature dimension: {cqcc_dim}\")\n",
    "    print(f\"Prosody feature dimension: {prosody_dim}\")\n",
    "    print(f\"Prosody sample shape: {prosody_sample.shape}\")\n",
    "\n",
    "    # Choose CNN model\n",
    "    if USE_RESNET_CNN:\n",
    "        print(\"Using ResNet-style CNN model\")\n",
    "        model = CNNAttentionFusionModelV2(\n",
    "            cqcc_feature_dim=cqcc_dim,\n",
    "            prosody_feature_dim=prosody_dim\n",
    "        ).to(device)\n",
    "    else:\n",
    "        print(\"Using standard CNN model\")\n",
    "        model = CNNAttentionFusionModel(\n",
    "            cqcc_feature_dim=cqcc_dim,\n",
    "            prosody_feature_dim=prosody_dim\n",
    "        ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_cnn_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDataset(TEST_CQCC_FILE, TEST_PROSODY_FILE, TEST_LABELS_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_cnn_model.pth' found to test. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cd542a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CQCC data shape: (46019, 128, 157)\n",
      "Prosody data shape: (46019, 23, 157)\n",
      "Labels shape: (46019,)\n",
      "Training data loaded: 46019 samples.\n",
      "CQCC data shape: (24844, 128, 157)\n",
      "Prosody data shape: (24844, 23, 157)\n",
      "Labels shape: (24844,)\n",
      "Validation data loaded: 24844 samples.\n",
      "CQCC feature dimension: 128\n",
      "Prosody feature dimension: 3611\n",
      "Prosody sample shape: torch.Size([23, 157])\n",
      "Using Advanced BiLSTM model with multi-head attention\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.2955, Train Acc: 0.8596 | Val Loss: 0.0937, Val EER: 0.0582, Val F1: 0.8537\n",
      "-> New best model saved with EER: 0.0582\n",
      "Epoch 2/30 | Train Loss: 0.0610, Train Acc: 0.9870 | Val Loss: 0.1579, Val EER: 0.0494, Val F1: 0.8725\n",
      "-> New best model saved with EER: 0.0494\n",
      "Epoch 3/30 | Train Loss: 0.0400, Train Acc: 0.9934 | Val Loss: 0.2637, Val EER: 0.0648, Val F1: 0.8472\n",
      "Epoch 4/30 | Train Loss: 0.0288, Train Acc: 0.9953 | Val Loss: 0.2921, Val EER: 0.0805, Val F1: 0.8638\n",
      "Epoch 5/30 | Train Loss: 0.0245, Train Acc: 0.9961 | Val Loss: 0.2006, Val EER: 0.0718, Val F1: 0.8887\n",
      "Epoch 6/30 | Train Loss: 0.0175, Train Acc: 0.9972 | Val Loss: 0.2955, Val EER: 0.0570, Val F1: 0.8563\n",
      "Epoch 7/30 | Train Loss: 0.0125, Train Acc: 0.9982 | Val Loss: 0.2658, Val EER: 0.0586, Val F1: 0.8869\n",
      "Epoch 8/30 | Train Loss: 0.0112, Train Acc: 0.9982 | Val Loss: 0.1765, Val EER: 0.0440, Val F1: 0.9128\n",
      "-> New best model saved with EER: 0.0440\n",
      "Epoch 9/30 | Train Loss: 0.0112, Train Acc: 0.9984 | Val Loss: 0.4682, Val EER: 0.0569, Val F1: 0.7757\n",
      "Epoch 10/30 | Train Loss: 0.0089, Train Acc: 0.9985 | Val Loss: 0.2991, Val EER: 0.0424, Val F1: 0.8573\n",
      "-> New best model saved with EER: 0.0424\n",
      "Epoch 11/30 | Train Loss: 0.0078, Train Acc: 0.9987 | Val Loss: 0.1736, Val EER: 0.0416, Val F1: 0.9270\n",
      "-> New best model saved with EER: 0.0416\n",
      "Epoch 12/30 | Train Loss: 0.0049, Train Acc: 0.9993 | Val Loss: 0.1705, Val EER: 0.0416, Val F1: 0.9162\n",
      "Epoch 13/30 | Train Loss: 0.0056, Train Acc: 0.9990 | Val Loss: 0.3357, Val EER: 0.0420, Val F1: 0.8687\n",
      "Epoch 14/30 | Train Loss: 0.0062, Train Acc: 0.9991 | Val Loss: 0.1825, Val EER: 0.0412, Val F1: 0.9201\n",
      "-> New best model saved with EER: 0.0412\n",
      "Epoch 15/30 | Train Loss: 0.0088, Train Acc: 0.9986 | Val Loss: 0.2084, Val EER: 0.0459, Val F1: 0.9222\n",
      "Epoch 16/30 | Train Loss: 0.0036, Train Acc: 0.9994 | Val Loss: 0.3302, Val EER: 0.0518, Val F1: 0.8775\n",
      "Epoch 17/30 | Train Loss: 0.0074, Train Acc: 0.9991 | Val Loss: 0.3612, Val EER: 0.0717, Val F1: 0.8780\n",
      "Epoch 18/30 | Train Loss: 0.0022, Train Acc: 0.9996 | Val Loss: 0.4020, Val EER: 0.0734, Val F1: 0.8875\n",
      "Epoch 19/30 | Train Loss: 0.0039, Train Acc: 0.9995 | Val Loss: 0.3328, Val EER: 0.0389, Val F1: 0.9168\n",
      "-> New best model saved with EER: 0.0389\n",
      "Epoch 20/30 | Train Loss: 0.0044, Train Acc: 0.9993 | Val Loss: 0.4466, Val EER: 0.0977, Val F1: 0.8442\n",
      "Epoch 21/30 | Train Loss: 0.0049, Train Acc: 0.9994 | Val Loss: 0.3820, Val EER: 0.0699, Val F1: 0.8756\n",
      "Epoch 22/30 | Train Loss: 0.0049, Train Acc: 0.9993 | Val Loss: 0.2549, Val EER: 0.0459, Val F1: 0.8950\n",
      "Epoch 23/30 | Train Loss: 0.0029, Train Acc: 0.9995 | Val Loss: 0.2166, Val EER: 0.0385, Val F1: 0.8927\n",
      "-> New best model saved with EER: 0.0385\n",
      "Epoch 24/30 | Train Loss: 0.0038, Train Acc: 0.9995 | Val Loss: 0.5061, Val EER: 0.0565, Val F1: 0.8558\n",
      "Epoch 25/30 | Train Loss: 0.0004, Train Acc: 0.9999 | Val Loss: 0.4199, Val EER: 0.0597, Val F1: 0.8957\n",
      "Epoch 26/30 | Train Loss: 0.0000, Train Acc: 1.0000 | Val Loss: 0.4112, Val EER: 0.0585, Val F1: 0.8989\n",
      "Epoch 27/30 | Train Loss: 0.0000, Train Acc: 1.0000 | Val Loss: 0.4170, Val EER: 0.0569, Val F1: 0.8993\n",
      "Epoch 28/30 | Train Loss: 0.0048, Train Acc: 0.9993 | Val Loss: 0.3496, Val EER: 0.0938, Val F1: 0.9023\n",
      "Epoch 29/30 | Train Loss: 0.0048, Train Acc: 0.9993 | Val Loss: 0.2928, Val EER: 0.0495, Val F1: 0.9140\n",
      "Epoch 30/30 | Train Loss: 0.0003, Train Acc: 0.9999 | Val Loss: 0.4910, Val EER: 0.0734, Val F1: 0.8971\n",
      "\n",
      "--- Training Complete ---\n",
      "Best validation EER achieved: 0.0385\n",
      "\n",
      "--- Starting Testing ---\n",
      "CQCC data shape: (71237, 128, 157)\n",
      "Prosody data shape: (71237, 23, 157)\n",
      "Labels shape: (71237,)\n",
      "Test data loaded: 71237 samples.\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2868187/2723912294.py:431: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_bilstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 1.4918\n",
      "Test EER: 0.1048\n",
      "Test F1-Score: 0.6306\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- BiLSTM + Cross-Attention Fusion Model ---\n",
    "class BiLSTMAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that fuses CQCC and prosodic features using BiLSTM and cross-attention.\n",
    "    \n",
    "    1. A BiLSTM processes the CQCC spectrogram as a temporal sequence.\n",
    "    2. An MLP processes the 1D prosodic feature vector to create a query.\n",
    "    3. The prosody query attends to the BiLSTM's output sequence.\n",
    "    4. The resulting context vector is fused with the prosody query for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, hidden_dim=128, num_layers=2, dropout=0.1):\n",
    "        super(BiLSTMAttentionFusionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # --- 1. BiLSTM for CQCC Feature Processing ---\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=cqcc_feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # BiLSTM outputs 2 * hidden_dim (forward + backward)\n",
    "        self.bilstm_output_dim = 2 * hidden_dim\n",
    "        \n",
    "        # Optional: Project BiLSTM output to a specific dimension\n",
    "        self.bilstm_projection = nn.Linear(self.bilstm_output_dim, hidden_dim)\n",
    "        self.bilstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # --- 2. MLP for Prosodic Feature Processing ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, hidden_dim), # Output dimension must match hidden_dim for attention\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier Head ---\n",
    "        # The input is the context vector (size hidden_dim) + the prosody query (size hidden_dim)\n",
    "        classifier_input_dim = hidden_dim + hidden_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> Permute for LSTM: (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC features with BiLSTM\n",
    "        bilstm_out, (hidden, cell) = self.bilstm(cqcc_x)  # Shape: (batch, frames, 2*hidden_dim)\n",
    "        \n",
    "        # Project BiLSTM output to desired dimension\n",
    "        bilstm_features = self.bilstm_projection(bilstm_out)  # Shape: (batch, frames, hidden_dim)\n",
    "        bilstm_features = self.bilstm_dropout(bilstm_features)\n",
    "        \n",
    "        # 2. Process prosodic features to get the query\n",
    "        # Flatten prosody features if they have multiple dimensions\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        \n",
    "        prosody_query = self.prosody_mlp(prosody_x)  # Shape: (batch, hidden_dim)\n",
    "\n",
    "        # --- 3. Cross-Attention ---\n",
    "        keys = bilstm_features  # Shape: (batch, frames, hidden_dim)\n",
    "        values = bilstm_features  # Shape: (batch, frames, hidden_dim)\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)  # Shape: (batch, 1, hidden_dim)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))  # (batch, 1, frames)\n",
    "        attention_scores = attention_scores / (keys.size(-1) ** 0.5)  # Scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector by applying weights to values\n",
    "        context = torch.bmm(attention_weights, values)  # (batch, 1, hidden_dim)\n",
    "        context = context.squeeze(1)  # Remove the sequence dimension -> (batch, hidden_dim)\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)  # (batch, 2*hidden_dim)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- Alternative BiLSTM model with multiple layers and attention ---\n",
    "class BiLSTMAttentionFusionModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced BiLSTM model with multiple processing layers and self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, hidden_dim=128, num_layers=3, dropout=0.1):\n",
    "        super(BiLSTMAttentionFusionModelV2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # --- 1. Multi-layer BiLSTM for CQCC ---\n",
    "        self.bilstm1 = nn.LSTM(\n",
    "            input_size=cqcc_feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Additional BiLSTM layer for more complex feature extraction\n",
    "        self.bilstm2 = nn.LSTM(\n",
    "            input_size=2 * hidden_dim,\n",
    "            hidden_size=hidden_dim // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Final output dimension after second BiLSTM\n",
    "        self.bilstm_output_dim = hidden_dim  # (hidden_dim // 2) * 2 = hidden_dim\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.bilstm_output_dim)\n",
    "        \n",
    "        # --- 2. Enhanced MLP for Prosodic Features ---\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # --- 3. Multi-head attention (simplified) ---\n",
    "        self.num_heads = 4\n",
    "        self.head_dim = hidden_dim // self.num_heads\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # --- 4. Enhanced Classifier ---\n",
    "        classifier_input_dim = hidden_dim + hidden_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def multi_head_attention(self, query, keys, values):\n",
    "        \"\"\"Simplified multi-head attention mechanism.\"\"\"\n",
    "        batch_size, seq_len, hidden_dim = keys.size()\n",
    "        \n",
    "        # Project query, keys, values\n",
    "        Q = self.query_proj(query).view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key_proj(keys).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value_proj(values).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.out_proj(context)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # CQCC input shape: (batch, features, frames) -> (batch, frames, features)\n",
    "        cqcc_x = cqcc_x.permute(0, 2, 1)\n",
    "        \n",
    "        # 1. Process CQCC features with stacked BiLSTMs\n",
    "        bilstm_out1, _ = self.bilstm1(cqcc_x)\n",
    "        bilstm_out1 = self.dropout(bilstm_out1)\n",
    "        \n",
    "        bilstm_out2, _ = self.bilstm2(bilstm_out1)\n",
    "        bilstm_features = self.layer_norm(bilstm_out2)\n",
    "        \n",
    "        # 2. Process prosodic features\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        \n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "        \n",
    "        # 3. Multi-head Cross-Attention\n",
    "        context = self.multi_head_attention(\n",
    "            prosody_query.unsqueeze(1), \n",
    "            bilstm_features, \n",
    "            bilstm_features\n",
    "        )\n",
    "        \n",
    "        # 4. Fusion and Classification\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class AudioSpoofDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CQCC, prosody, and labels from .npy files.\"\"\"\n",
    "    def __init__(self, cqcc_file, prosody_file, labels_file):\n",
    "        if not all(os.path.exists(f) for f in [cqcc_file, prosody_file, labels_file]):\n",
    "            raise FileNotFoundError(f\"One or more feature files not found: {cqcc_file}, {prosody_file}, {labels_file}\")\n",
    "        \n",
    "        self.cqcc_data = np.load(cqcc_file)\n",
    "        self.prosody_data = np.load(prosody_file)\n",
    "        self.labels = np.load(labels_file)\n",
    "        \n",
    "        assert len(self.cqcc_data) == len(self.prosody_data) == len(self.labels), \"Data length mismatch!\"\n",
    "        \n",
    "        # Debug: Print shapes to understand the data structure\n",
    "        print(f\"CQCC data shape: {self.cqcc_data.shape}\")\n",
    "        print(f\"Prosody data shape: {self.prosody_data.shape}\")\n",
    "        print(f\"Labels shape: {self.labels.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqcc = torch.tensor(self.cqcc_data[idx], dtype=torch.float32)\n",
    "        prosody = torch.tensor(self.prosody_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "        return cqcc, prosody, label\n",
    "\n",
    "# --- Evaluation Metric ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for cqcc, prosody, labels in dataloader:\n",
    "        cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cqcc, prosody)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for LSTM stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in dataloader:\n",
    "            cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "            outputs = model(cqcc, prosody)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            scores = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_scores.extend(scores.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_scores = np.array(all_scores)\n",
    "    \n",
    "    eer = calculate_eer(y_true, y_scores)\n",
    "    y_pred = (y_scores > 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return avg_loss, eer, f1\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Update these paths to your new .npy files\n",
    "    \n",
    "    # Training data files\n",
    "    TRAIN_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_train.npy'\n",
    "    TRAIN_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_train.npy'\n",
    "    TRAIN_LABELS_FILE = 'processed_data_aligned_lld/labels_train.npy'\n",
    "    \n",
    "    # Validation data files\n",
    "    VAL_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_dev.npy'\n",
    "    VAL_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_dev.npy'\n",
    "    VAL_LABELS_FILE = 'processed_data_aligned_lld/labels_dev.npy'\n",
    "\n",
    "    # Test data files\n",
    "    TEST_CQCC_FILE = 'processed_data_aligned_lld/cqcc_features_test.npy'\n",
    "    TEST_PROSODY_FILE = 'processed_data_aligned_lld/egmaps_lld_features_test.npy'\n",
    "    TEST_LABELS_FILE = 'processed_data_aligned_lld/labels_test.npy'\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 0.0001\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    USE_ADVANCED_BILSTM = True  # Set to True to use the advanced BiLSTM model (BiLSTMAttentionFusionModelV2)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load Data and Create DataLoaders ---\n",
    "    try:\n",
    "        train_dataset = AudioSpoofDataset(TRAIN_CQCC_FILE, TRAIN_PROSODY_FILE, TRAIN_LABELS_FILE)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Training data loaded: {len(train_dataset)} samples.\")\n",
    "        \n",
    "        val_dataset = AudioSpoofDataset(VAL_CQCC_FILE, VAL_PROSODY_FILE, VAL_LABELS_FILE)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(f\"Validation data loaded: {len(val_dataset)} samples.\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure you have run the feature extraction script correctly and that all file paths are correct.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize Model, Loss, and Optimizer ---\n",
    "    cqcc_sample, prosody_sample, _ = train_dataset[0]\n",
    "    cqcc_dim = cqcc_sample.shape[0] \n",
    "    \n",
    "    # Handle prosody dimensions properly\n",
    "    if prosody_sample.dim() > 1:\n",
    "        prosody_dim = prosody_sample.numel()  # Total number of elements if multi-dimensional\n",
    "    else:\n",
    "        prosody_dim = prosody_sample.shape[0]\n",
    "\n",
    "    print(f\"CQCC feature dimension: {cqcc_dim}\")\n",
    "    print(f\"Prosody feature dimension: {prosody_dim}\")\n",
    "    print(f\"Prosody sample shape: {prosody_sample.shape}\")\n",
    "\n",
    "    # Choose BiLSTM model\n",
    "    if USE_ADVANCED_BILSTM:\n",
    "        print(\"Using Advanced BiLSTM model with multi-head attention\")\n",
    "        model = BiLSTMAttentionFusionModelV2(\n",
    "            cqcc_feature_dim=cqcc_dim,\n",
    "            prosody_feature_dim=prosody_dim,\n",
    "            hidden_dim=128,\n",
    "            num_layers=3\n",
    "        ).to(device)\n",
    "    else:\n",
    "        print(\"Using Standard BiLSTM model\")\n",
    "        model = BiLSTMAttentionFusionModel(\n",
    "            cqcc_feature_dim=cqcc_dim,\n",
    "            prosody_feature_dim=prosody_dim,\n",
    "            hidden_dim=128,\n",
    "            num_layers=2\n",
    "        ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # --- 3. Training Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_eer = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_eer, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val EER: {val_eer:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        if val_eer < best_val_eer:\n",
    "            best_val_eer = val_eer\n",
    "            torch.save(model.state_dict(), 'best_bilstm_model.pth')\n",
    "            print(f\"-> New best model saved with EER: {best_val_eer:.4f}\")\n",
    "            \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"Best validation EER achieved: {best_val_eer:.4f}\")\n",
    "\n",
    "    # --- 4. Testing Loop ---\n",
    "    print(\"\\n--- Starting Testing ---\")\n",
    "    if os.path.exists('best_bilstm_model.pth'):\n",
    "        try:\n",
    "            test_dataset = AudioSpoofDataset(TEST_CQCC_FILE, TEST_PROSODY_FILE, TEST_LABELS_FILE)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            print(f\"Test data loaded: {len(test_dataset)} samples.\")\n",
    "\n",
    "            print(\"Loading best model for testing...\")\n",
    "            model.load_state_dict(torch.load('best_bilstm_model.pth'))\n",
    "            \n",
    "            test_loss, test_eer, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            print(\"\\n--- Test Results ---\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Test EER: {test_eer:.4f}\")\n",
    "            print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        except (FileNotFoundError, ValueError, AssertionError) as e:\n",
    "            print(f\"Error loading test files: {e}\")\n",
    "            print(\"Please ensure your test feature files are generated and paths are correct.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'best_bilstm_model.pth' found to test. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19593105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "Training data tensor shape: torch.Size([46019, 1, 128, 157])\n",
      "Validation data tensor shape: torch.Size([24844, 1, 128, 157])\n",
      "Test data tensor shape: torch.Size([71237, 1, 128, 157])\n",
      "\n",
      "Model Architecture:\n",
      "CNN_2D(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=38912, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.25it/s]\n",
      "Epoch 1/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 523.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 0.0908 | Val Loss: 0.0930 | Val Accuracy: 96.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 197.14it/s]\n",
      "Epoch 2/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 522.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] | Train Loss: 0.0167 | Val Loss: 0.0544 | Val Accuracy: 98.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 200.01it/s]\n",
      "Epoch 3/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 521.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] | Train Loss: 0.0090 | Val Loss: 0.0742 | Val Accuracy: 97.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.84it/s]\n",
      "Epoch 4/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 522.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] | Train Loss: 0.0050 | Val Loss: 0.0826 | Val Accuracy: 98.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.91it/s]\n",
      "Epoch 5/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 522.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] | Train Loss: 0.0062 | Val Loss: 0.0587 | Val Accuracy: 98.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 191.15it/s]\n",
      "Epoch 6/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 524.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] | Train Loss: 0.0046 | Val Loss: 0.1053 | Val Accuracy: 97.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.77it/s]\n",
      "Epoch 7/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 528.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] | Train Loss: 0.0035 | Val Loss: 0.0622 | Val Accuracy: 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 200.00it/s]\n",
      "Epoch 8/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 527.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] | Train Loss: 0.0058 | Val Loss: 0.2615 | Val Accuracy: 96.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.84it/s]\n",
      "Epoch 9/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 529.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] | Train Loss: 0.0019 | Val Loss: 0.1237 | Val Accuracy: 97.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.72it/s]\n",
      "Epoch 10/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 528.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] | Train Loss: 0.0046 | Val Loss: 0.0894 | Val Accuracy: 98.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.73it/s]\n",
      "Epoch 11/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 527.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] | Train Loss: 0.0019 | Val Loss: 0.1997 | Val Accuracy: 97.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.65it/s]\n",
      "Epoch 12/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 527.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] | Train Loss: 0.0038 | Val Loss: 0.1657 | Val Accuracy: 97.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.56it/s]\n",
      "Epoch 13/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 525.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] | Train Loss: 0.0027 | Val Loss: 0.2266 | Val Accuracy: 97.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.68it/s]\n",
      "Epoch 14/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 528.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] | Train Loss: 0.0020 | Val Loss: 0.1167 | Val Accuracy: 98.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.69it/s]\n",
      "Epoch 15/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 524.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] | Train Loss: 0.0028 | Val Loss: 0.1741 | Val Accuracy: 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.52it/s]\n",
      "Epoch 16/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 529.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] | Train Loss: 0.0033 | Val Loss: 0.1217 | Val Accuracy: 98.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.65it/s]\n",
      "Epoch 17/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 521.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] | Train Loss: 0.0006 | Val Loss: 0.1881 | Val Accuracy: 97.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.39it/s]\n",
      "Epoch 18/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 522.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] | Train Loss: 0.0021 | Val Loss: 0.1278 | Val Accuracy: 98.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.11it/s]\n",
      "Epoch 19/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 520.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] | Train Loss: 0.0014 | Val Loss: 0.3071 | Val Accuracy: 96.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 184.98it/s]\n",
      "Epoch 20/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 523.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] | Train Loss: 0.0018 | Val Loss: 0.4044 | Val Accuracy: 96.45%\n",
      "\n",
      "Training Finished!\n",
      "\n",
      "Evaluating the model on the final test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 2227/2227 [00:04<00:00, 528.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "      Final Test Set Results\n",
      "==============================\n",
      "\n",
      "Accuracy: 94.40%\n",
      "EER (Equal Error Rate): 9.35%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Bona Fide (0)       0.98      0.96      0.97     63882\n",
      "    Spoof (1)       0.70      0.81      0.75      7355\n",
      "\n",
      "     accuracy                           0.94     71237\n",
      "    macro avg       0.84      0.88      0.86     71237\n",
      " weighted avg       0.95      0.94      0.95     71237\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_curve, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup and Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Helper Function for EER ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    # Find the absolute difference between fnr and fpr\n",
    "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer_threshold = thresholds[eer_index]\n",
    "    eer = fpr[eer_index]\n",
    "    return eer * 100 # Return as a percentage\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "print(\"Loading training data...\")\n",
    "X_train_np = np.load('processed_data_aligned_lld/cqcc_features_train.npy')\n",
    "y_train_np = np.load('processed_data_aligned_lld/labels_train.npy')\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "X_val_np = np.load('processed_data_aligned_lld/cqcc_features_dev.npy')\n",
    "y_val_np = np.load('processed_data_aligned_lld/labels_dev.npy')\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "X_test_np = np.load('processed_data_aligned_lld/cqcc_features_test.npy')\n",
    "y_test_np = np.load('processed_data_aligned_lld/labels_test.npy')\n",
    "\n",
    "\n",
    "# Add channel dimension for Conv2D\n",
    "X_train_np = np.expand_dims(X_train_np, axis=1)\n",
    "X_val_np = np.expand_dims(X_val_np, axis=1)\n",
    "X_test_np = np.expand_dims(X_test_np, axis=1)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.from_numpy(X_train_np).float()\n",
    "y_train = torch.from_numpy(y_train_np).float().view(-1, 1)\n",
    "X_val = torch.from_numpy(X_val_np).float()\n",
    "y_val = torch.from_numpy(y_val_np).float().view(-1, 1)\n",
    "X_test = torch.from_numpy(X_test_np).float()\n",
    "y_test = torch.from_numpy(y_test_np).float().view(-1, 1)\n",
    "\n",
    "print(f\"Training data tensor shape: {X_train.shape}\")\n",
    "print(f\"Validation data tensor shape: {X_val.shape}\")\n",
    "print(f\"Test data tensor shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 3. Define the Dynamic 2D-CNN Model ---\n",
    "class CNN_2D(nn.Module):\n",
    "    def __init__(self, input_height, input_width):\n",
    "        super(CNN_2D, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, input_height, input_width)\n",
    "            dummy_output = self.feature_extractor(dummy_input)\n",
    "            flattened_size = dummy_output.flatten(1).shape[1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "input_height = X_train.shape[2]\n",
    "input_width = X_train.shape[3]\n",
    "model = CNN_2D(input_height=input_height, input_width=input_width).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# --- 4. Define Loss Function and Optimizer ---\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- 5. Training and Validation Loop ---\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    # Wrap train_loader with tqdm\n",
    "    for data, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\"):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        # Wrap val_loader with tqdm\n",
    "        for data, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_val_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += (predicted == targets).sum().item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = (correct_val / total_val) * 100\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "\n",
    "# --- 6. Final Evaluation on the Test Set ---\n",
    "print(\"\\nEvaluating the model on the final test set...\")\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    # Wrap test_loader with tqdm\n",
    "    for data, targets in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Get raw scores for EER\n",
    "        outputs = model(data)\n",
    "        scores = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Get predictions for classification report\n",
    "        predicted = scores > 0.5\n",
    "        \n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "        all_scores.extend(scores.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_scores = np.array(all_scores)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Calculate metrics\n",
    "eer_score = calculate_eer(all_labels, all_scores)\n",
    "report = classification_report(all_labels, all_preds, target_names=['Bona Fide (0)', 'Spoof (1)'])\n",
    "accuracy = (all_preds == all_labels).mean() * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"      Final Test Set Results\")\n",
    "print(\"=\"*30)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "print(f\"EER (Equal Error Rate): {eer_score:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a32110b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "Training data tensor shape: torch.Size([46019, 1, 128, 157])\n",
      "Validation data tensor shape: torch.Size([24844, 1, 128, 157])\n",
      "Test data tensor shape: torch.Size([71237, 1, 128, 157])\n",
      "\n",
      "Model Architecture:\n",
      "CNN_2D(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=38912, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.35it/s]\n",
      "Epoch 1/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 521.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 0.0796 | Val Loss: 0.0569 | Val Acc: 97.94% | Val F1: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 186.45it/s]\n",
      "Epoch 2/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 521.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] | Train Loss: 0.0143 | Val Loss: 0.0453 | Val Acc: 98.32% | Val F1: 0.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.58it/s]\n",
      "Epoch 3/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 522.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] | Train Loss: 0.0086 | Val Loss: 0.0598 | Val Acc: 98.17% | Val F1: 0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.48it/s]\n",
      "Epoch 4/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 519.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] | Train Loss: 0.0055 | Val Loss: 0.0872 | Val Acc: 98.06% | Val F1: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.18it/s]\n",
      "Epoch 5/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 520.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] | Train Loss: 0.0046 | Val Loss: 0.1309 | Val Acc: 97.73% | Val F1: 0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.72it/s]\n",
      "Epoch 6/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 523.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] | Train Loss: 0.0064 | Val Loss: 0.1187 | Val Acc: 98.01% | Val F1: 0.8935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.48it/s]\n",
      "Epoch 7/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 524.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] | Train Loss: 0.0035 | Val Loss: 0.0478 | Val Acc: 98.62% | Val F1: 0.9324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.59it/s]\n",
      "Epoch 8/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 517.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] | Train Loss: 0.0030 | Val Loss: 0.1144 | Val Acc: 98.16% | Val F1: 0.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.36it/s]\n",
      "Epoch 9/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 519.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] | Train Loss: 0.0041 | Val Loss: 0.1896 | Val Acc: 97.33% | Val F1: 0.8507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.07it/s]\n",
      "Epoch 10/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 518.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] | Train Loss: 0.0026 | Val Loss: 0.1158 | Val Acc: 98.27% | Val F1: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.01it/s]\n",
      "Epoch 11/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 519.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] | Train Loss: 0.0022 | Val Loss: 0.3549 | Val Acc: 96.68% | Val F1: 0.8067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.32it/s]\n",
      "Epoch 12/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 517.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] | Train Loss: 0.0036 | Val Loss: 0.1159 | Val Acc: 98.10% | Val F1: 0.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.24it/s]\n",
      "Epoch 13/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 527.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] | Train Loss: 0.0008 | Val Loss: 0.2221 | Val Acc: 97.27% | Val F1: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 198.88it/s]\n",
      "Epoch 14/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 517.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] | Train Loss: 0.0035 | Val Loss: 0.1377 | Val Acc: 97.71% | Val F1: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.03it/s]\n",
      "Epoch 15/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 519.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] | Train Loss: 0.0023 | Val Loss: 0.4571 | Val Acc: 95.11% | Val F1: 0.6871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.03it/s]\n",
      "Epoch 16/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 518.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] | Train Loss: 0.0025 | Val Loss: 0.2509 | Val Acc: 97.06% | Val F1: 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.30it/s]\n",
      "Epoch 17/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 523.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] | Train Loss: 0.0029 | Val Loss: 0.0907 | Val Acc: 98.60% | Val F1: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 199.13it/s]\n",
      "Epoch 18/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 519.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] | Train Loss: 0.0019 | Val Loss: 0.1879 | Val Acc: 97.74% | Val F1: 0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 198.87it/s]\n",
      "Epoch 19/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 520.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] | Train Loss: 0.0019 | Val Loss: 0.3318 | Val Acc: 96.57% | Val F1: 0.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Training]: 100%|██████████| 1439/1439 [00:07<00:00, 185.09it/s]\n",
      "Epoch 20/20 [Validation]: 100%|██████████| 777/777 [00:01<00:00, 521.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] | Train Loss: 0.0022 | Val Loss: 0.1723 | Val Acc: 97.85% | Val F1: 0.8832\n",
      "\n",
      "Training Finished!\n",
      "\n",
      "Evaluating the model on the final test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 2227/2227 [00:04<00:00, 526.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "      Final Test Set Results\n",
      "==============================\n",
      "\n",
      "Accuracy: 93.17%\n",
      "EER (Equal Error Rate): 8.44%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Bona Fide (0)       0.99      0.94      0.96     63882\n",
      "    Spoof (1)       0.62      0.89      0.73      7355\n",
      "\n",
      "     accuracy                           0.93     71237\n",
      "    macro avg       0.80      0.91      0.84     71237\n",
      " weighted avg       0.95      0.93      0.94     71237\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_curve, classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup and Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Helper Function for EER ---\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    # Find the absolute difference between fnr and fpr\n",
    "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer_threshold = thresholds[eer_index]\n",
    "    eer = fpr[eer_index]\n",
    "    return eer * 100 # Return as a percentage\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "print(\"Loading training data...\")\n",
    "X_train_np = np.load('processed_data_aligned_lld/cqcc_features_train.npy')\n",
    "y_train_np = np.load('processed_data_aligned_lld/labels_train.npy')\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "X_val_np = np.load('processed_data_aligned_lld/cqcc_features_dev.npy')\n",
    "y_val_np = np.load('processed_data_aligned_lld/labels_dev.npy')\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "X_test_np = np.load('processed_data_aligned_lld/cqcc_features_test.npy')\n",
    "y_test_np = np.load('processed_data_aligned_lld/labels_test.npy')\n",
    "\n",
    "\n",
    "# Add channel dimension for Conv2D\n",
    "X_train_np = np.expand_dims(X_train_np, axis=1)\n",
    "X_val_np = np.expand_dims(X_val_np, axis=1)\n",
    "X_test_np = np.expand_dims(X_test_np, axis=1)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.from_numpy(X_train_np).float()\n",
    "y_train = torch.from_numpy(y_train_np).float().view(-1, 1)\n",
    "X_val = torch.from_numpy(X_val_np).float()\n",
    "y_val = torch.from_numpy(y_val_np).float().view(-1, 1)\n",
    "X_test = torch.from_numpy(X_test_np).float()\n",
    "y_test = torch.from_numpy(y_test_np).float().view(-1, 1)\n",
    "\n",
    "print(f\"Training data tensor shape: {X_train.shape}\")\n",
    "print(f\"Validation data tensor shape: {X_val.shape}\")\n",
    "print(f\"Test data tensor shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 3. Define the Dynamic 2D-CNN Model ---\n",
    "class CNN_2D(nn.Module):\n",
    "    def __init__(self, input_height, input_width):\n",
    "        super(CNN_2D, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, input_height, input_width)\n",
    "            dummy_output = self.feature_extractor(dummy_input)\n",
    "            flattened_size = dummy_output.flatten(1).shape[1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "input_height = X_train.shape[2]\n",
    "input_width = X_train.shape[3]\n",
    "model = CNN_2D(input_height=input_height, input_width=input_width).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# --- 4. Define Loss Function and Optimizer ---\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- 5. Training and Validation Loop ---\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    # Wrap train_loader with tqdm\n",
    "    for data, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\"):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "    with torch.no_grad():\n",
    "        # Wrap val_loader with tqdm\n",
    "        for data, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            all_val_labels.extend(targets.cpu().numpy())\n",
    "            all_val_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = (np.array(all_val_preds) == np.array(all_val_labels)).mean() * 100\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "\n",
    "# --- 6. Final Evaluation on the Test Set ---\n",
    "print(\"\\nEvaluating the model on the final test set...\")\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    # Wrap test_loader with tqdm\n",
    "    for data, targets in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Get raw scores for EER\n",
    "        outputs = model(data)\n",
    "        scores = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Get predictions for classification report\n",
    "        predicted = scores > 0.5\n",
    "        \n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "        all_scores.extend(scores.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_scores = np.array(all_scores)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Calculate metrics\n",
    "eer_score = calculate_eer(all_labels, all_scores)\n",
    "report = classification_report(all_labels, all_preds, target_names=['Bona Fide (0)', 'Spoof (1)'])\n",
    "accuracy = (all_preds == all_labels).mean() * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"      Final Test Set Results\")\n",
    "print(\"=\"*30)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "print(f\"EER (Equal Error Rate): {eer_score:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30d3c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading and Preparing Data ---\n",
      "Converting 3D LLD prosodic features to 2D summary statistics (mean)...\n",
      "--- Scaling Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAttentionFusionModel(\n",
      "  (cqcc_projection): Linear(in_features=157, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prosody_mlp): Sequential(\n",
      "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 720/720 [00:09<00:00, 78.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.3237 | Val Loss: 0.2048 | Val Acc: 88.92% | F1: 0.5934 | EER: 12.72%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 720/720 [00:10<00:00, 68.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.1707 | Val Loss: 0.1676 | Val Acc: 91.87% | F1: 0.6395 | EER: 12.02%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 720/720 [00:11<00:00, 61.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.1379 | Val Loss: 0.1472 | Val Acc: 93.25% | F1: 0.6425 | EER: 10.79%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 720/720 [00:10<00:00, 67.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.1151 | Val Loss: 0.1628 | Val Acc: 93.27% | F1: 0.6953 | EER: 10.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 720/720 [00:11<00:00, 64.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.0971 | Val Loss: 0.1350 | Val Acc: 94.47% | F1: 0.7225 | EER: 10.06%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 720/720 [00:11<00:00, 64.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0824 | Val Loss: 0.1474 | Val Acc: 94.67% | F1: 0.7077 | EER: 9.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 720/720 [00:10<00:00, 70.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0736 | Val Loss: 0.1640 | Val Acc: 94.36% | F1: 0.6724 | EER: 9.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 720/720 [00:08<00:00, 80.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0658 | Val Loss: 0.1484 | Val Acc: 94.76% | F1: 0.7003 | EER: 9.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 720/720 [00:08<00:00, 81.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0611 | Val Loss: 0.1574 | Val Acc: 95.22% | F1: 0.7331 | EER: 9.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 720/720 [00:08<00:00, 81.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0540 | Val Loss: 0.1772 | Val Acc: 94.73% | F1: 0.6871 | EER: 9.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 720/720 [00:08<00:00, 87.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0464 | Val Loss: 0.2107 | Val Acc: 94.59% | F1: 0.6733 | EER: 9.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 720/720 [00:08<00:00, 87.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0346 | Val Loss: 0.1801 | Val Acc: 95.26% | F1: 0.7350 | EER: 9.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 720/720 [00:05<00:00, 138.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0309 | Val Loss: 0.2129 | Val Acc: 95.00% | F1: 0.7037 | EER: 9.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 720/720 [00:07<00:00, 102.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0315 | Val Loss: 0.2098 | Val Acc: 95.01% | F1: 0.7048 | EER: 8.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|██████████| 720/720 [00:09<00:00, 79.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0291 | Val Loss: 0.2396 | Val Acc: 94.76% | F1: 0.6769 | EER: 9.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|██████████| 720/720 [00:08<00:00, 80.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0283 | Val Loss: 0.2357 | Val Acc: 94.85% | F1: 0.6835 | EER: 8.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|██████████| 720/720 [00:08<00:00, 83.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0266 | Val Loss: 0.2133 | Val Acc: 95.14% | F1: 0.7145 | EER: 8.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|██████████| 720/720 [00:07<00:00, 102.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0247 | Val Loss: 0.2171 | Val Acc: 95.11% | F1: 0.7099 | EER: 8.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|██████████| 720/720 [00:07<00:00, 90.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0235 | Val Loss: 0.2084 | Val Acc: 95.19% | F1: 0.7189 | EER: 8.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|██████████| 720/720 [00:05<00:00, 131.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0236 | Val Loss: 0.2227 | Val Acc: 95.13% | F1: 0.7109 | EER: 8.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|██████████| 720/720 [00:09<00:00, 73.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0223 | Val Loss: 0.2408 | Val Acc: 94.94% | F1: 0.6928 | EER: 8.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|██████████| 720/720 [00:08<00:00, 88.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0236 | Val Loss: 0.2189 | Val Acc: 95.13% | F1: 0.7137 | EER: 8.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|██████████| 720/720 [00:08<00:00, 86.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0218 | Val Loss: 0.2309 | Val Acc: 95.07% | F1: 0.7050 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|██████████| 720/720 [00:08<00:00, 87.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0229 | Val Loss: 0.2288 | Val Acc: 95.05% | F1: 0.7043 | EER: 8.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|██████████| 720/720 [00:05<00:00, 139.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0228 | Val Loss: 0.2239 | Val Acc: 95.14% | F1: 0.7124 | EER: 8.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|██████████| 720/720 [00:05<00:00, 142.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0219 | Val Loss: 0.2284 | Val Acc: 95.08% | F1: 0.7064 | EER: 8.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|██████████| 720/720 [00:05<00:00, 138.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0216 | Val Loss: 0.2293 | Val Acc: 95.09% | F1: 0.7066 | EER: 8.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|██████████| 720/720 [00:05<00:00, 138.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0226 | Val Loss: 0.2232 | Val Acc: 95.13% | F1: 0.7122 | EER: 8.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|██████████| 720/720 [00:07<00:00, 100.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0216 | Val Loss: 0.2251 | Val Acc: 95.12% | F1: 0.7107 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|██████████| 720/720 [00:10<00:00, 70.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0211 | Val Loss: 0.2273 | Val Acc: 95.11% | F1: 0.7091 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|██████████| 720/720 [00:09<00:00, 75.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0230 | Val Loss: 0.2279 | Val Acc: 95.09% | F1: 0.7077 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|██████████| 720/720 [00:08<00:00, 81.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0223 | Val Loss: 0.2289 | Val Acc: 95.09% | F1: 0.7072 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|██████████| 720/720 [00:08<00:00, 80.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0200 | Val Loss: 0.2274 | Val Acc: 95.10% | F1: 0.7082 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|██████████| 720/720 [00:08<00:00, 83.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0217 | Val Loss: 0.2276 | Val Acc: 95.10% | F1: 0.7084 | EER: 8.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|██████████| 720/720 [00:08<00:00, 86.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0207 | Val Loss: 0.2285 | Val Acc: 95.09% | F1: 0.7070 | EER: 8.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|██████████| 720/720 [00:08<00:00, 81.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0217 | Val Loss: 0.2283 | Val Acc: 95.08% | F1: 0.7066 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|██████████| 720/720 [00:07<00:00, 93.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0213 | Val Loss: 0.2286 | Val Acc: 95.09% | F1: 0.7070 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|██████████| 720/720 [00:08<00:00, 87.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0216 | Val Loss: 0.2284 | Val Acc: 95.08% | F1: 0.7068 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|██████████| 720/720 [00:05<00:00, 139.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0219 | Val Loss: 0.2284 | Val Acc: 95.08% | F1: 0.7068 | EER: 8.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|██████████| 720/720 [00:07<00:00, 90.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0212 | Val Loss: 0.2285 | Val Acc: 95.08% | F1: 0.7068 | EER: 8.83%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_transformer_23feat.png\n",
      "\n",
      "--- Starting Final Testing and Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Final Testing: 100%|██████████| 1114/1114 [00:02<00:00, 433.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Results ---\n",
      "Accuracy: 86.87% | F1-Score: 0.5660 | EER: 14.14%\n",
      "Confusion Matrix:\n",
      " [[55783  8099]\n",
      " [ 1255  6100]]\n",
      "\n",
      "--- Running Cross-Attention Weight Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Attention: 100%|██████████| 1114/1114 [00:03<00:00, 292.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention plot saved to saved_models/attention_importance_transformer_23feat.png\n",
      "\n",
      "--- Running Feature Ablation Analysis ---\n",
      "Baseline EER with all features: 14.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Ablation: 100%|██████████| 23/23 [01:00<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance based on EER Increase:\n",
      "- slope500-1500_sma3: EER increases by 0.88%\n",
      "- spectralFlux_sma3: EER increases by 0.58%\n",
      "- mfcc1_sma3: EER increases by 0.49%\n",
      "- hammarbergIndex_sma3: EER increases by 0.45%\n",
      "- slope0-500_sma3: EER increases by 0.38%\n",
      "- F3frequency_sma3nz: EER increases by 0.37%\n",
      "- F1amplitudeLogRelF0_sma3nz: EER increases by 0.33%\n",
      "- HNRdBACF_sma3nz: EER increases by 0.27%\n",
      "- F1frequency_sma3nz: EER increases by 0.24%\n",
      "- F0semitoneFrom27.5Hz_sma3nz: EER increases by 0.23%\n",
      "- F2amplitudeLogRelF0_sma3nz: EER increases by 0.19%\n",
      "- logRelF0-H1-A3_sma3nz: EER increases by 0.18%\n",
      "- F3amplitudeLogRelF0_sma3nz: EER increases by 0.15%\n",
      "- shimmerLocaldB_sma3nz: EER increases by 0.14%\n",
      "- Loudness_sma3: EER increases by 0.13%\n",
      "- mfcc3_sma3: EER increases by 0.12%\n",
      "- mfcc4_sma3: EER increases by 0.06%\n",
      "- jitterLocal_sma3nz: EER increases by 0.02%\n",
      "- F2frequency_sma3nz: EER increases by 0.00%\n",
      "- logRelF0-H1-H2_sma3nz: EER increases by -0.03%\n",
      "- mfcc2_sma3: EER increases by -0.11%\n",
      "- F1bandwidth_sma3nz: EER increases by -0.21%\n",
      "- alphaRatio_sma3: EER increases by -0.22%\n",
      "\n",
      "Ablation plot saved to saved_models/ablation_importance_transformer_23feat.png\n",
      "\n",
      "--- Running SHAP Analysis ---\n",
      "Calculating SHAP values (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c7ff758ad74f8ead8121b40529c37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting SHAP summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "The figure layout has changed to tight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP plot saved to saved_models/shap_importance_transformer_23feat.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/TransformerFusion_PyTorch_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_transformer_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_transformer_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_transformer_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_transformer_23feat.png\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- CORRECTED: Transformer + Cross-Attention Fusion Model ---\n",
    "class TransformerAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, nhead=4, num_encoder_layers=3, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerAttentionFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # This layer projects the input CQCC features into the model's dimension (d_model)\n",
    "        self.cqcc_projection = nn.Linear(cqcc_feature_dim, d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # TransformerEncoderLayer expects input as (N, S, E) with batch_first=True\n",
    "        # N=batch_size, S=sequence_length, E=embedding_dim (d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # The classifier takes the concatenated features from attention context and prosody query\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # The input cqcc_x has shape (batch_size, seq_len, feature_dim)\n",
    "        # The projection layer expects the feature_dim to be the last dimension, which is correct.\n",
    "        # --- FIX: The incorrect permute operation has been removed. ---\n",
    "        # INCORRECT LINE: cqcc_x = cqcc_x.permute(0, 2, 1) \n",
    "        \n",
    "        # 1. Project CQCC features to the model dimension\n",
    "        # Input: (batch_size, seq_len, cqcc_feature_dim) -> Output: (batch_size, seq_len, d_model)\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x) * math.sqrt(self.d_model)\n",
    "\n",
    "        # 2. Add positional encoding.\n",
    "        # The PositionalEncoding module expects (seq_len, batch_size, d_model).\n",
    "        # So we permute the dimensions from (N, S, E) to (S, N, E).\n",
    "        cqcc_embed_permuted = cqcc_embed.permute(1, 0, 2)\n",
    "        cqcc_pos = self.pos_encoder(cqcc_embed_permuted)\n",
    "\n",
    "        # 3. Pass through the Transformer Encoder.\n",
    "        # The TransformerEncoder (with batch_first=True) expects (batch_size, seq_len, d_model).\n",
    "        # So we permute back from (S, N, E) to (N, S, E).\n",
    "        transformer_input = cqcc_pos.permute(1, 0, 2)\n",
    "        transformer_out = self.transformer_encoder(transformer_input)\n",
    "        \n",
    "        # 4. Process prosody features to create a query vector\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "\n",
    "        # 5. Perform cross-attention\n",
    "        # Query: prosody_query (batch_size, d_model)\n",
    "        # Keys/Values: transformer_out (batch_size, seq_len, d_model)\n",
    "        keys = values = transformer_out\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1) # Shape: (batch_size, 1, d_model)\n",
    "        \n",
    "        # Calculate attention scores: Q * K^T\n",
    "        # (B, 1, D) @ (B, D, S) -> (B, 1, S)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2)) / (keys.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Calculate context vector: AttentionWeights * V\n",
    "        # (B, 1, S) @ (B, S, D) -> (B, 1, D)\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1) # Shape: (batch_size, d_model)\n",
    "        \n",
    "        # 6. Fuse features and classify\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, device, save_path):\n",
    "    print(\"\\n--- Running Cross-Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            all_weights.append(weights.squeeze(1).cpu().numpy())\n",
    "    \n",
    "    avg_weights = np.mean(np.concatenate(all_weights, axis=0), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(avg_weights, color='purple')\n",
    "    plt.xlabel('CQCC Time Frame')\n",
    "    plt.ylabel('Average Attention Weight')\n",
    "    plt.title('Cross-Attention: Importance of Acoustic Time Frames Guided by Prosody')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAttention plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody[:, feature_to_ablate] = 0.0\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "    # Use a larger background set for more stable SHAP values if possible\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        \n",
    "        # Repeat the first CQCC sample from the background set for each prosody sample\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1) # Adjusted repeat for 3D tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(cqcc_tensor, prosody_tensor)\n",
    "            output = torch.sigmoid(logits)\n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    \n",
    "    # Use a dedicated figure for shap plot to avoid conflicts\n",
    "    plt.figure() \n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        X_cqcc_train = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "        X_cqcc_val = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        X_prosody_train = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Loudness_sma3','alphaRatio_sma3','hammarbergIndex_sma3','slope0-500_sma3',\n",
    "            'slope500-1500_sma3','spectralFlux_sma3','mfcc1_sma3','mfcc2_sma3',\n",
    "            'mfcc3_sma3','mfcc4_sma3','F0semitoneFrom27.5Hz_sma3nz','jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz','HNRdBACF_sma3nz','logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz','F1frequency_sma3nz','F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz','F2frequency_sma3nz','F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz','F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        num_prosodic_features = X_prosody_train.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            print(f\"Warning: Number of feature names ({len(feature_columns)}) does not match number of features ({num_prosodic_features}). Using generic names.\")\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"--- Scaling Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val)\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    ns, nx, ny = X_cqcc_train.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train.reshape(ns, -1)).reshape(ns, nx, ny)\n",
    "    nsv, nxv, nyv = X_cqcc_val.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val.reshape(nsv, -1)).reshape(nsv, nxv, nyv)\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = TransformerAttentionFusionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train.shape[2], # feature_dim is the last dimension\n",
    "        prosody_feature_dim=X_prosody_train.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        history.update({'train_loss': history['train_loss']+[avg_train_loss], 'val_loss': history['val_loss']+[avg_val_loss], 'val_acc': history['val_acc']+[val_accuracy], 'f1': history['f1']+[f1], 'eer': history['eer']+[eer]})\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH)\n",
    "\n",
    "    # --- FINAL TESTING AND ANALYSIS ---\n",
    "    print(\"\\n--- Starting Final Testing and Analysis ---\")\n",
    "    try:\n",
    "        X_cqcc_test = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        X_prosody_test = np.mean(X_prosody_test_3d, axis=2)\n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test)\n",
    "        ns_test, nx_test, ny_test = X_cqcc_test.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test.reshape(ns_test, -1)).reshape(ns_test, nx_test, ny_test)\n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        analysis_model = TransformerAttentionFusionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train.shape[2],\n",
    "            prosody_feature_dim=X_prosody_train.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        all_test_labels, all_test_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in tqdm(test_loader, desc=\"Final Testing\"):\n",
    "                cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "                logits, _ = analysis_model(cqcc, prosody)\n",
    "                all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "        all_test_preds = (all_test_scores > 0.5).astype(int)\n",
    "        test_accuracy = 100 * np.sum(all_test_preds == all_test_labels) / len(all_test_labels)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        test_cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "        print(\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Accuracy: {test_accuracy:.2f}% | F1-Score: {test_f1:.4f} | EER: {test_eer:.2f}%\")\n",
    "        print(\"Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "        analyze_attention_weights(analysis_model, test_loader, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eff0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading and Preparing Data ---\n",
      "Converting 3D LLD prosodic features to 2D summary statistics (mean)...\n",
      "\n",
      "--- Scaling Full Feature Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAttentionFusionModel(\n",
      "  (cqcc_projection): Linear(in_features=157, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prosody_mlp): Sequential(\n",
      "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 100.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.3354 | Val Loss: 0.2303 | Val Acc: 87.44% | F1: 0.5833 | EER: 12.84%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 80.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.1853 | Val Loss: 0.2121 | Val Acc: 89.45% | F1: 0.6206 | EER: 11.85%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (All Feats): 100%|██████████| 720/720 [00:06<00:00, 110.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.1477 | Val Loss: 0.1645 | Val Acc: 92.51% | F1: 0.6684 | EER: 11.19%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (All Feats): 100%|██████████| 720/720 [00:09<00:00, 79.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.1221 | Val Loss: 0.1444 | Val Acc: 93.48% | F1: 0.7022 | EER: 9.62%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (All Feats): 100%|██████████| 720/720 [00:06<00:00, 116.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.1069 | Val Loss: 0.1337 | Val Acc: 94.54% | F1: 0.7138 | EER: 9.85%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 94.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0917 | Val Loss: 0.1331 | Val Acc: 94.80% | F1: 0.7228 | EER: 9.35%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 86.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0779 | Val Loss: 0.1312 | Val Acc: 94.52% | F1: 0.7434 | EER: 8.83%\n",
      "   -> Val loss decreased. New best model saved to saved_models/TransformerFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 102.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0675 | Val Loss: 0.1419 | Val Acc: 95.25% | F1: 0.7390 | EER: 8.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 88.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0608 | Val Loss: 0.1544 | Val Acc: 95.20% | F1: 0.7323 | EER: 9.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 89.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0544 | Val Loss: 0.1459 | Val Acc: 95.27% | F1: 0.7373 | EER: 8.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 95.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0515 | Val Loss: 0.2168 | Val Acc: 94.65% | F1: 0.6612 | EER: 8.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 86.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0482 | Val Loss: 0.1741 | Val Acc: 95.18% | F1: 0.7235 | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (All Feats): 100%|██████████| 720/720 [00:05<00:00, 139.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0394 | Val Loss: 0.1497 | Val Acc: 95.66% | F1: 0.7639 | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 82.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0280 | Val Loss: 0.1837 | Val Acc: 95.43% | F1: 0.7385 | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 90.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0263 | Val Loss: 0.1898 | Val Acc: 95.45% | F1: 0.7410 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (All Feats): 100%|██████████| 720/720 [00:06<00:00, 111.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0256 | Val Loss: 0.2245 | Val Acc: 95.19% | F1: 0.7099 | EER: 8.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 85.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0244 | Val Loss: 0.1995 | Val Acc: 95.48% | F1: 0.7427 | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 90.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0222 | Val Loss: 0.2146 | Val Acc: 95.30% | F1: 0.7270 | EER: 8.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (All Feats): 100%|██████████| 720/720 [00:06<00:00, 106.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0218 | Val Loss: 0.2189 | Val Acc: 95.32% | F1: 0.7244 | EER: 8.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 81.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0194 | Val Loss: 0.2075 | Val Acc: 95.54% | F1: 0.7424 | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 70.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0194 | Val Loss: 0.2128 | Val Acc: 95.49% | F1: 0.7387 | EER: 8.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 99.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0196 | Val Loss: 0.2250 | Val Acc: 95.38% | F1: 0.7284 | EER: 8.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 98.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0193 | Val Loss: 0.2252 | Val Acc: 95.36% | F1: 0.7281 | EER: 8.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 82.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0197 | Val Loss: 0.2152 | Val Acc: 95.54% | F1: 0.7426 | EER: 8.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 91.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0176 | Val Loss: 0.2183 | Val Acc: 95.47% | F1: 0.7384 | EER: 8.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 83.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0171 | Val Loss: 0.2171 | Val Acc: 95.49% | F1: 0.7400 | EER: 8.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0178 | Val Loss: 0.2221 | Val Acc: 95.46% | F1: 0.7369 | EER: 8.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (All Feats): 100%|██████████| 720/720 [00:09<00:00, 72.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0179 | Val Loss: 0.2202 | Val Acc: 95.47% | F1: 0.7378 | EER: 8.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (All Feats): 100%|██████████| 720/720 [00:09<00:00, 76.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0161 | Val Loss: 0.2186 | Val Acc: 95.48% | F1: 0.7394 | EER: 8.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (All Feats): 100%|██████████| 720/720 [00:09<00:00, 75.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0167 | Val Loss: 0.2172 | Val Acc: 95.51% | F1: 0.7416 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 71.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0176 | Val Loss: 0.2195 | Val Acc: 95.48% | F1: 0.7399 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0173 | Val Loss: 0.2217 | Val Acc: 95.46% | F1: 0.7371 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 80.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0177 | Val Loss: 0.2215 | Val Acc: 95.46% | F1: 0.7374 | EER: 8.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 87.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0178 | Val Loss: 0.2232 | Val Acc: 95.46% | F1: 0.7371 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 86.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0171 | Val Loss: 0.2225 | Val Acc: 95.46% | F1: 0.7371 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 85.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0172 | Val Loss: 0.2217 | Val Acc: 95.48% | F1: 0.7385 | EER: 8.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 99.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0174 | Val Loss: 0.2236 | Val Acc: 95.46% | F1: 0.7370 | EER: 8.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (All Feats): 100%|██████████| 720/720 [00:07<00:00, 91.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0184 | Val Loss: 0.2238 | Val Acc: 95.45% | F1: 0.7361 | EER: 8.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 85.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0165 | Val Loss: 0.2236 | Val Acc: 95.46% | F1: 0.7368 | EER: 8.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (All Feats): 100%|██████████| 720/720 [00:08<00:00, 84.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0169 | Val Loss: 0.2237 | Val Acc: 95.46% | F1: 0.7370 | EER: 8.28%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_transformer_23feat.png\n",
      "\n",
      "--- Starting Final Testing and Analysis (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Final Testing (All Feats): 100%|██████████| 1114/1114 [00:02<00:00, 438.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Results (All Features) --- | EER: 14.10%\n",
      "\n",
      "--- Running Cross-Attention Weight Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Attention: 100%|██████████| 1114/1114 [00:02<00:00, 444.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention plot saved to saved_models/attention_importance_transformer_23feat.png\n",
      "\n",
      "--- Running Feature Ablation Analysis ---\n",
      "Baseline EER with all features: 14.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Ablation: 100%|██████████| 23/23 [01:03<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance based on EER Increase:\n",
      "- slope500-1500_sma3: EER increases by 0.79%\n",
      "- F3frequency_sma3nz: EER increases by 0.53%\n",
      "- F1frequency_sma3nz: EER increases by 0.53%\n",
      "- mfcc1_sma3: EER increases by 0.42%\n",
      "- slope0-500_sma3: EER increases by 0.42%\n",
      "- HNRdBACF_sma3nz: EER increases by 0.40%\n",
      "- F0semitoneFrom27.5Hz_sma3nz: EER increases by 0.40%\n",
      "- spectralFlux_sma3: EER increases by 0.33%\n",
      "- Loudness_sma3: EER increases by 0.24%\n",
      "- logRelF0-H1-H2_sma3nz: EER increases by 0.22%\n",
      "- F2amplitudeLogRelF0_sma3nz: EER increases by 0.22%\n",
      "- F3amplitudeLogRelF0_sma3nz: EER increases by 0.20%\n",
      "- F2frequency_sma3nz: EER increases by 0.16%\n",
      "- mfcc4_sma3: EER increases by 0.16%\n",
      "- F1amplitudeLogRelF0_sma3nz: EER increases by 0.16%\n",
      "- mfcc3_sma3: EER increases by 0.14%\n",
      "- mfcc2_sma3: EER increases by 0.11%\n",
      "- jitterLocal_sma3nz: EER increases by 0.09%\n",
      "- shimmerLocaldB_sma3nz: EER increases by 0.08%\n",
      "- hammarbergIndex_sma3: EER increases by 0.03%\n",
      "- alphaRatio_sma3: EER increases by -0.00%\n",
      "- logRelF0-H1-A3_sma3nz: EER increases by -0.03%\n",
      "- F1bandwidth_sma3nz: EER increases by -0.16%\n",
      "\n",
      "Ablation plot saved to saved_models/ablation_importance_transformer_23feat.png\n",
      "\n",
      "--- Running SHAP Analysis ---\n",
      "Calculating SHAP values (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b02b431bfb4fddb281d0969d797f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting SHAP summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "The figure layout has changed to tight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP plot saved to saved_models/shap_importance_transformer_23feat.png\n",
      "\n",
      "\n",
      "--- Starting Retraining with Top 6 Features ---\n",
      "Top 6 features selected for retraining: ['slope500-1500_sma3', 'F3frequency_sma3nz', 'F1frequency_sma3nz', 'mfcc1_sma3', 'slope0-500_sma3', 'HNRdBACF_sma3nz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAttentionFusionModel(\n",
      "  (cqcc_projection): Linear(in_features=157, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prosody_mlp): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (Top 6 Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (Top 6): 100%|██████████| 720/720 [00:08<00:00, 82.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.3394 | Val Loss: 0.2290 | EER: 12.37%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (Top 6): 100%|██████████| 720/720 [00:09<00:00, 76.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.1836 | Val Loss: 0.2199 | EER: 11.07%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (Top 6): 100%|██████████| 720/720 [00:07<00:00, 96.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.1510 | Val Loss: 0.1721 | EER: 10.60%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (Top 6): 100%|██████████| 720/720 [00:08<00:00, 80.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.1280 | Val Loss: 0.1525 | EER: 10.28%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (Top 6): 100%|██████████| 720/720 [00:08<00:00, 83.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.1143 | Val Loss: 0.1463 | EER: 10.68%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (Top 6): 100%|██████████| 720/720 [00:07<00:00, 95.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.1001 | Val Loss: 0.1407 | EER: 9.80%\n",
      "   -> Val loss decreased. New best model (Top 6) saved to saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (Top 6): 100%|██████████| 720/720 [00:09<00:00, 72.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0912 | Val Loss: 0.1450 | EER: 9.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (Top 6): 100%|██████████| 720/720 [00:09<00:00, 74.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0818 | Val Loss: 0.1747 | EER: 11.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (Top 6): 100%|██████████| 720/720 [00:10<00:00, 71.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0740 | Val Loss: 0.1643 | EER: 9.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (Top 6): 100%|██████████| 720/720 [00:09<00:00, 74.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0672 | Val Loss: 0.1775 | EER: 9.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0587 | Val Loss: 0.1609 | EER: 9.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0547 | Val Loss: 0.1783 | EER: 9.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 110.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0414 | Val Loss: 0.1908 | EER: 9.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0362 | Val Loss: 0.2059 | EER: 9.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0342 | Val Loss: 0.2038 | EER: 9.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0324 | Val Loss: 0.2145 | EER: 9.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0333 | Val Loss: 0.2076 | EER: 9.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (Top 6): 100%|██████████| 720/720 [00:08<00:00, 87.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0299 | Val Loss: 0.2083 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0277 | Val Loss: 0.2114 | EER: 9.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (Top 6): 100%|██████████| 720/720 [00:05<00:00, 124.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0281 | Val Loss: 0.2162 | EER: 9.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0262 | Val Loss: 0.2285 | EER: 9.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0280 | Val Loss: 0.2251 | EER: 9.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 113.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0272 | Val Loss: 0.2222 | EER: 9.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0262 | Val Loss: 0.2221 | EER: 9.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0262 | Val Loss: 0.2243 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (Top 6): 100%|██████████| 720/720 [00:05<00:00, 130.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0241 | Val Loss: 0.2272 | EER: 9.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (Top 6): 100%|██████████| 720/720 [00:05<00:00, 131.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0244 | Val Loss: 0.2305 | EER: 9.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 115.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0252 | Val Loss: 0.2282 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0247 | Val Loss: 0.2277 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0242 | Val Loss: 0.2267 | EER: 9.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0247 | Val Loss: 0.2289 | EER: 9.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0240 | Val Loss: 0.2282 | EER: 9.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 113.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0244 | Val Loss: 0.2283 | EER: 9.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0251 | Val Loss: 0.2289 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0244 | Val Loss: 0.2312 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 109.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0249 | Val Loss: 0.2302 | EER: 9.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 106.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0249 | Val Loss: 0.2300 | EER: 9.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0250 | Val Loss: 0.2294 | EER: 9.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 107.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0250 | Val Loss: 0.2292 | EER: 9.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (Top 6): 100%|██████████| 720/720 [00:06<00:00, 108.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0230 | Val Loss: 0.2290 | EER: 9.30%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_transformer_Top6feat.png\n",
      "\n",
      "--- Starting Final Testing (Top 6 Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Final Testing (Top 6): 100%|██████████| 1114/1114 [00:02<00:00, 396.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Results (Top 6 Features) --- | EER: 15.05%\n",
      "\n",
      "--- Experiment Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/TransformerFusion_PyTorch_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_transformer_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_transformer_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_transformer_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_transformer_23feat.png\"\n",
    "# --- MODIFIED: Added paths for the new model trained on top 6 features ---\n",
    "MODEL_SAVE_PATH_TOP6 = \"saved_models/TransformerFusion_PyTorch_Best_Top6feat.pth\"\n",
    "PLOT_SAVE_PATH_TOP6 = \"saved_models/training_metrics_transformer_Top6feat.png\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path, title_prefix=\"\"):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'{title_prefix} Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Transformer + Cross-Attention Fusion Model ---\n",
    "class TransformerAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, d_model=128, nhead=4, num_encoder_layers=3, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerAttentionFusionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cqcc_projection = nn.Linear(cqcc_feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, d_model), nn.ReLU()\n",
    "        )\n",
    "        classifier_input_dim = d_model + d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        cqcc_embed = self.cqcc_projection(cqcc_x) * math.sqrt(self.d_model)\n",
    "        cqcc_embed_permuted = cqcc_embed.permute(1, 0, 2)\n",
    "        cqcc_pos = self.pos_encoder(cqcc_embed_permuted)\n",
    "        transformer_input = cqcc_pos.permute(1, 0, 2)\n",
    "        transformer_out = self.transformer_encoder(transformer_input)\n",
    "        if prosody_x.dim() > 2:\n",
    "            prosody_x = prosody_x.view(prosody_x.size(0), -1)\n",
    "        prosody_query = self.prosody_mlp(prosody_x)\n",
    "        keys = values = transformer_out\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2)) / (keys.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1)\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, device, save_path):\n",
    "    print(\"\\n--- Running Cross-Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            all_weights.append(weights.squeeze(1).cpu().numpy())\n",
    "    avg_weights = np.mean(np.concatenate(all_weights, axis=0), axis=0)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(avg_weights, color='purple')\n",
    "    plt.xlabel('CQCC Time Frame')\n",
    "    plt.ylabel('Average Attention Weight')\n",
    "    plt.title('Cross-Attention: Importance of Acoustic Time Frames Guided by Prosody')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAttention plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# --- MODIFIED: perform_feature_ablation now returns the sorted features ---\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody_clone = prosody.clone()\n",
    "                    prosody_clone[:, feature_to_ablate] = 0.0\n",
    "                    logits, _ = model(cqcc, prosody_clone)\n",
    "                else:\n",
    "                    logits, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "    \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "    return sorted_features\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(cqcc_tensor, prosody_tensor)\n",
    "            output = torch.sigmoid(logits)\n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    plt.figure() \n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        X_cqcc_train_full = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "        X_cqcc_val_full = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        X_prosody_train_full = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val_full = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Loudness_sma3','alphaRatio_sma3','hammarbergIndex_sma3','slope0-500_sma3',\n",
    "            'slope500-1500_sma3','spectralFlux_sma3','mfcc1_sma3','mfcc2_sma3',\n",
    "            'mfcc3_sma3','mfcc4_sma3','F0semitoneFrom27.5Hz_sma3nz','jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz','HNRdBACF_sma3nz','logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz','F1frequency_sma3nz','F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz','F2frequency_sma3nz','F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz','F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        num_prosodic_features = X_prosody_train_full.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- Scaling Full Feature Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train_full)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val_full)\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    ns, nx, ny = X_cqcc_train_full.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train_full.reshape(ns, -1)).reshape(ns, nx, ny)\n",
    "    nsv, nxv, nyv = X_cqcc_val_full.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val_full.reshape(nsv, -1)).reshape(nsv, nxv, nyv)\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = TransformerAttentionFusionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (All Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (All Feats)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        all_preds = (all_scores > 0.5).astype(int)\n",
    "        val_accuracy = 100 * np.sum(all_preds == all_labels) / len(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f} | EER: {eer:.2f}%\")\n",
    "        history.update({'train_loss': history['train_loss']+[avg_train_loss], 'val_loss': history['val_loss']+[avg_val_loss], 'val_acc': history['val_acc']+[val_accuracy], 'f1': history['f1']+[f1], 'eer': history['eer']+[eer]})\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH, title_prefix=\"All Features\")\n",
    "\n",
    "    # --- FINAL TESTING AND ANALYSIS (ALL FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing and Analysis (All Features) ---\")\n",
    "    try:\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        X_prosody_test_full = np.mean(X_prosody_test_3d, axis=2)\n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_full)\n",
    "        ns_test, nx_test, ny_test = X_cqcc_test_full.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_full.reshape(ns_test, -1)).reshape(ns_test, nx_test, ny_test)\n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        analysis_model = TransformerAttentionFusionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "            prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "\n",
    "        all_test_labels, all_test_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in tqdm(test_loader, desc=\"Final Testing (All Feats)\"):\n",
    "                cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "                logits, _ = analysis_model(cqcc, prosody)\n",
    "                all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "        test_eer = calculate_eer(all_test_labels, all_test_scores)\n",
    "        print(f\"\\n--- Final Test Results (All Features) --- | EER: {test_eer:.2f}%\")\n",
    "\n",
    "        analyze_attention_weights(analysis_model, test_loader, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        # --- MODIFIED: Capture the sorted features from the ablation analysis ---\n",
    "        sorted_features = perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- NEW: RETRAINING WITH TOP 6 FEATURES ---\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\\n--- Starting Retraining with Top 6 Features ---\")\n",
    "    \n",
    "    # 1. Get top 6 feature names and their original indices\n",
    "    top_6_feature_names = [item[0] for item in sorted_features[:6]]\n",
    "    top_6_indices = [feature_columns.index(name) for name in top_6_feature_names]\n",
    "    print(\"Top 6 features selected for retraining:\", top_6_feature_names)\n",
    "\n",
    "    # 2. Filter the original prosody datasets\n",
    "    X_prosody_train_top6 = X_prosody_train_full[:, top_6_indices]\n",
    "    X_prosody_val_top6 = X_prosody_val_full[:, top_6_indices]\n",
    "    X_prosody_test_top6 = X_prosody_test_full[:, top_6_indices]\n",
    "\n",
    "    # 3. Create and fit a new scaler for the top 6 features\n",
    "    scaler_prosody_top6 = StandardScaler()\n",
    "    X_prosody_train_scaled_top6 = scaler_prosody_top6.fit_transform(X_prosody_train_top6)\n",
    "    X_prosody_val_scaled_top6 = scaler_prosody_top6.transform(X_prosody_val_top6)\n",
    "    \n",
    "    # 4. Create new datasets and dataloaders\n",
    "    train_dataset_top6 = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled_top6, y_train)\n",
    "    val_dataset_top6 = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled_top6, y_val)\n",
    "    train_loader_top6 = DataLoader(train_dataset_top6, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_top6 = DataLoader(val_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 5. Create a new model instance with prosody_feature_dim=6\n",
    "    model_top6 = TransformerAttentionFusionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=6  # <-- Key change: only 6 features\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer_top6 = optim.Adam(model_top6.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_top6 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_top6, 'min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "    print(model_top6)\n",
    "    best_val_loss_top6 = float('inf')\n",
    "    history_top6 = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (Top 6 Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_top6.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader_top6, desc=f\"Epoch {epoch+1}/{EPOCHS} (Top 6)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer_top6.zero_grad()\n",
    "            logits, _ = model_top6(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer_top6.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model_top6.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader_top6:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model_top6(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader_top6)\n",
    "        avg_val_loss = val_loss / len(val_loader_top6)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EER: {eer:.2f}%\")\n",
    "        history_top6.update({'train_loss': history_top6['train_loss']+[avg_train_loss], 'val_loss': history_top6['val_loss']+[avg_val_loss], 'eer': history_top6['eer']+[eer]})\n",
    "        scheduler_top6.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss_top6:\n",
    "            best_val_loss_top6 = avg_val_loss\n",
    "            torch.save(model_top6.state_dict(), MODEL_SAVE_PATH_TOP6)\n",
    "            print(f\"   -> Val loss decreased. New best model (Top 6) saved to {MODEL_SAVE_PATH_TOP6}\")\n",
    "\n",
    "    plot_training_history(history_top6, PLOT_SAVE_PATH_TOP6, title_prefix=\"Top 6 Features\")\n",
    "\n",
    "    # --- FINAL TESTING (TOP 6 FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing (Top 6 Features) ---\")\n",
    "    X_prosody_test_scaled_top6 = scaler_prosody_top6.transform(X_prosody_test_top6)\n",
    "    test_dataset_top6 = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled_top6, y_test)\n",
    "    test_loader_top6 = DataLoader(test_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_top6.load_state_dict(torch.load(MODEL_SAVE_PATH_TOP6))\n",
    "    model_top6.eval()\n",
    "\n",
    "    all_test_labels, all_test_scores = [], []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in tqdm(test_loader_top6, desc=\"Final Testing (Top 6)\"):\n",
    "            cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "            logits, _ = model_top6(cqcc, prosody)\n",
    "            all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "    test_eer_top6 = calculate_eer(all_test_labels, all_test_scores)\n",
    "    print(f\"\\n--- Final Test Results (Top 6 Features) --- | EER: {test_eer_top6:.2f}%\")\n",
    "    print(\"\\n--- Experiment Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81fe79ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading and Preparing Data ---\n",
      "Converting 3D LLD prosodic features to 2D summary statistics (mean)...\n",
      "\n",
      "--- Scaling Full Feature Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNAttentionFusionModel(\n",
      "  (cnn_branch): Sequential(\n",
      "    (0): Conv1d(157, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (prosody_mlp): Sequential(\n",
      "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 172.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.3000 | Val Loss: 0.2450 | EER: 13.05%\n",
      "   -> Val loss decreased. New best model saved to saved_models/CNNAttentionFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (All Feats): 100%|██████████| 720/720 [00:03<00:00, 204.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.1319 | Val Loss: 0.1583 | EER: 10.31%\n",
      "   -> Val loss decreased. New best model saved to saved_models/CNNAttentionFusion_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 155.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.0833 | Val Loss: 0.1658 | EER: 10.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 251.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.0526 | Val Loss: 0.1664 | EER: 9.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (All Feats): 100%|██████████| 720/720 [00:03<00:00, 224.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.0342 | Val Loss: 0.1857 | EER: 9.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (All Feats): 100%|██████████| 720/720 [00:03<00:00, 214.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0263 | Val Loss: 0.2113 | EER: 9.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (All Feats): 100%|██████████| 720/720 [00:03<00:00, 207.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0189 | Val Loss: 0.3263 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 243.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0191 | Val Loss: 0.2012 | EER: 9.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 258.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0079 | Val Loss: 0.3043 | EER: 10.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0044 | Val Loss: 0.3103 | EER: 9.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 255.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0036 | Val Loss: 0.3603 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 261.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0032 | Val Loss: 0.4316 | EER: 10.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 258.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0036 | Val Loss: 0.3933 | EER: 9.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 256.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0031 | Val Loss: 0.4045 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 273.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0027 | Val Loss: 0.4046 | EER: 10.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 277.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0023 | Val Loss: 0.4143 | EER: 10.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 269.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0017 | Val Loss: 0.4081 | EER: 9.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 264.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0014 | Val Loss: 0.4544 | EER: 10.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 261.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0016 | Val Loss: 0.4257 | EER: 10.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0010 | Val Loss: 0.4910 | EER: 10.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 261.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0013 | Val Loss: 0.4992 | EER: 10.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 258.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0012 | Val Loss: 0.4504 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0009 | Val Loss: 0.4679 | EER: 10.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 257.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0013 | Val Loss: 0.4529 | EER: 10.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0009 | Val Loss: 0.5117 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 257.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0014 | Val Loss: 0.4995 | EER: 10.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0008 | Val Loss: 0.4825 | EER: 10.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0012 | Val Loss: 0.4647 | EER: 10.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 257.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0012 | Val Loss: 0.4828 | EER: 10.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 257.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0009 | Val Loss: 0.4743 | EER: 10.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0010 | Val Loss: 0.4736 | EER: 10.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 170.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0008 | Val Loss: 0.4856 | EER: 10.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 170.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0010 | Val Loss: 0.4775 | EER: 10.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 156.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0011 | Val Loss: 0.4888 | EER: 10.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 164.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0008 | Val Loss: 0.4836 | EER: 10.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 163.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0011 | Val Loss: 0.4822 | EER: 10.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (All Feats): 100%|██████████| 720/720 [00:04<00:00, 175.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0009 | Val Loss: 0.4704 | EER: 10.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (All Feats): 100%|██████████| 720/720 [00:03<00:00, 206.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0014 | Val Loss: 0.4696 | EER: 10.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 260.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0012 | Val Loss: 0.4801 | EER: 10.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (All Feats): 100%|██████████| 720/720 [00:02<00:00, 259.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0009 | Val Loss: 0.4810 | EER: 10.47%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_cnn_attention_23feat.png\n",
      "\n",
      "--- Starting Final Testing and Analysis (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Cross-Attention Weight Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Attention: 100%|██████████| 1114/1114 [00:02<00:00, 509.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention plot saved to saved_models/attention_importance_cnn_attention_23feat.png\n",
      "\n",
      "--- Running Feature Ablation Analysis ---\n",
      "Baseline EER with all features: 13.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Ablation: 100%|██████████| 23/23 [00:54<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance based on EER Increase:\n",
      "- slope500-1500_sma3: EER increases by 0.64%\n",
      "- spectralFlux_sma3: EER increases by 0.63%\n",
      "- hammarbergIndex_sma3: EER increases by 0.61%\n",
      "- slope0-500_sma3: EER increases by 0.40%\n",
      "- F1frequency_sma3nz: EER increases by 0.30%\n",
      "- mfcc1_sma3: EER increases by 0.23%\n",
      "- F2amplitudeLogRelF0_sma3nz: EER increases by 0.23%\n",
      "- HNRdBACF_sma3nz: EER increases by 0.20%\n",
      "- F3amplitudeLogRelF0_sma3nz: EER increases by 0.18%\n",
      "- mfcc3_sma3: EER increases by 0.12%\n",
      "- F3frequency_sma3nz: EER increases by 0.12%\n",
      "- F2frequency_sma3nz: EER increases by 0.12%\n",
      "- shimmerLocaldB_sma3nz: EER increases by 0.11%\n",
      "- logRelF0-H1-H2_sma3nz: EER increases by 0.07%\n",
      "- F0semitoneFrom27.5Hz_sma3nz: EER increases by 0.00%\n",
      "- Loudness_sma3: EER increases by -0.01%\n",
      "- logRelF0-H1-A3_sma3nz: EER increases by -0.03%\n",
      "- jitterLocal_sma3nz: EER increases by -0.05%\n",
      "- F1amplitudeLogRelF0_sma3nz: EER increases by -0.05%\n",
      "- mfcc4_sma3: EER increases by -0.08%\n",
      "- alphaRatio_sma3: EER increases by -0.12%\n",
      "- mfcc2_sma3: EER increases by -0.26%\n",
      "- F1bandwidth_sma3nz: EER increases by -0.52%\n",
      "\n",
      "Ablation plot saved to saved_models/ablation_importance_cnn_attention_23feat.png\n",
      "\n",
      "--- Running SHAP Analysis ---\n",
      "Calculating SHAP values (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e7dee82a0247a695aa0a1d68c7e316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting SHAP summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "The figure layout has changed to tight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP plot saved to saved_models/shap_importance_cnn_attention_23feat.png\n",
      "\n",
      "\n",
      "--- Starting Retraining with Top 6 Features ---\n",
      "Top 10 features selected for retraining: ['slope500-1500_sma3', 'spectralFlux_sma3', 'hammarbergIndex_sma3', 'slope0-500_sma3', 'F1frequency_sma3nz', 'mfcc1_sma3', 'F2amplitudeLogRelF0_sma3nz', 'HNRdBACF_sma3nz', 'F3amplitudeLogRelF0_sma3nz', 'mfcc3_sma3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNAttentionFusionModel(\n",
      "  (cnn_branch): Sequential(\n",
      "    (0): Conv1d(157, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (prosody_mlp): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (Top 10 Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (Top 10): 100%|██████████| 720/720 [00:05<00:00, 136.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.3061 | Val Loss: 0.1963 | EER: 12.91%\n",
      "   -> Val loss decreased. New best model (Top 10) saved to saved_models/CNNAttentionFusion_PyTorch_Best_Top10feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (Top 10): 100%|██████████| 720/720 [00:05<00:00, 129.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.1451 | Val Loss: 0.1637 | EER: 12.05%\n",
      "   -> Val loss decreased. New best model (Top 10) saved to saved_models/CNNAttentionFusion_PyTorch_Best_Top10feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (Top 10): 100%|██████████| 720/720 [00:05<00:00, 132.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.0875 | Val Loss: 0.1669 | EER: 11.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 151.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.0569 | Val Loss: 0.2137 | EER: 12.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 227.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.0396 | Val Loss: 0.2045 | EER: 9.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 227.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0263 | Val Loss: 0.2393 | EER: 11.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 224.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0220 | Val Loss: 0.1824 | EER: 9.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 228.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0190 | Val Loss: 0.2406 | EER: 10.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 229.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0087 | Val Loss: 0.2251 | EER: 9.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 237.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0057 | Val Loss: 0.2923 | EER: 10.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 239.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0046 | Val Loss: 0.3047 | EER: 10.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 227.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0037 | Val Loss: 0.3246 | EER: 10.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 228.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0031 | Val Loss: 0.2883 | EER: 10.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 234.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0032 | Val Loss: 0.3545 | EER: 10.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (Top 10): 100%|██████████| 720/720 [00:02<00:00, 240.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0020 | Val Loss: 0.3459 | EER: 10.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 229.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0018 | Val Loss: 0.3898 | EER: 10.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 234.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0020 | Val Loss: 0.3756 | EER: 10.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 232.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0017 | Val Loss: 0.3731 | EER: 10.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 232.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0014 | Val Loss: 0.3814 | EER: 10.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 233.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0015 | Val Loss: 0.4044 | EER: 10.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 230.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0018 | Val Loss: 0.3961 | EER: 10.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 232.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0009 | Val Loss: 0.3938 | EER: 10.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 168.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0012 | Val Loss: 0.4236 | EER: 10.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 170.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0013 | Val Loss: 0.4041 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 177.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0015 | Val Loss: 0.3989 | EER: 10.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 165.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0013 | Val Loss: 0.3929 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 179.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0013 | Val Loss: 0.3982 | EER: 10.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 165.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0011 | Val Loss: 0.4012 | EER: 10.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 170.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0012 | Val Loss: 0.4055 | EER: 10.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 171.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0015 | Val Loss: 0.3876 | EER: 10.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 168.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0011 | Val Loss: 0.4126 | EER: 10.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 168.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0011 | Val Loss: 0.3751 | EER: 10.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 167.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0011 | Val Loss: 0.3856 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 181.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0010 | Val Loss: 0.4126 | EER: 10.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 233.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0013 | Val Loss: 0.4070 | EER: 10.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (Top 10): 100%|██████████| 720/720 [00:04<00:00, 164.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0011 | Val Loss: 0.4196 | EER: 10.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 231.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0010 | Val Loss: 0.4127 | EER: 10.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 232.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0009 | Val Loss: 0.4115 | EER: 10.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (Top 10): 100%|██████████| 720/720 [00:02<00:00, 240.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0011 | Val Loss: 0.4206 | EER: 10.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (Top 10): 100%|██████████| 720/720 [00:03<00:00, 234.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0011 | Val Loss: 0.3762 | EER: 10.52%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_cnn_attention_Top10feat.png\n",
      "\n",
      "--- Starting Final Testing (Top 10 Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Final Testing (Top 10): 100%|██████████| 1114/1114 [00:02<00:00, 504.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Results (Top 10 Features) --- | EER: 15.55%\n",
      "\n",
      "--- Experiment Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/CNNAttentionFusion_PyTorch_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_cnn_attention_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_cnn_attention_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_cnn_attention_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_cnn_attention_23feat.png\"\n",
    "MODEL_SAVE_PATH_TOP10 = \"saved_models/CNNAttentionFusion_PyTorch_Best_Top10feat.pth\"\n",
    "PLOT_SAVE_PATH_TOP10= \"saved_models/training_metrics_cnn_attention_Top10feat.png\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path, title_prefix=\"\"):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'{title_prefix} Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- NEW: CNN + Attention Fusion Model ---\n",
    "class CNNAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, cnn_out_channels=128, mlp_out_dim=128):\n",
    "        super(CNNAttentionFusionModel, self).__init__()\n",
    "        \n",
    "        # CNN branch for processing CQCC features. This will produce the Key and Value for attention.\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=cqcc_feature_dim, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(in_channels=128, out_channels=cnn_out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_out_channels),\n",
    "        )\n",
    "        \n",
    "        # MLP branch for processing prosodic features. This will produce the Query.\n",
    "        self.prosody_mlp = nn.Sequential(\n",
    "            nn.Linear(prosody_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            # The output dimension must match the CNN output channels for attention\n",
    "            nn.Linear(256, cnn_out_channels), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier to combine the attention context and the prosody query\n",
    "        classifier_input_dim = cnn_out_channels + cnn_out_channels\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # 1. Process CQCC features through the CNN branch\n",
    "        # Permute from (batch, seq_len, features) to (batch, features, seq_len)\n",
    "        cqcc_x_permuted = cqcc_x.permute(0, 2, 1)\n",
    "        cnn_out = self.cnn_branch(cqcc_x_permuted) # Shape: (batch, cnn_out_channels, seq_len)\n",
    "        \n",
    "        # 2. Define Key and Value from CNN output for attention\n",
    "        # Permute to (batch, seq_len, cnn_out_channels) for attention calculation\n",
    "        keys = values = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        # 3. Process prosodic features through the MLP to get the Query\n",
    "        prosody_query = self.prosody_mlp(prosody_x) # Shape: (batch, cnn_out_channels)\n",
    "\n",
    "        # 4. Perform Cross-Attention\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1) # Shape: (batch, 1, cnn_out_channels)\n",
    "        \n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2)) / (keys.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1) # Shape: (batch, 1, seq_len)\n",
    "        \n",
    "        context = torch.bmm(attention_weights, values).squeeze(1) # Shape: (batch, cnn_out_channels)\n",
    "        \n",
    "        # 5. Fuse the attention context with the prosody query and classify\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        # Return logits for loss and attention_weights for analysis\n",
    "        return logits, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, device, save_path):\n",
    "    print(\"\\n--- Running Cross-Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            all_weights.append(weights.squeeze(1).cpu().numpy())\n",
    "    \n",
    "    avg_weights = np.mean(np.concatenate(all_weights, axis=0), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(avg_weights, color='purple')\n",
    "    plt.xlabel('CQCC Time Frame (after CNN processing)')\n",
    "    plt.ylabel('Average Attention Weight')\n",
    "    plt.title('Cross-Attention: Importance of Acoustic Time Frames Guided by Prosody')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAttention plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody_clone = prosody.clone()\n",
    "                    prosody_clone[:, feature_to_ablate] = 0.0\n",
    "                    logits, _ = model(cqcc, prosody_clone)\n",
    "                else:\n",
    "                    logits, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "    \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "    return sorted_features\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(cqcc_tensor, prosody_tensor)\n",
    "            output = torch.sigmoid(logits)\n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    plt.figure() \n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        X_cqcc_train_full = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "        X_cqcc_val_full = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "        \n",
    "        print(\"Converting 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "        X_prosody_train_full = np.mean(X_prosody_train_3d, axis=2)\n",
    "        X_prosody_val_full = np.mean(X_prosody_val_3d, axis=2)\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Loudness_sma3','alphaRatio_sma3','hammarbergIndex_sma3','slope0-500_sma3',\n",
    "            'slope500-1500_sma3','spectralFlux_sma3','mfcc1_sma3','mfcc2_sma3',\n",
    "            'mfcc3_sma3','mfcc4_sma3','F0semitoneFrom27.5Hz_sma3nz','jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz','HNRdBACF_sma3nz','logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz','F1frequency_sma3nz','F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz','F2frequency_sma3nz','F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz','F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        num_prosodic_features = X_prosody_train_full.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- Scaling Full Feature Data ---\")\n",
    "    scaler_prosody = StandardScaler()\n",
    "    X_prosody_train_scaled = scaler_prosody.fit_transform(X_prosody_train_full)\n",
    "    X_prosody_val_scaled = scaler_prosody.transform(X_prosody_val_full)\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    ns, nx, ny = X_cqcc_train_full.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train_full.reshape(ns, -1)).reshape(ns, nx, ny)\n",
    "    nsv, nxv, nyv = X_cqcc_val_full.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val_full.reshape(nsv, -1)).reshape(nsv, nxv, nyv)\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = CNNAttentionFusionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (All Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (All Feats)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EER: {eer:.2f}%\")\n",
    "        history.update({'train_loss': history['train_loss']+[avg_train_loss], 'val_loss': history['val_loss']+[avg_val_loss], 'eer': history['eer']+[eer]})\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH, title_prefix=\"All Features (CNN-Attention)\")\n",
    "\n",
    "    # --- FINAL TESTING AND ANALYSIS (ALL FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing and Analysis (All Features) ---\")\n",
    "    try:\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        X_prosody_test_full = np.mean(X_prosody_test_3d, axis=2)\n",
    "        X_prosody_test_scaled = scaler_prosody.transform(X_prosody_test_full)\n",
    "        ns_test, nx_test, ny_test = X_cqcc_test_full.shape\n",
    "        X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_full.reshape(ns_test, -1)).reshape(ns_test, nx_test, ny_test)\n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        analysis_model = CNNAttentionFusionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "            prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "        \n",
    "        analyze_attention_weights(analysis_model, test_loader, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        sorted_features = perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- NEW: RETRAINING WITH TOP 10 FEATURES ---\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\\n--- Starting Retraining with Top 6 Features ---\")\n",
    "    \n",
    "    top_10_feature_names = [item[0] for item in sorted_features[:10]]\n",
    "    top_10_indices = [feature_columns.index(name) for name in top_10_feature_names]\n",
    "    print(\"Top 10 features selected for retraining:\", top_10_feature_names)\n",
    "\n",
    "    X_prosody_train_top10 = X_prosody_train_full[:, top_10_indices]\n",
    "    X_prosody_val_top10 = X_prosody_val_full[:, top_10_indices]\n",
    "    X_prosody_test_top10 = X_prosody_test_full[:, top_10_indices]\n",
    "\n",
    "    scaler_prosody_top10 = StandardScaler()\n",
    "    X_prosody_train_scaled_top10 = scaler_prosody_top10.fit_transform(X_prosody_train_top10)\n",
    "    X_prosody_val_scaled_top10 = scaler_prosody_top10.transform(X_prosody_val_top10)\n",
    "    \n",
    "    train_dataset_top10 = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_scaled_top10, y_train)\n",
    "    val_dataset_top10 = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_scaled_top10, y_val)\n",
    "    train_loader_top10 = DataLoader(train_dataset_top10, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_top10 = DataLoader(val_dataset_top10, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model_top10 = CNNAttentionFusionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=10\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer_top10 = optim.Adam(model_top10.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_top10 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_top10, 'min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "    print(model_top10)\n",
    "    best_val_loss_top10 = float('inf')\n",
    "    history_top10 = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (Top 10 Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_top10.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader_top10, desc=f\"Epoch {epoch+1}/{EPOCHS} (Top 10)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer_top10.zero_grad()\n",
    "            logits, _ = model_top10(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer_top10.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model_top10.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader_top10:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model_top10(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader_top10)\n",
    "        avg_val_loss = val_loss / len(val_loader_top10)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EER: {eer:.2f}%\")\n",
    "        history_top10.update({'train_loss': history_top10['train_loss']+[avg_train_loss], 'val_loss': history_top10['val_loss']+[avg_val_loss], 'eer': history_top10['eer']+[eer]})\n",
    "        scheduler_top10.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss_top10:\n",
    "            best_val_loss_top10 = avg_val_loss\n",
    "            torch.save(model_top10.state_dict(), MODEL_SAVE_PATH_TOP10)\n",
    "            print(f\"   -> Val loss decreased. New best model (Top 10) saved to {MODEL_SAVE_PATH_TOP10}\")\n",
    "\n",
    "    plot_training_history(history_top10, PLOT_SAVE_PATH_TOP10, title_prefix=\"Top 10 Features (CNN-Attention)\")\n",
    "\n",
    "    # --- FINAL TESTING (TOP 10 FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing (Top 10 Features) ---\")\n",
    "    X_prosody_test_scaled_top10 = scaler_prosody_top10.transform(X_prosody_test_top10)\n",
    "    test_dataset_top10 = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_scaled_top10, y_test)\n",
    "    test_loader_top10 = DataLoader(test_dataset_top10, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_top10.load_state_dict(torch.load(MODEL_SAVE_PATH_TOP10))\n",
    "    model_top10.eval()\n",
    "\n",
    "    all_test_labels, all_test_scores = [], []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in tqdm(test_loader_top10, desc=\"Final Testing (Top 10)\"):\n",
    "            cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "            logits, _ = model_top10(cqcc, prosody)\n",
    "            all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "    test_eer_top10 = calculate_eer(all_test_labels, all_test_scores)\n",
    "    print(f\"\\n--- Final Test Results (Top 10 Features) --- | EER: {test_eer_top10:.2f}%\")\n",
    "    print(\"\\n--- Experiment Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6b1d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading and Preparing Data ---\n",
      "\n",
      "--- Scaling LLD Prosodic Features ---\n",
      "Converting SCALED 3D LLD prosodic features to 2D summary statistics (mean)...\n",
      "\n",
      "--- Scaling CQCC Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNBiLSTMAttentionModel(\n",
      "  (cnn_branch): Sequential(\n",
      "    (0): Conv1d(157, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (bilstm_branch): LSTM(23, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (query_transform): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.2988 | Val Loss: 0.1552 | EER: 10.60%\n",
      "   -> Val loss decreased. New best model saved to saved_models/CNNBiLSTMAttention_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.0847 | Val Loss: 0.1354 | EER: 10.09%\n",
      "   -> Val loss decreased. New best model saved to saved_models/CNNBiLSTMAttention_PyTorch_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.0412 | Val Loss: 0.1605 | EER: 9.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.0220 | Val Loss: 0.2041 | EER: 9.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (All Feats): 100%|██████████| 720/720 [00:13<00:00, 52.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.0153 | Val Loss: 0.2036 | EER: 9.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0118 | Val Loss: 0.2359 | EER: 9.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0092 | Val Loss: 0.1957 | EER: 8.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0062 | Val Loss: 0.2056 | EER: 8.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0021 | Val Loss: 0.3009 | EER: 8.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 60.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0005 | Val Loss: 0.3192 | EER: 8.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0045 | Val Loss: 0.3149 | EER: 9.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 58.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0014 | Val Loss: 0.3372 | EER: 9.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0004 | Val Loss: 0.3756 | EER: 9.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0003 | Val Loss: 0.3781 | EER: 9.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0002 | Val Loss: 0.3553 | EER: 8.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 65.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0001 | Val Loss: 0.3686 | EER: 9.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0005 | Val Loss: 0.3523 | EER: 8.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0003 | Val Loss: 0.3815 | EER: 8.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0001 | Val Loss: 0.3872 | EER: 9.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0002 | Val Loss: 0.3527 | EER: 8.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0001 | Val Loss: 0.3818 | EER: 9.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0001 | Val Loss: 0.3900 | EER: 9.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 65.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0001 | Val Loss: 0.4011 | EER: 8.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0002 | Val Loss: 0.3749 | EER: 8.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 65.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0002 | Val Loss: 0.3819 | EER: 9.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0001 | Val Loss: 0.4538 | EER: 9.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0001 | Val Loss: 0.3978 | EER: 9.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0001 | Val Loss: 0.4069 | EER: 8.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0001 | Val Loss: 0.4082 | EER: 9.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 65.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0001 | Val Loss: 0.4276 | EER: 9.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0001 | Val Loss: 0.4252 | EER: 9.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 65.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0001 | Val Loss: 0.3839 | EER: 9.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0002 | Val Loss: 0.4246 | EER: 9.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0001 | Val Loss: 0.4261 | EER: 9.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 69.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0001 | Val Loss: 0.4039 | EER: 9.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (All Feats):  85%|████████▌ | 614/720 [00:09<00:01, 67.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 374\u001b[0m\n\u001b[1;32m    372\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    373\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 374\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    377\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/adam.py:584\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    582\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    586\u001b[0m     ]\n\u001b[1;32m    587\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    589\u001b[0m     ]\n\u001b[1;32m    591\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/adam.py:585\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    582\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 585\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    586\u001b[0m     ]\n\u001b[1;32m    587\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    589\u001b[0m     ]\n\u001b[1;32m    591\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/mitul/uniaudio/lib/python3.8/site-packages/torch/optim/optimizer.py:101\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_value\u001b[39m(x):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# item is significantly faster than a cpu tensor in eager mode\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m is_compiling():\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/CNNBiLSTMAttention_PyTorch_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_cnn_bilstm_attention_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_cnn_bilstm_attention_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_cnn_bilstm_attention_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_cnn_bilstm_attention_23feat.png\"\n",
    "MODEL_SAVE_PATH_TOP6 = \"saved_models/CNNBiLSTMAttention_PyTorch_Best_Top6feat.pth\"\n",
    "PLOT_SAVE_PATH_TOP6 = \"saved_models/training_metrics_cnn_bilstm_attention_Top6feat.png\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"Calculates the Equal Error Rate (EER).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "def plot_training_history(history, save_path, title_prefix=\"\"):\n",
    "    \"\"\"Plots and saves the training history graph.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%)', color=color)\n",
    "    ax2.plot(history['eer'], color=color, linestyle='-', label='EER (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'{title_prefix} Training and Validation Metrics')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- NEW: CNN + BiLSTM + Attention Fusion Model ---\n",
    "class CNNBiLSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, cnn_out_channels=128, lstm_hidden_dim=128):\n",
    "        super(CNNBiLSTMAttentionModel, self).__init__()\n",
    "        \n",
    "        # CNN branch for processing CQCC features (Key, Value)\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=cqcc_feature_dim, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(in_channels=128, out_channels=cnn_out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_out_channels),\n",
    "        )\n",
    "        \n",
    "        # BiLSTM branch for processing prosodic features (Query)\n",
    "        self.bilstm_branch = nn.LSTM(\n",
    "            input_size=prosody_feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Linear layer to transform BiLSTM output to match CNN channel dimension for attention\n",
    "        # BiLSTM output is 2 * lstm_hidden_dim because it's bidirectional\n",
    "        self.query_transform = nn.Linear(lstm_hidden_dim * 2, cnn_out_channels)\n",
    "        \n",
    "        # Classifier to combine the attention context and the prosody query\n",
    "        classifier_input_dim = cnn_out_channels + cnn_out_channels\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # 1. Process CQCC features through the CNN branch\n",
    "        cqcc_x_permuted = cqcc_x.permute(0, 2, 1)\n",
    "        cnn_out = self.cnn_branch(cqcc_x_permuted)\n",
    "        keys = values = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        # 2. Process prosodic features through the BiLSTM branch\n",
    "        # Add a sequence dimension: (batch, features) -> (batch, 1, features)\n",
    "        prosody_x_unsqueezed = prosody_x.unsqueeze(1)\n",
    "        lstm_out, _ = self.bilstm_branch(prosody_x_unsqueezed)\n",
    "        # Take the output of the last time step\n",
    "        lstm_out_last = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 3. Transform BiLSTM output to create the Query\n",
    "        prosody_query = self.query_transform(lstm_out_last)\n",
    "\n",
    "        # 4. Perform Cross-Attention\n",
    "        query_unsqueezed = prosody_query.unsqueeze(1)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2)) / (keys.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1)\n",
    "        \n",
    "        # 5. Fuse and classify\n",
    "        fused_features = torch.cat([context, prosody_query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, device, save_path):\n",
    "    print(\"\\n--- Running Cross-Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            all_weights.append(weights.squeeze(1).cpu().numpy())\n",
    "    \n",
    "    avg_weights = np.mean(np.concatenate(all_weights, axis=0), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(avg_weights, color='purple')\n",
    "    plt.xlabel('CQCC Time Frame (after CNN processing)')\n",
    "    plt.ylabel('Average Attention Weight')\n",
    "    plt.title('Cross-Attention: Importance of Acoustic Time Frames Guided by Prosody')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAttention plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody_clone = prosody.clone()\n",
    "                    prosody_clone[:, feature_to_ablate] = 0.0\n",
    "                    logits, _ = model(cqcc, prosody_clone)\n",
    "                else:\n",
    "                    logits, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        return calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "    \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "    return sorted_features\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(cqcc_tensor, prosody_tensor)\n",
    "            output = torch.sigmoid(logits)\n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    plt.figure() \n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        X_cqcc_train_full = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "        \n",
    "        X_cqcc_val_full = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Loudness_sma3','alphaRatio_sma3','hammarbergIndex_sma3','slope0-500_sma3',\n",
    "            'slope500-1500_sma3','spectralFlux_sma3','mfcc1_sma3','mfcc2_sma3',\n",
    "            'mfcc3_sma3','mfcc4_sma3','F0semitoneFrom27.5Hz_sma3nz','jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz','HNRdBACF_sma3nz','logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz','F1frequency_sma3nz','F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz','F2frequency_sma3nz','F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz','F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        # Assuming shape is (samples, features, timesteps)\n",
    "        num_prosodic_features = X_prosody_train_3d.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- NEW: Scaling LLD Prosodic Features before creating summary stats ---\n",
    "    print(\"\\n--- Scaling LLD Prosodic Features ---\")\n",
    "    # Assuming shape is (samples, features, timesteps). We need to scale across the features.\n",
    "    # To do this, we transpose to (samples, timesteps, features) and then reshape.\n",
    "    ns_p, nf_p, nt_p = X_prosody_train_3d.shape\n",
    "    X_prosody_train_reshaped = X_prosody_train_3d.transpose(0, 2, 1).reshape(-1, nf_p)\n",
    "    \n",
    "    # Fit scaler ONLY on training data\n",
    "    scaler_prosody_lld = StandardScaler().fit(X_prosody_train_reshaped)\n",
    "\n",
    "    # Transform train data and reshape back\n",
    "    X_prosody_train_3d_scaled = scaler_prosody_lld.transform(X_prosody_train_reshaped).reshape(ns_p, nt_p, nf_p).transpose(0, 2, 1)\n",
    "\n",
    "    # Transform validation data\n",
    "    nsv_p, nfv_p, ntv_p = X_prosody_val_3d.shape\n",
    "    X_prosody_val_reshaped = X_prosody_val_3d.transpose(0, 2, 1).reshape(-1, nfv_p)\n",
    "    X_prosody_val_3d_scaled = scaler_prosody_lld.transform(X_prosody_val_reshaped).reshape(nsv_p, ntv_p, nfv_p).transpose(0, 2, 1)\n",
    "    \n",
    "    # Transform test data\n",
    "    nst_p, nft_p, ntt_p = X_prosody_test_3d.shape\n",
    "    X_prosody_test_reshaped = X_prosody_test_3d.transpose(0, 2, 1).reshape(-1, nft_p)\n",
    "    X_prosody_test_3d_scaled = scaler_prosody_lld.transform(X_prosody_test_reshaped).reshape(nst_p, ntt_p, nft_p).transpose(0, 2, 1)\n",
    "\n",
    "    print(\"Converting SCALED 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "    X_prosody_train_full = np.mean(X_prosody_train_3d_scaled, axis=2)\n",
    "    X_prosody_val_full = np.mean(X_prosody_val_3d_scaled, axis=2)\n",
    "    X_prosody_test_full = np.mean(X_prosody_test_3d_scaled, axis=2)\n",
    "\n",
    "    # --- Scaling CQCC Data ---\n",
    "    print(\"\\n--- Scaling CQCC Data ---\")\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    ns, nx, ny = X_cqcc_train_full.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train_full.reshape(ns, -1)).reshape(ns, nx, ny)\n",
    "    nsv, nxv, nyv = X_cqcc_val_full.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val_full.reshape(nsv, -1)).reshape(nsv, nxv, nyv)\n",
    "    ns_test, nx_test, ny_test = X_cqcc_test_full.shape\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_full.reshape(ns_test, -1)).reshape(ns_test, nx_test, ny_test)\n",
    "\n",
    "    # Use the scaled & summarized prosody data\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_full, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_full, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = CNNBiLSTMAttentionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (All Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (All Feats)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EER: {eer:.2f}%\")\n",
    "        history.update({'train_loss': history['train_loss']+[avg_train_loss], 'val_loss': history['val_loss']+[avg_val_loss], 'eer': history['eer']+[eer]})\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val loss decreased. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH, title_prefix=\"All Features (CNN-BiLSTM-Attention)\")\n",
    "\n",
    "    # --- FINAL TESTING AND ANALYSIS (ALL FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing and Analysis (All Features) ---\")\n",
    "    try:\n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_full, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        analysis_model = CNNBiLSTMAttentionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "            prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "        \n",
    "        analyze_attention_weights(analysis_model, test_loader, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        sorted_features = perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- NEW: RETRAINING WITH TOP 6 FEATURES ---\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\\n--- Starting Retraining with Top 6 Features ---\")\n",
    "    \n",
    "    top_6_feature_names = [item[0] for item in sorted_features[:6]]\n",
    "    top_6_indices = [feature_columns.index(name) for name in top_6_feature_names]\n",
    "    print(\"Top 6 features selected for retraining:\", top_6_feature_names)\n",
    "\n",
    "    X_prosody_train_top6 = X_prosody_train_full[:, top_6_indices]\n",
    "    X_prosody_val_top6 = X_prosody_val_full[:, top_6_indices]\n",
    "    X_prosody_test_top6 = X_prosody_test_full[:, top_6_indices]\n",
    "    \n",
    "    train_dataset_top6 = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_top6, y_train)\n",
    "    val_dataset_top6 = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_top6, y_val)\n",
    "    train_loader_top6 = DataLoader(train_dataset_top6, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_top6 = DataLoader(val_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model_top6 = CNNBiLSTMAttentionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=6\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer_top6 = optim.Adam(model_top6.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler_top6 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_top6, 'min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "    print(model_top6)\n",
    "    best_val_loss_top6 = float('inf')\n",
    "    history_top6 = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'f1': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (Top 6 Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_top6.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader_top6, desc=f\"Epoch {epoch+1}/{EPOCHS} (Top 6)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer_top6.zero_grad()\n",
    "            logits, _ = model_top6(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer_top6.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model_top6.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader_top6:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model_top6(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader_top6)\n",
    "        avg_val_loss = val_loss / len(val_loader_top6)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        eer = calculate_eer(all_labels, all_scores)\n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EER: {eer:.2f}%\")\n",
    "        history_top6.update({'train_loss': history_top6['train_loss']+[avg_train_loss], 'val_loss': history_top6['val_loss']+[avg_val_loss], 'eer': history_top6['eer']+[eer]})\n",
    "        scheduler_top6.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss_top6:\n",
    "            best_val_loss_top6 = avg_val_loss\n",
    "            torch.save(model_top6.state_dict(), MODEL_SAVE_PATH_TOP6)\n",
    "            print(f\"   -> Val loss decreased. New best model (Top 6) saved to {MODEL_SAVE_PATH_TOP6}\")\n",
    "\n",
    "    plot_training_history(history_top6, PLOT_SAVE_PATH_TOP6, title_prefix=\"Top 6 Features (CNN-BiLSTM-Attention)\")\n",
    "\n",
    "    # --- FINAL TESTING (TOP 6 FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing (Top 6 Features) ---\")\n",
    "    test_dataset_top6 = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_top6, y_test)\n",
    "    test_loader_top6 = DataLoader(test_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_top6.load_state_dict(torch.load(MODEL_SAVE_PATH_TOP6))\n",
    "    model_top6.eval()\n",
    "\n",
    "    all_test_labels, all_test_scores = [], []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, labels in tqdm(test_loader_top6, desc=\"Final Testing (Top 6)\"):\n",
    "            cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "            logits, _ = model_top6(cqcc, prosody)\n",
    "            all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "    test_eer_top6 = calculate_eer(all_test_labels, all_test_scores)\n",
    "    print(f\"\\n--- Final Test Results (Top 6 Features) --- | EER: {test_eer_top6:.2f}%\")\n",
    "    print(\"\\n--- Experiment Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35b3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading and Preparing Data ---\n",
      "\n",
      "--- Scaling LLD Prosodic Features ---\n",
      "Converting SCALED 3D LLD prosodic features to 2D summary statistics (mean)...\n",
      "\n",
      "--- Scaling CQCC Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcousticProsodicAttentionModel(\n",
      "  (acoustic_branch): Sequential(\n",
      "    (0): Conv1d(157, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (prosodic_branch): LSTM(23, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (key_value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (query_projection): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Starting Model Training (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.2606 | Val Loss: 0.1447 | Val Acc: 89.74% | EER: 10.27%\n",
      "   -> Val EER decreased to 10.27%. New best model saved to saved_models/AcousticProsodicAttention_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.0344 | Val Loss: 0.1452 | Val Acc: 91.53% | EER: 8.48%\n",
      "   -> Val EER decreased to 8.48%. New best model saved to saved_models/AcousticProsodicAttention_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.0129 | Val Loss: 0.1936 | Val Acc: 91.36% | EER: 8.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 65.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.0061 | Val Loss: 0.2020 | Val Acc: 91.34% | EER: 8.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 (All Feats): 100%|██████████| 720/720 [00:13<00:00, 52.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.0050 | Val Loss: 0.2703 | Val Acc: 89.98% | EER: 10.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.0046 | Val Loss: 0.2676 | Val Acc: 91.24% | EER: 8.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 60.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.0043 | Val Loss: 0.3063 | Val Acc: 91.33% | EER: 8.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 61.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.0045 | Val Loss: 0.3253 | Val Acc: 91.15% | EER: 8.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.0011 | Val Loss: 0.2395 | Val Acc: 91.66% | EER: 8.34%\n",
      "   -> Val EER decreased to 8.34%. New best model saved to saved_models/AcousticProsodicAttention_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 63.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0006 | Val Loss: 0.2623 | Val Acc: 91.79% | EER: 8.20%\n",
      "   -> Val EER decreased to 8.20%. New best model saved to saved_models/AcousticProsodicAttention_Best_23feat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0005 | Val Loss: 0.3019 | Val Acc: 91.49% | EER: 8.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0003 | Val Loss: 0.2861 | Val Acc: 91.48% | EER: 8.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 61.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0002 | Val Loss: 0.3354 | Val Acc: 91.44% | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 69.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 0.0001 | Val Loss: 0.3353 | Val Acc: 91.59% | EER: 8.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 0.0001 | Val Loss: 0.3153 | Val Acc: 91.76% | EER: 8.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 0.0001 | Val Loss: 0.4011 | Val Acc: 91.48% | EER: 8.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 0.0001 | Val Loss: 0.3622 | Val Acc: 91.45% | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 0.0001 | Val Loss: 0.4123 | Val Acc: 91.45% | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 0.0001 | Val Loss: 0.3507 | Val Acc: 91.40% | EER: 8.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 0.0001 | Val Loss: 0.4132 | Val Acc: 91.49% | EER: 8.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | Train Loss: 0.0001 | Val Loss: 0.3374 | Val Acc: 91.57% | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 65.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | Train Loss: 0.0000 | Val Loss: 0.3814 | Val Acc: 91.54% | EER: 8.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | Train Loss: 0.0001 | Val Loss: 0.4143 | Val Acc: 91.47% | EER: 8.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | Train Loss: 0.0000 | Val Loss: 0.3685 | Val Acc: 91.58% | EER: 8.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | Train Loss: 0.0000 | Val Loss: 0.3671 | Val Acc: 91.49% | EER: 8.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | Train Loss: 0.0000 | Val Loss: 0.3893 | Val Acc: 91.55% | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 65.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | Train Loss: 0.0000 | Val Loss: 0.4206 | Val Acc: 91.54% | EER: 8.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | Train Loss: 0.0025 | Val Loss: 0.4235 | Val Acc: 91.44% | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | Train Loss: 0.0000 | Val Loss: 0.4109 | Val Acc: 91.37% | EER: 8.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | Train Loss: 0.0000 | Val Loss: 0.4245 | Val Acc: 91.57% | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 67.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | Train Loss: 0.0000 | Val Loss: 0.3286 | Val Acc: 91.56% | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 68.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | Train Loss: 0.0000 | Val Loss: 0.4343 | Val Acc: 91.30% | EER: 8.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 (All Feats): 100%|██████████| 720/720 [00:10<00:00, 66.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | Train Loss: 0.0000 | Val Loss: 0.3679 | Val Acc: 91.49% | EER: 8.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | Train Loss: 0.0000 | Val Loss: 0.3467 | Val Acc: 91.58% | EER: 8.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | Train Loss: 0.0003 | Val Loss: 0.4036 | Val Acc: 91.44% | EER: 8.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | Train Loss: 0.0000 | Val Loss: 0.3608 | Val Acc: 91.52% | EER: 8.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | Train Loss: 0.0000 | Val Loss: 0.3850 | Val Acc: 91.33% | EER: 8.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | Train Loss: 0.0000 | Val Loss: 0.3768 | Val Acc: 91.54% | EER: 8.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 (All Feats): 100%|██████████| 720/720 [00:11<00:00, 60.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | Train Loss: 0.0000 | Val Loss: 0.3689 | Val Acc: 91.56% | EER: 8.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 (All Feats): 100%|██████████| 720/720 [00:12<00:00, 59.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | Train Loss: 0.0000 | Val Loss: 0.3745 | Val Acc: 91.48% | EER: 8.52%\n",
      "\n",
      "Training plot saved to saved_models/training_metrics_AcousticProsodicAttention_23feat.png\n",
      "\n",
      "--- Starting Final Testing and Analysis (All Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Cross-Attention Weight Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Attention: 100%|██████████| 1114/1114 [00:06<00:00, 168.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention plot saved to saved_models/attention_importance_AcousticProsodicAttention_23feat.png\n",
      "\n",
      "--- Running Feature Ablation Analysis ---\n",
      "Baseline EER with all features: 12.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Ablation: 100%|██████████| 23/23 [04:07<00:00, 10.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance based on EER Increase:\n",
      "- slope0-500_sma3: EER increases by 0.72%\n",
      "- F1amplitudeLogRelF0_sma3nz: EER increases by 0.21%\n",
      "- spectralFlux_sma3: EER increases by 0.21%\n",
      "- mfcc3_sma3: EER increases by 0.18%\n",
      "- F3amplitudeLogRelF0_sma3nz: EER increases by 0.17%\n",
      "- F2amplitudeLogRelF0_sma3nz: EER increases by 0.15%\n",
      "- mfcc1_sma3: EER increases by 0.11%\n",
      "- shimmerLocaldB_sma3nz: EER increases by 0.10%\n",
      "- logRelF0-H1-A3_sma3nz: EER increases by 0.08%\n",
      "- slope500-1500_sma3: EER increases by 0.08%\n",
      "- F0semitoneFrom27.5Hz_sma3nz: EER increases by 0.05%\n",
      "- HNRdBACF_sma3nz: EER increases by 0.02%\n",
      "- mfcc4_sma3: EER increases by 0.02%\n",
      "- F3frequency_sma3nz: EER increases by 0.00%\n",
      "- F2frequency_sma3nz: EER increases by -0.01%\n",
      "- jitterLocal_sma3nz: EER increases by -0.01%\n",
      "- logRelF0-H1-H2_sma3nz: EER increases by -0.01%\n",
      "- F1frequency_sma3nz: EER increases by -0.01%\n",
      "- hammarbergIndex_sma3: EER increases by -0.05%\n",
      "- Loudness_sma3: EER increases by -0.07%\n",
      "- mfcc2_sma3: EER increases by -0.11%\n",
      "- alphaRatio_sma3: EER increases by -0.14%\n",
      "- F1bandwidth_sma3nz: EER increases by -0.22%\n",
      "\n",
      "Ablation plot saved to saved_models/ablation_importance_AcousticProsodicAttention_23feat.png\n",
      "\n",
      "--- Running SHAP Analysis ---\n",
      "Calculating SHAP values (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69fe815b5034b5da3f15f47efd30ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during testing/analysis: CUDA out of memory. Tried to allocate 492.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 378.06 MiB is free. Process 3599198 has 9.90 GiB memory in use. Including non-PyTorch memory, this process has 306.00 MiB memory in use. Of the allocated memory 58.53 MiB is allocated by PyTorch, and 47.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "--- Skipping Retraining with Top 6 Features due to previous error ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, accuracy_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for TRAINING data\n",
    "CQCC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/cqcc_features_train.npy\"\n",
    "PROSODIC_FEATURES_TRAIN_PATH = \"processed_data_aligned_lld/egmaps_lld_features_train.npy\"\n",
    "LABELS_TRAIN_PATH = \"processed_data_aligned_lld/labels_train.npy\"\n",
    "\n",
    "# Paths for VALIDATION data\n",
    "CQCC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/cqcc_features_dev.npy\"\n",
    "PROSODIC_FEATURES_VAL_PATH = \"processed_data_aligned_lld/egmaps_lld_features_dev.npy\"\n",
    "LABELS_VAL_PATH = \"processed_data_aligned_lld/labels_dev.npy\"\n",
    "\n",
    "# Paths for TEST data\n",
    "CQCC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/cqcc_features_test.npy\"\n",
    "PROSODIC_FEATURES_TEST_PATH = \"processed_data_aligned_lld/egmaps_lld_features_test.npy\"\n",
    "LABELS_TEST_PATH = \"processed_data_aligned_lld/labels_test.npy\"\n",
    "\n",
    "# --- Model and Analysis Configuration ---\n",
    "MODEL_SAVE_PATH = \"saved_models/AcousticProsodicAttention_Best_23feat.pth\"\n",
    "PLOT_SAVE_PATH = \"saved_models/training_metrics_AcousticProsodicAttention_23feat.png\"\n",
    "ATTENTION_PLOT_PATH = \"saved_models/attention_importance_AcousticProsodicAttention_23feat.png\"\n",
    "ABLATION_PLOT_PATH = \"saved_models/ablation_importance_AcousticProsodicAttention_23feat.png\"\n",
    "SHAP_PLOT_PATH = \"saved_models/shap_importance_AcousticProsodicAttention_23feat.png\"\n",
    "MODEL_SAVE_PATH_TOP6 = \"saved_models/AcousticProsodicAttention_Best_Top6feat.pth\"\n",
    "PLOT_SAVE_PATH_TOP6 = \"saved_models/training_metrics_AcousticProsodicAttention_Top6feat.png\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- MODIFIED: calculate_eer now returns the threshold as well ---\n",
    "def calculate_eer(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Calculates the Equal Error Rate (EER) and the threshold at which it occurs.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    \n",
    "    # Calculate the EER\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    \n",
    "    # Calculate the threshold at which EER occurs\n",
    "    thresh = interp1d(fpr, thresholds)(eer)\n",
    "    \n",
    "    return eer * 100, thresh\n",
    "\n",
    "# --- MODIFIED: plot_training_history now includes validation accuracy ---\n",
    "def plot_training_history(history, save_path, title_prefix=\"\"):\n",
    "    \"\"\"Plots and saves the training history graph, including validation accuracy.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot Loss on the primary y-axis\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(history['train_loss'], color=color, linestyle='--', label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], color=color, linestyle='-', label='Val Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Plot EER and Accuracy on the secondary y-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "    color_eer = 'tab:blue'\n",
    "    ax2.set_ylabel('EER (%) / Accuracy (%)', color=color_eer)\n",
    "    ax2.plot(history['eer'], color=color_eer, linestyle='-', label='EER (%)')\n",
    "    ax2.plot(history['val_acc'], color='tab:green', linestyle=':', label='Val Accuracy (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor=color_eer)\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title(f'{title_prefix} Training and Validation Metrics')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nTraining plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset.\"\"\"\n",
    "    def __init__(self, cqcc_data, prosody_data, labels):\n",
    "        self.cqcc_data = torch.tensor(cqcc_data, dtype=torch.float32)\n",
    "        self.prosody_data = torch.tensor(prosody_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cqcc_data[idx], self.prosody_data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- COMPLETELY NEW ARCHITECTURE ---\n",
    "class AcousticProsodicAttentionModel(nn.Module):\n",
    "    def __init__(self, cqcc_feature_dim, prosody_feature_dim, cnn_channels=128, lstm_hidden=128, attention_dim=128):\n",
    "        super(AcousticProsodicAttentionModel, self).__init__()\n",
    "        \n",
    "        # Acoustic Branch (CNN for CQCC)\n",
    "        self.acoustic_branch = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=cqcc_feature_dim, out_channels=64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=cnn_channels, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_channels)\n",
    "        )\n",
    "        \n",
    "        # Prosodic Branch (BiLSTM for Prosody)\n",
    "        self.prosodic_branch = nn.LSTM(\n",
    "            input_size=prosody_feature_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.4\n",
    "        )\n",
    "        \n",
    "        # Attention Mechanism Layers\n",
    "        # Project acoustic features (Key/Value)\n",
    "        self.key_value_projection = nn.Linear(cnn_channels, attention_dim)\n",
    "        # Project prosodic features (Query)\n",
    "        self.query_projection = nn.Linear(lstm_hidden * 2, attention_dim)\n",
    "        \n",
    "        # Final Classifier\n",
    "        # Input will be the concatenated attention context and prosodic query\n",
    "        classifier_input_dim = attention_dim + attention_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cqcc_x, prosody_x):\n",
    "        # 1. Process Acoustic Features (CQCC)\n",
    "        # Input: (B, Seq, Feat) -> Permute: (B, Feat, Seq)\n",
    "        acoustic_out = self.acoustic_branch(cqcc_x.permute(0, 2, 1))\n",
    "        # Permute back: (B, Seq, Channels)\n",
    "        acoustic_out = acoustic_out.permute(0, 2, 1)\n",
    "\n",
    "        # 2. Process Prosodic Features\n",
    "        # Input: (B, Feat) -> Unsqueeze: (B, 1, Feat) for LSTM\n",
    "        prosody_x_unsqueezed = prosody_x.unsqueeze(1)\n",
    "        prosodic_out, _ = self.prosodic_branch(prosody_x_unsqueezed)\n",
    "        # Take the last hidden state: (B, LSTM_Hidden * 2)\n",
    "        prosodic_out = prosodic_out[:, -1, :]\n",
    "        \n",
    "        # 3. Project features for Attention\n",
    "        # Keys and Values from acoustic branch\n",
    "        keys = values = self.key_value_projection(acoustic_out) # (B, Seq, Attention_Dim)\n",
    "        # Query from prosodic branch\n",
    "        query = self.query_projection(prosodic_out) # (B, Attention_Dim)\n",
    "        \n",
    "        # 4. Perform Cross-Attention\n",
    "        # Unsqueeze query for batch matrix multiplication: (B, 1, Attention_Dim)\n",
    "        query_unsqueezed = query.unsqueeze(1)\n",
    "        \n",
    "        # Attention Score = (Query * Key^T) / sqrt(d_k)\n",
    "        attention_scores = torch.bmm(query_unsqueezed, keys.transpose(1, 2))\n",
    "        attention_scores = attention_scores / math.sqrt(keys.size(-1))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1) # (B, 1, Seq)\n",
    "        \n",
    "        # Context = Attention Weights * Value\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1) # (B, Attention_Dim)\n",
    "        \n",
    "        # 5. Fuse and Classify\n",
    "        # Concatenate the focused acoustic info (context) and the prosodic info (query)\n",
    "        fused_features = torch.cat([context, query], dim=1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_attention_weights(model, dataloader, device, save_path):\n",
    "    print(\"\\n--- Running Cross-Attention Weight Analysis ---\")\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for cqcc, prosody, _ in tqdm(dataloader, desc=\"Analyzing Attention\"):\n",
    "            cqcc, prosody = cqcc.to(device), prosody.to(device)\n",
    "            _, weights = model(cqcc, prosody)\n",
    "            all_weights.append(weights.squeeze(1).cpu().numpy())\n",
    "    \n",
    "    avg_weights = np.mean(np.concatenate(all_weights, axis=0), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(avg_weights, color='purple')\n",
    "    plt.xlabel('CQCC Time Frame (after CNN processing)')\n",
    "    plt.ylabel('Average Attention Weight')\n",
    "    plt.title('Cross-Attention: Importance of Acoustic Time Frames Guided by Prosody')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAttention plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def perform_feature_ablation(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running Feature Ablation Analysis ---\")\n",
    "    def evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=None):\n",
    "        model.eval()\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in dataloader:\n",
    "                cqcc, prosody, labels = cqcc.to(device), prosody.to(device), labels.to(device)\n",
    "                if feature_to_ablate is not None:\n",
    "                    prosody_clone = prosody.clone()\n",
    "                    prosody_clone[:, feature_to_ablate] = 0.0\n",
    "                    logits, _ = model(cqcc, prosody_clone)\n",
    "                else:\n",
    "                    logits, _ = model(cqcc, prosody)\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        eer, _ = calculate_eer(np.array(all_labels), np.array(all_scores).flatten())\n",
    "        return eer\n",
    "\n",
    "    baseline_eer = evaluate_eer_for_ablation(model, dataloader, device)\n",
    "    print(f\"Baseline EER with all features: {baseline_eer:.2f}%\")\n",
    "    eer_increases = {}\n",
    "    for i, name in enumerate(tqdm(feature_names, desc=\"Performing Ablation\")):\n",
    "        ablated_eer = evaluate_eer_for_ablation(model, dataloader, device, feature_to_ablate=i)\n",
    "        eer_increases[name] = ablated_eer - baseline_eer\n",
    "    \n",
    "    sorted_features = sorted(eer_increases.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFeature Importance based on EER Increase:\")\n",
    "    for feature, increase in sorted_features:\n",
    "        print(f\"- {feature}: EER increases by {increase:.2f}%\")\n",
    "    names = [item[0] for item in sorted_features]\n",
    "    increases = [item[1] for item in sorted_features]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(names, increases, color='salmon')\n",
    "    plt.xlabel('EER Increase (%)')\n",
    "    plt.title('Prosodic Feature Importance based on Feature Ablation')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nAblation plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "    return sorted_features\n",
    "\n",
    "\n",
    "def analyze_with_shap(model, dataloader, feature_names, device, save_path):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    model.eval()\n",
    "    background_cqcc, background_prosody, _ = next(iter(dataloader))\n",
    "    test_cqcc, test_prosody, _ = next(iter(dataloader))\n",
    "    \n",
    "    def model_wrapper(prosodic_features_numpy):\n",
    "        num_samples = prosodic_features_numpy.shape[0]\n",
    "        prosody_tensor = torch.from_numpy(prosodic_features_numpy).float().to(device)\n",
    "        cqcc_background_sample = background_cqcc[0:1].to(device)\n",
    "        cqcc_tensor = cqcc_background_sample.repeat(num_samples, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(cqcc_tensor, prosody_tensor)\n",
    "            output = torch.sigmoid(logits)\n",
    "        return output.cpu().numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_wrapper, background_prosody.numpy())\n",
    "    print(\"Calculating SHAP values (this may take a while)...\")\n",
    "    shap_values = explainer.shap_values(test_prosody.numpy(), nsamples=100)\n",
    "    print(\"Plotting SHAP summary...\")\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    plt.figure() \n",
    "    shap.summary_plot(shap_values, test_prosody.numpy(), feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary for Prosodic Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"SHAP plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        X_cqcc_train_full = np.load(CQCC_FEATURES_TRAIN_PATH)\n",
    "        X_prosody_train_3d = np.load(PROSODIC_FEATURES_TRAIN_PATH)\n",
    "        y_train = np.load(LABELS_TRAIN_PATH)\n",
    "        \n",
    "        X_cqcc_val_full = np.load(CQCC_FEATURES_VAL_PATH)\n",
    "        X_prosody_val_3d = np.load(PROSODIC_FEATURES_VAL_PATH)\n",
    "        y_val = np.load(LABELS_VAL_PATH)\n",
    "\n",
    "        X_cqcc_test_full = np.load(CQCC_FEATURES_TEST_PATH)\n",
    "        X_prosody_test_3d = np.load(PROSODIC_FEATURES_TEST_PATH)\n",
    "        y_test = np.load(LABELS_TEST_PATH)\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Loudness_sma3','alphaRatio_sma3','hammarbergIndex_sma3','slope0-500_sma3',\n",
    "            'slope500-1500_sma3','spectralFlux_sma3','mfcc1_sma3','mfcc2_sma3',\n",
    "            'mfcc3_sma3','mfcc4_sma3','F0semitoneFrom27.5Hz_sma3nz','jitterLocal_sma3nz',\n",
    "            'shimmerLocaldB_sma3nz','HNRdBACF_sma3nz','logRelF0-H1-H2_sma3nz',\n",
    "            'logRelF0-H1-A3_sma3nz','F1frequency_sma3nz','F1bandwidth_sma3nz',\n",
    "            'F1amplitudeLogRelF0_sma3nz','F2frequency_sma3nz','F2amplitudeLogRelF0_sma3nz',\n",
    "            'F3frequency_sma3nz','F3amplitudeLogRelF0_sma3nz'\n",
    "        ]\n",
    "        num_prosodic_features = X_prosody_train_3d.shape[1]\n",
    "        if len(feature_columns) != num_prosodic_features:\n",
    "            feature_columns = [f'ProsodicFeat_{i+1}' for i in range(num_prosodic_features)]\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- Scaling LLD Prosodic Features ---\")\n",
    "    ns_p, nf_p, nt_p = X_prosody_train_3d.shape\n",
    "    X_prosody_train_reshaped = X_prosody_train_3d.transpose(0, 2, 1).reshape(-1, nf_p)\n",
    "    scaler_prosody_lld = StandardScaler().fit(X_prosody_train_reshaped)\n",
    "    X_prosody_train_3d_scaled = scaler_prosody_lld.transform(X_prosody_train_reshaped).reshape(ns_p, nt_p, nf_p).transpose(0, 2, 1)\n",
    "    nsv_p, nfv_p, ntv_p = X_prosody_val_3d.shape\n",
    "    X_prosody_val_reshaped = X_prosody_val_3d.transpose(0, 2, 1).reshape(-1, nfv_p)\n",
    "    X_prosody_val_3d_scaled = scaler_prosody_lld.transform(X_prosody_val_reshaped).reshape(nsv_p, ntv_p, nfv_p).transpose(0, 2, 1)\n",
    "    nst_p, nft_p, ntt_p = X_prosody_test_3d.shape\n",
    "    X_prosody_test_reshaped = X_prosody_test_3d.transpose(0, 2, 1).reshape(-1, nft_p)\n",
    "    X_prosody_test_3d_scaled = scaler_prosody_lld.transform(X_prosody_test_reshaped).reshape(nst_p, ntt_p, nft_p).transpose(0, 2, 1)\n",
    "\n",
    "    print(\"Converting SCALED 3D LLD prosodic features to 2D summary statistics (mean)...\")\n",
    "    X_prosody_train_full = np.mean(X_prosody_train_3d_scaled, axis=2)\n",
    "    X_prosody_val_full = np.mean(X_prosody_val_3d_scaled, axis=2)\n",
    "    X_prosody_test_full = np.mean(X_prosody_test_3d_scaled, axis=2)\n",
    "\n",
    "    print(\"\\n--- Scaling CQCC Data ---\")\n",
    "    scaler_cqcc = StandardScaler()\n",
    "    ns, nx, ny = X_cqcc_train_full.shape\n",
    "    X_cqcc_train_scaled = scaler_cqcc.fit_transform(X_cqcc_train_full.reshape(ns, -1)).reshape(ns, nx, ny)\n",
    "    nsv, nxv, nyv = X_cqcc_val_full.shape\n",
    "    X_cqcc_val_scaled = scaler_cqcc.transform(X_cqcc_val_full.reshape(nsv, -1)).reshape(nsv, nxv, nyv)\n",
    "    ns_test, nx_test, ny_test = X_cqcc_test_full.shape\n",
    "    X_cqcc_test_scaled = scaler_cqcc.transform(X_cqcc_test_full.reshape(ns_test, -1)).reshape(ns_test, nx_test, ny_test)\n",
    "\n",
    "    train_dataset = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_full, y_train)\n",
    "    val_dataset = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_full, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = AcousticProsodicAttentionModel(\n",
    "        cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "        prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    \n",
    "    print(model)\n",
    "    # --- MODIFIED: Checkpointing based on best EER ---\n",
    "    best_eer = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'eer': []}\n",
    "    print(\"\\n--- Starting Model Training (All Features) ---\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for cqcc, prosody, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (All Feats)\"):\n",
    "            cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(cqcc, prosody)\n",
    "            loss = criterion(logits, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in val_loader:\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                logits, _ = model(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "        \n",
    "        # --- MODIFIED: Calculate EER, threshold, and accuracy ---\n",
    "        eer, eer_thresh = calculate_eer(all_labels, all_scores)\n",
    "        val_preds = (all_scores > eer_thresh).astype(int)\n",
    "        val_accuracy = accuracy_score(all_labels, val_preds) * 100\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | EER: {eer:.2f}%\")\n",
    "        history.update({'train_loss': history['train_loss']+[avg_train_loss], 'val_loss': history['val_loss']+[avg_val_loss], 'val_acc': history['val_acc']+[val_accuracy], 'eer': history['eer']+[eer]})\n",
    "        scheduler.step(eer) # Schedule based on EER\n",
    "        \n",
    "        # --- MODIFIED: Save model if EER has improved ---\n",
    "        if eer < best_eer:\n",
    "            best_eer = eer\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"   -> Val EER decreased to {eer:.2f}%. New best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    plot_training_history(history, PLOT_SAVE_PATH, title_prefix=\"All Features (New Architecture)\")\n",
    "\n",
    "    # --- FINAL TESTING AND ANALYSIS (ALL FEATURES) ---\n",
    "    print(\"\\n--- Starting Final Testing and Analysis (All Features) ---\")\n",
    "    try:\n",
    "        test_dataset = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_full, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        analysis_model = AcousticProsodicAttentionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "            prosody_feature_dim=X_prosody_train_full.shape[1]\n",
    "        ).to(DEVICE)\n",
    "        analysis_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        analysis_model.eval()\n",
    "        \n",
    "        analyze_attention_weights(analysis_model, test_loader, DEVICE, ATTENTION_PLOT_PATH)\n",
    "        sorted_features = perform_feature_ablation(analysis_model, test_loader, feature_columns, DEVICE, ABLATION_PLOT_PATH)\n",
    "        analyze_with_shap(analysis_model, test_loader, feature_columns, DEVICE, SHAP_PLOT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing/analysis: {e}\")\n",
    "        sorted_features = [] # Ensure sorted_features exists\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- RETRAINING WITH TOP 6 FEATURES ---\n",
    "    # ==============================================================================\n",
    "    if not sorted_features:\n",
    "        print(\"\\n--- Skipping Retraining with Top 6 Features due to previous error ---\")\n",
    "    else:\n",
    "        print(\"\\n\\n--- Starting Retraining with Top 6 Features ---\")\n",
    "        \n",
    "        top_6_feature_names = [item[0] for item in sorted_features[:6]]\n",
    "        top_6_indices = [feature_columns.index(name) for name in top_6_feature_names]\n",
    "        print(\"Top 6 features selected for retraining:\", top_6_feature_names)\n",
    "\n",
    "        X_prosody_train_top6 = X_prosody_train_full[:, top_6_indices]\n",
    "        X_prosody_val_top6 = X_prosody_val_full[:, top_6_indices]\n",
    "        X_prosody_test_top6 = X_prosody_test_full[:, top_6_indices]\n",
    "        \n",
    "        train_dataset_top6 = AudioFeatureDataset(X_cqcc_train_scaled, X_prosody_train_top6, y_train)\n",
    "        val_dataset_top6 = AudioFeatureDataset(X_cqcc_val_scaled, X_prosody_val_top6, y_val)\n",
    "        train_loader_top6 = DataLoader(train_dataset_top6, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader_top6 = DataLoader(val_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model_top6 = AcousticProsodicAttentionModel(\n",
    "            cqcc_feature_dim=X_cqcc_train_full.shape[2],\n",
    "            prosody_feature_dim=6\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        optimizer_top6 = optim.Adam(model_top6.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler_top6 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_top6, 'min', patience=5, verbose=True)\n",
    "\n",
    "        print(model_top6)\n",
    "        best_eer_top6 = float('inf')\n",
    "        history_top6 = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'eer': []}\n",
    "        print(\"\\n--- Starting Model Training (Top 6 Features) ---\")\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            model_top6.train()\n",
    "            running_loss = 0.0\n",
    "            for cqcc, prosody, labels in tqdm(train_loader_top6, desc=f\"Epoch {epoch+1}/{EPOCHS} (Top 6)\"):\n",
    "                cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer_top6.zero_grad()\n",
    "                logits, _ = model_top6(cqcc, prosody)\n",
    "                loss = criterion(logits, labels.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer_top6.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            model_top6.eval()\n",
    "            val_loss = 0.0\n",
    "            all_labels, all_scores = [], []\n",
    "            with torch.no_grad():\n",
    "                for cqcc, prosody, labels in val_loader_top6:\n",
    "                    cqcc, prosody, labels = cqcc.to(DEVICE), prosody.to(DEVICE), labels.to(DEVICE)\n",
    "                    logits, _ = model_top6(cqcc, prosody)\n",
    "                    loss = criterion(logits, labels.unsqueeze(1))\n",
    "                    val_loss += loss.item()\n",
    "                    all_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader_top6)\n",
    "            avg_val_loss = val_loss / len(val_loader_top6)\n",
    "            all_labels, all_scores = np.array(all_labels), np.array(all_scores).flatten()\n",
    "            eer, eer_thresh = calculate_eer(all_labels, all_scores)\n",
    "            val_preds = (all_scores > eer_thresh).astype(int)\n",
    "            val_accuracy = accuracy_score(all_labels, val_preds) * 100\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | EER: {eer:.2f}%\")\n",
    "            history_top6.update({'train_loss': history_top6['train_loss']+[avg_train_loss], 'val_loss': history_top6['val_loss']+[avg_val_loss], 'val_acc': history_top6['val_acc']+[val_accuracy], 'eer': history_top6['eer']+[eer]})\n",
    "            scheduler_top6.step(eer)\n",
    "\n",
    "            if eer < best_eer_top6:\n",
    "                best_eer_top6 = eer\n",
    "                torch.save(model_top6.state_dict(), MODEL_SAVE_PATH_TOP6)\n",
    "                print(f\"   -> Val EER decreased to {eer:.2f}%. New best model (Top 6) saved to {MODEL_SAVE_PATH_TOP6}\")\n",
    "\n",
    "        plot_training_history(history_top6, PLOT_SAVE_PATH_TOP6, title_prefix=\"Top 6 Features (New Architecture)\")\n",
    "\n",
    "        # --- FINAL TESTING (TOP 6 FEATURES) ---\n",
    "        print(\"\\n--- Starting Final Testing (Top 6 Features) ---\")\n",
    "        test_dataset_top6 = AudioFeatureDataset(X_cqcc_test_scaled, X_prosody_test_top6, y_test)\n",
    "        test_loader_top6 = DataLoader(test_dataset_top6, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        model_top6.load_state_dict(torch.load(MODEL_SAVE_PATH_TOP6))\n",
    "        model_top6.eval()\n",
    "\n",
    "        all_test_labels, all_test_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for cqcc, prosody, labels in tqdm(test_loader_top6, desc=\"Final Testing (Top 6)\"):\n",
    "                cqcc, prosody = cqcc.to(DEVICE), prosody.to(DEVICE)\n",
    "                logits, _ = model_top6(cqcc, prosody)\n",
    "                all_test_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_test_labels, all_test_scores = np.array(all_test_labels), np.array(all_test_scores).flatten()\n",
    "        test_eer_top6, _ = calculate_eer(all_test_labels, all_test_scores)\n",
    "        print(f\"\\n--- Final Test Results (Top 6 Features) --- | EER: {test_eer_top6:.2f}%\")\n",
    "        print(\"\\n--- Experiment Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c61a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
